"""
Production Training Script

Industry-standard training using:
- HuggingFace Trainer (most widely used)
- Weights & Biases (experiment tracking)
- QLoRA/LoRA (parameter-efficient fine-tuning)
- Best practices from top AI labs

This is the kind of training script used at:
- OpenAI, Anthropic, Google, Meta
- Startups like Cohere, Stability AI
- Research labs worldwide

Key features:
‚úÖ Reproducible (seed setting, deterministic ops)
‚úÖ Efficient (mixed precision, gradient accumulation)
‚úÖ Monitored (W&B logging, metrics)
‚úÖ Production-ready (checkpointing, resuming)
"""

import os
import sys
import argparse
from pathlib import Path
from typing import Optional, Dict, Any

import torch
from datasets import load_from_disk
from transformers import (
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback,
    set_seed
)
import wandb

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

from models import MusicTransformerConfig, MusicTransformerForGeneration
from models.qlora import QLoRAConfig, get_bnb_config, apply_qlora_to_model
from data import EventTokenizer, MusicDataCollator


def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Train Music Transformer with QLoRA")

    # Model args
    parser.add_argument("--model_name_or_path", type=str, default=None,
                       help="Pretrained model path (if fine-tuning)")
    parser.add_argument("--hidden_size", type=int, default=512,
                       help="Model hidden size")
    parser.add_argument("--num_layers", type=int, default=8,
                       help="Number of transformer layers")
    parser.add_argument("--num_heads", type=int, default=8,
                       help="Number of attention heads")

    # Data args
    parser.add_argument("--data_dir", type=str, required=True,
                       help="Directory with processed dataset")
    parser.add_argument("--max_seq_len", type=int, default=2048,
                       help="Maximum sequence length")

    # QLoRA args
    parser.add_argument("--use_qlora", action="store_true",
                       help="Use QLoRA (recommended)")
    parser.add_argument("--lora_rank", type=int, default=8,
                       help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=16,
                       help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.1,
                       help="LoRA dropout")

    # Training args
    parser.add_argument("--output_dir", type=str, required=True,
                       help="Output directory for checkpoints")
    parser.add_argument("--num_train_epochs", type=int, default=5,
                       help="Number of training epochs")
    parser.add_argument("--per_device_train_batch_size", type=int, default=4,
                       help="Training batch size per GPU")
    parser.add_argument("--per_device_eval_batch_size", type=int, default=4,
                       help="Evaluation batch size per GPU")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=4,
                       help="Gradient accumulation steps")
    parser.add_argument("--learning_rate", type=float, default=3e-4,
                       help="Learning rate")
    parser.add_argument("--warmup_steps", type=int, default=100,
                       help="Warmup steps")
    parser.add_argument("--weight_decay", type=float, default=0.01,
                       help="Weight decay")
    parser.add_argument("--max_grad_norm", type=float, default=1.0,
                       help="Max gradient norm for clipping")

    # Efficiency args
    parser.add_argument("--fp16", action="store_true",
                       help="Use FP16 mixed precision")
    parser.add_argument("--bf16", action="store_true",
                       help="Use BF16 mixed precision (better on A100)")

    # Logging args
    parser.add_argument("--wandb_project", type=str, default="music-transformer",
                       help="W&B project name")
    parser.add_argument("--wandb_run_name", type=str, default=None,
                       help="W&B run name")
    parser.add_argument("--logging_steps", type=int, default=10,
                       help="Log every N steps")
    parser.add_argument("--eval_steps", type=int, default=100,
                       help="Evaluate every N steps")
    parser.add_argument("--save_steps", type=int, default=500,
                       help="Save checkpoint every N steps")

    # Other args
    parser.add_argument("--seed", type=int, default=42,
                       help="Random seed for reproducibility")
    parser.add_argument("--resume_from_checkpoint", type=str, default=None,
                       help="Resume training from checkpoint")

    return parser.parse_args()


def setup_wandb(args) -> None:
    """
    Initialize Weights & Biases

    W&B is the industry standard for experiment tracking
    Used by: OpenAI, Cohere, Anthropic, Stability AI, etc.
    """
    wandb.init(
        project=args.wandb_project,
        name=args.wandb_run_name or f"music_transformer_{args.output_dir.split('/')[-1]}",
        config=vars(args),
        tags=["music", "transformer", "qlora" if args.use_qlora else "full-finetune"]
    )

    # Log system info
    wandb.config.update({
        "gpu": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU",
        "cuda_version": torch.version.cuda if torch.cuda.is_available() else None,
        "pytorch_version": torch.__version__
    })


def create_or_load_model(args, tokenizer: EventTokenizer):
    """
    Create new model or load pretrained model

    Args:
        args: Command line arguments
        tokenizer: Event tokenizer

    Returns:
        Model (with QLoRA if specified)
    """
    if args.model_name_or_path:
        # Load pretrained model
        print(f"Loading model from {args.model_name_or_path}...")

        if args.use_qlora:
            # Load with 4-bit quantization
            qlora_config = QLoRAConfig(
                lora_rank=args.lora_rank,
                lora_alpha=args.lora_alpha,
                lora_dropout=args.lora_dropout
            )
            bnb_config = get_bnb_config(qlora_config)

            model = MusicTransformerForGeneration.from_pretrained(
                args.model_name_or_path,
                quantization_config=bnb_config,
                device_map="auto"
            )

            # Apply LoRA adapters
            model = apply_qlora_to_model(model, qlora_config)

        else:
            # Load normally
            model = MusicTransformerForGeneration.from_pretrained(
                args.model_name_or_path
            )

    else:
        # Create new model from scratch
        print("Creating new model from scratch...")

        config = MusicTransformerConfig(
            vocab_size=tokenizer.vocab_size,
            hidden_size=args.hidden_size,
            num_hidden_layers=args.num_layers,
            num_attention_heads=args.num_heads,
            max_position_embeddings=args.max_seq_len,
            pad_token_id=tokenizer.pad_token_id,
            bos_token_id=tokenizer.bos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )

        model = MusicTransformerForGeneration(config)

        if args.use_qlora:
            print("‚ö†Ô∏è  QLoRA requires a pretrained model. Training from scratch without LoRA.")

    # Print model info
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"\nModel statistics:")
    print(f"  Total parameters: {total_params:,}")
    print(f"  Trainable parameters: {trainable_params:,}")
    print(f"  Trainable ratio: {trainable_params / total_params * 100:.2f}%")

    # Log to W&B
    wandb.config.update({
        "total_params": total_params,
        "trainable_params": trainable_params,
        "trainable_ratio": trainable_params / total_params
    })

    return model


def main():
    """Main training function"""

    # Parse arguments
    args = parse_args()

    # Set seed for reproducibility (IMPORTANT for research!)
    set_seed(args.seed)

    # Setup W&B
    print("Initializing Weights & Biases...")
    setup_wandb(args)

    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)

    # Load dataset
    print(f"\nLoading dataset from {args.data_dir}...")
    dataset = load_from_disk(args.data_dir)

    print(f"Dataset loaded:")
    print(f"  Train: {len(dataset['train'])} examples")
    print(f"  Validation: {len(dataset['validation'])} examples")

    # Create tokenizer
    print("\nCreating tokenizer...")
    tokenizer = EventTokenizer()

    # Create data collator
    data_collator = MusicDataCollator(pad_token_id=tokenizer.pad_token_id)

    # Create or load model
    print("\nPreparing model...")
    model = create_or_load_model(args, tokenizer)

    # Training arguments (HuggingFace Trainer configuration)
    training_args = TrainingArguments(
        # Output
        output_dir=args.output_dir,
        overwrite_output_dir=True,

        # Training
        num_train_epochs=args.num_train_epochs,
        per_device_train_batch_size=args.per_device_train_batch_size,
        per_device_eval_batch_size=args.per_device_eval_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        max_grad_norm=args.max_grad_norm,
        warmup_steps=args.warmup_steps,

        # Evaluation
        evaluation_strategy="steps",
        eval_steps=args.eval_steps,
        save_strategy="steps",
        save_steps=args.save_steps,
        save_total_limit=3,  # Keep only 3 best checkpoints
        load_best_model_at_end=True,
        metric_for_best_model="loss",

        # Logging
        logging_dir=os.path.join(args.output_dir, "logs"),
        logging_steps=args.logging_steps,
        report_to="wandb",  # Log to W&B

        # Efficiency
        fp16=args.fp16,
        bf16=args.bf16,
        dataloader_num_workers=4,
        dataloader_pin_memory=True,

        # Reproducibility
        seed=args.seed,
        data_seed=args.seed,

        # Other
        remove_unused_columns=False,
        push_to_hub=False  # Set to True to push to HuggingFace Hub
    )

    # Create Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset['train'],
        eval_dataset=dataset['validation'],
        data_collator=data_collator,
        callbacks=[
            EarlyStoppingCallback(early_stopping_patience=3)
        ]
    )

    # Train!
    print("\n" + "=" * 60)
    print("üéπ Starting training...")
    print("=" * 60 + "\n")

    if args.resume_from_checkpoint:
        print(f"Resuming from checkpoint: {args.resume_from_checkpoint}")
        trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)
    else:
        trainer.train()

    # Save final model
    print("\n" + "=" * 60)
    print("üíæ Saving final model...")
    print("=" * 60 + "\n")

    final_model_path = os.path.join(args.output_dir, "final_model")
    trainer.save_model(final_model_path)

    print(f"‚úÖ Model saved to {final_model_path}")

    # Evaluate final model
    print("\n" + "=" * 60)
    print("üìä Final evaluation...")
    print("=" * 60 + "\n")

    eval_results = trainer.evaluate()

    print(f"Final validation loss: {eval_results['eval_loss']:.4f}")

    # Log to W&B
    wandb.log({"final_eval_loss": eval_results['eval_loss']})

    # Finish W&B run
    wandb.finish()

    print("\n" + "=" * 60)
    print("‚úÖ Training complete!")
    print("=" * 60)
    print(f"\nOutputs:")
    print(f"  üìÅ Checkpoints: {args.output_dir}")
    print(f"  üìä W&B dashboard: {wandb.run.url if wandb.run else 'N/A'}")
    print(f"  üíæ Final model: {final_model_path}")


if __name__ == "__main__":
    main()
