# í•„ìˆ˜ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸ ğŸ“š

**Brad Mehldau MIDI Generator í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ í•µì‹¬ ë…¼ë¬¸ë“¤**

ì´ ë¬¸ì„œëŠ” í”„ë¡œì íŠ¸ì˜ 4ê°€ì§€ ì ‘ê·¼ ë°©ì‹ì„ ì´í•´í•˜ëŠ”ë° í•„ìˆ˜ì ì¸ ë…¼ë¬¸ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

---

## ğŸ¯ ì™œ ë…¼ë¬¸ì„ ì½ì–´ì•¼ í•˜ëŠ”ê°€?

### 1. **ë©´ì ‘ ëŒ€ë¹„**
- "ì–´ë–¤ ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„í–ˆë‚˜ìš”?"
- "Transformerì™€ RNNì˜ ì°¨ì´ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"
- "LoRAê°€ ì™œ íš¨ìœ¨ì ì¸ê°€ìš”?"

â†’ ë…¼ë¬¸ì„ ì½ìœ¼ë©´ ê¹Šì´ìˆê²Œ ë‹µë³€ ê°€ëŠ¥!

### 2. **êµ¬í˜„ ì´í•´**
- ë‹¨ìˆœíˆ ì½”ë“œë¥¼ ë² ë¼ëŠ” ê²ƒ vs ì›ë¦¬ë¥¼ ì´í•´í•˜ê³  êµ¬í˜„
- ë…¼ë¬¸ì„ ì½ìœ¼ë©´ ì™œ ê·¸ë ‡ê²Œ êµ¬í˜„í–ˆëŠ”ì§€ ì´í•´ë¨
- ë¬¸ì œê°€ ìƒê²¼ì„ ë•Œ ë””ë²„ê¹… ê°€ëŠ¥

### 3. **ìµœì‹  íŠ¸ë Œë“œ íŒŒì•…**
- AI ë¶„ì•¼ëŠ” ë¹ ë¥´ê²Œ ë°œì „
- 2017 Transformer â†’ 2021 LoRA â†’ 2023 QLoRA
- ë‹¤ìŒì€ ë¬´ì—‡ì´ ë‚˜ì˜¬ê¹Œ?

---

## ğŸ“‘ ì¹´í…Œê³ ë¦¬ë³„ í•„ìˆ˜ ë…¼ë¬¸

### ğŸµ 1. Music Generation (ìŒì•… ìƒì„±)

#### â­ Music Transformer (Google Magenta, 2018)
**ì œëª©**: Music Transformer: Generating Music with Long-Term Structure

**ì €ì**: Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon, Curtis Hawthorne, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, Douglas Eck

**ì¶œíŒ**: ICLR 2019

**arXiv**: https://arxiv.org/abs/1809.04281

**ê´€ë ¨ ë¸Œëœì¹˜**:
- âœ… ë¸Œëœì¹˜ 3: Perceiver + Music Transformer
- âœ… ë¸Œëœì¹˜ 4: Production Transformer

**í•µì‹¬ ê¸°ì—¬**:
1. **Relative Positional Encoding** for music
   - ì ˆëŒ€ ìœ„ì¹˜ ëŒ€ì‹  ìƒëŒ€ì  ìœ„ì¹˜ ì‚¬ìš©
   - ìŒì•…ì˜ ë°˜ë³µ íŒ¨í„´ì„ ë” ì˜ í•™ìŠµ

2. **Long-term Structure**
   - ìˆ˜ì²œ ê°œì˜ í† í° ì‹œí€€ìŠ¤ ì²˜ë¦¬
   - ë¶„ ë‹¨ìœ„ ìŒì•… ìƒì„± ê°€ëŠ¥

3. **Memory Efficiency**
   - Relative attentionì˜ ë©”ëª¨ë¦¬ë¥¼ O(LÂ²D) â†’ O(LD)ë¡œ ê°ì†Œ

**ì™œ ì¤‘ìš”í•œê°€**:
- ìŒì•… ìƒì„±ì— Transformerë¥¼ ì„±ê³µì ìœ¼ë¡œ ì ìš©í•œ ì²« ì‚¬ë¡€
- Relative attentionì´ symbolic musicì— í•„ìˆ˜ì ì„ì„ ì¦ëª…
- ìš°ë¦¬ í”„ë¡œì íŠ¸ì˜ ê¸°ë°˜ ì•„í‚¤í…ì²˜

**ì½ëŠ” ë²•**:
1. Section 3 (Relative Attention) ì§‘ì¤‘
2. Figure 2 (Relative Positional Encoding) ì´í•´
3. Section 5 (Experiments) ê²°ê³¼ ë¶„ì„

**ë©´ì ‘ ì§ˆë¬¸ ì˜ˆìƒ**:
- Q: "ì™œ Music TransformerëŠ” ìƒëŒ€ ìœ„ì¹˜ë¥¼ ì‚¬ìš©í•˜ë‚˜ìš”?"
- A: "ìŒì•…ì€ ì ˆëŒ€ ìœ„ì¹˜ë³´ë‹¤ ìƒëŒ€ì  ê°„ê²©ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 'C-E-G'ëŠ” ì–´ë–¤ ì˜¥íƒ€ë¸Œë“  C major chordì…ë‹ˆë‹¤. Relative attentionì€ ì´ëŸ° ì „ì´ ë¶ˆë³€ì„±(translational invariance)ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."

---

### ğŸ¤– 2. Transformer Architecture (ê¸°ë³¸ ì•„í‚¤í…ì²˜)

#### â­â­â­ Attention Is All You Need (Google, 2017)
**ì œëª©**: Attention Is All You Need

**ì €ì**: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Åukasz Kaiser, Illia Polosukhin

**ì¶œíŒ**: NeurIPS 2017

**arXiv**: https://arxiv.org/abs/1706.03762

**ê´€ë ¨ ë¸Œëœì¹˜**:
- âœ… **ëª¨ë“  ë¸Œëœì¹˜!** (Transformer ê¸°ë°˜)

**í•µì‹¬ ê¸°ì—¬**:
1. **Self-Attention Mechanism**
   - RNN ì—†ì´ ìˆœì„œ ë°ì´í„° ì²˜ë¦¬
   - ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥ â†’ ë¹ ë¦„

2. **Multi-Head Attention**
   - ì—¬ëŸ¬ ê´€ì ì—ì„œ attention ê³„ì‚°
   - ë‹¤ì–‘í•œ íŒ¨í„´ í•™ìŠµ

3. **Positional Encoding**
   - ìœ„ì¹˜ ì •ë³´ ì£¼ì…
   - sin/cos í•¨ìˆ˜ ì‚¬ìš©

**ì™œ ì¤‘ìš”í•œê°€**:
- **í˜„ëŒ€ AIì˜ ê¸°ì´ˆ** (GPT, BERT, ëª¨ë“  LLMì˜ ê¸°ë°˜)
- 21ì„¸ê¸° ê°€ì¥ ë§ì´ ì¸ìš©ëœ ë…¼ë¬¸ (173,000+ ì¸ìš©)
- ì´ ë…¼ë¬¸ì„ ëª¨ë¥´ë©´ Transformerë¥¼ ì´í•´í•  ìˆ˜ ì—†ìŒ

**ì½ëŠ” ë²•**:
1. Section 3.2 (Attention) - **ê°€ì¥ ì¤‘ìš”!**
2. Figure 1 (Architecture) ì™„ì „íˆ ì´í•´
3. Section 3.3 (Multi-Head Attention)
4. Section 3.5 (Positional Encoding)

**ë©´ì ‘ ì§ˆë¬¸ ì˜ˆìƒ**:
- Q: "Transformerê°€ RNNë³´ë‹¤ ë‚˜ì€ ì´ìœ ëŠ”?"
- A: "1) ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥ (RNNì€ ìˆœì°¨ ì²˜ë¦¬ í•„ìˆ˜), 2) Long-range dependencyë¥¼ ë” ì˜ í¬ì°© (attentionìœ¼ë¡œ ë©€ë¦¬ ìˆëŠ” í† í°ë„ ì§ì ‘ ì—°ê²°), 3) Vanishing gradient ë¬¸ì œ ì—†ìŒ"

**ê¼­ ì•”ê¸°í•´ì•¼ í•  ê³µì‹**:
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V

Q: Query (ë¬´ì—‡ì„ ì°¾ì„ê¹Œ?)
K: Key (ì–´ë””ë¥¼ ë³¼ê¹Œ?)
V: Value (ë¬´ì—‡ì„ ê°€ì ¸ì˜¬ê¹Œ?)
```

---

### ğŸ”§ 3. Efficient Fine-tuning (íš¨ìœ¨ì ì¸ í•™ìŠµ)

#### â­â­ LoRA (Microsoft, 2021)
**ì œëª©**: LoRA: Low-Rank Adaptation of Large Language Models

**ì €ì**: Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen

**ì¶œíŒ**: ICLR 2022

**arXiv**: https://arxiv.org/abs/2106.09685

**GitHub**: https://github.com/microsoft/LoRA

**ê´€ë ¨ ë¸Œëœì¹˜**:
- âœ… ë¸Œëœì¹˜ 2: Moonbeam + LoRA
- âœ… ë¸Œëœì¹˜ 3: Perceiver + QLoRA
- âœ… ë¸Œëœì¹˜ 4: Production Transformer

**í•µì‹¬ ê¸°ì—¬**:
1. **Low-Rank Decomposition**
   - ì „ì²´ weight matrixë¥¼ í•™ìŠµí•˜ì§€ ì•ŠìŒ
   - ì‘ì€ ë‘ í–‰ë ¬(A, B)ë§Œ í•™ìŠµ: Î”W = BA

2. **íš¨ìœ¨ì„±**
   - í•™ìŠµ íŒŒë¼ë¯¸í„° 10,000ë°° ê°ì†Œ
   - GPU ë©”ëª¨ë¦¬ 3ë°° ê°ì†Œ
   - ì„±ëŠ¥ì€ full fine-tuningê³¼ ë™ë“±

3. **Inference ì‹œ overhead ì—†ìŒ**
   - LoRAë¥¼ mergeí•  ìˆ˜ ìˆìŒ: W' = W + BA
   - ì¶”ë¡  ì†ë„ ë™ì¼

**ì™œ ì¤‘ìš”í•œê°€**:
- **2021-2024 ê°€ì¥ ë§ì´ ì“°ì´ëŠ” fine-tuning ë°©ë²•**
- Stable Diffusion, ChatGPT ê°œì¸í™” ëª¨ë‘ LoRA ì‚¬ìš©
- Consumer GPUì—ì„œë„ LLM fine-tuning ê°€ëŠ¥í•˜ê²Œ í•¨

**ìˆ˜ì‹**:
```
ì›ë˜: W âˆˆ R^(dÃ—k) ì „ì²´ í•™ìŠµ (dÃ—kê°œ íŒŒë¼ë¯¸í„°)

LoRA: W = W_0 + Î”W = W_0 + BA
      A âˆˆ R^(dÃ—r), B âˆˆ R^(rÃ—k), r << min(d,k)
      í•™ìŠµ íŒŒë¼ë¯¸í„°: r(d+k) << dk

ì˜ˆì‹œ: d=4096, k=4096, r=8
      ì›ë˜: 16,777,216ê°œ íŒŒë¼ë¯¸í„°
      LoRA: 65,536ê°œ íŒŒë¼ë¯¸í„° (0.39%!)
```

**ë©´ì ‘ ì§ˆë¬¸ ì˜ˆìƒ**:
- Q: "LoRAëŠ” ì–´ë–»ê²Œ ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ê°™ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë‚˜ìš”?"
- A: "Pre-trained ëª¨ë¸ì˜ weight ë³€í™”ëŠ” intrinsic rankê°€ ë‚®ìŠµë‹ˆë‹¤. ì¦‰, ëŒ€ë¶€ë¶„ì˜ ë³€í™”ëŠ” ì €ì°¨ì› ë¶€ê³µê°„ì—ì„œ ì¼ì–´ë‚©ë‹ˆë‹¤. LoRAëŠ” ì´ insightë¥¼ í™œìš©í•´ ë³€í™”ëŸ‰ Î”Wë¥¼ rank rë¡œ ì œí•œí•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼ r=8 ì •ë„ë©´ ì¶©ë¶„í–ˆìŠµë‹ˆë‹¤."

---

#### â­â­â­ QLoRA (University of Washington, 2023)
**ì œëª©**: QLoRA: Efficient Finetuning of Quantized LLMs

**ì €ì**: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer

**ì¶œíŒ**: NeurIPS 2023

**arXiv**: https://arxiv.org/abs/2305.14314

**GitHub**: https://github.com/artidoro/qlora

**ê´€ë ¨ ë¸Œëœì¹˜**:
- âœ… ë¸Œëœì¹˜ 3: Perceiver + QLoRA
- âœ… ë¸Œëœì¹˜ 4: Production Transformer (í•µì‹¬!)

**í•µì‹¬ ê¸°ì—¬**:
1. **4-bit NormalFloat (NF4)**
   - ì¼ë°˜ 4-bitë³´ë‹¤ ì •ë³´ ì´ë¡ ì ìœ¼ë¡œ ìµœì 
   - ì •ê·œë¶„í¬ weightì— íŠ¹í™”

2. **Double Quantization**
   - Quantization constantë„ quantize
   - ë©”ëª¨ë¦¬ ì¶”ê°€ ì ˆì•½

3. **Paged Optimizers**
   - GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ CPUë¡œ spill
   - OOM ë°©ì§€

**ì„±ëŠ¥**:
- 65B ëª¨ë¸ì„ **48GB GPU 1ê°œ**ë¡œ fine-tuning!
- ì›ë˜ëŠ” A100 8ê°œ í•„ìš” (~$100,000)
- RTX 3090 1ê°œë¡œ ê°€ëŠ¥ (~$1,500)

**ì™œ ì¤‘ìš”í•œê°€**:
- **2023-2025 SOTA fine-tuning ë°©ë²•**
- Consumer GPUë¡œë„ LLM fine-tuning ê°€ëŠ¥
- Hugging Face PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ í•µì‹¬

**ë©´ì ‘ ì§ˆë¬¸ ì˜ˆìƒ**:
- Q: "QLoRAê°€ LoRAë³´ë‹¤ ë‚˜ì€ ì ì€?"
- A: "QLoRAëŠ” base modelì„ 4-bitë¡œ quantizeí•´ì„œ ë©”ëª¨ë¦¬ë¥¼ 75% ì¤„ì…ë‹ˆë‹¤. ì´ë ‡ê²Œ ì ˆì•½í•œ ë©”ëª¨ë¦¬ë¡œ ë” í° ëª¨ë¸ì„ í•™ìŠµí•˜ê±°ë‚˜, ë” í° batch sizeë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„±ëŠ¥ ì €í•˜ëŠ” ê±°ì˜ ì—†ìŠµë‹ˆë‹¤(< 1%)."

**ìš°ë¦¬ í”„ë¡œì íŠ¸ì—ì„œ**:
```python
# Production Transformer ë¸Œëœì¹˜
from peft import LoraConfig, get_peft_model
from transformers import BitsAndBytesConfig

# 4-bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # NormalFloat4
    bnb_4bit_compute_dtype=torch.float16
)

# LoRA config
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["query", "key", "value"]
)
```

---

### ğŸ§  4. Advanced Architectures (ê³ ê¸‰ ì•„í‚¤í…ì²˜)

#### â­ Perceiver (DeepMind, 2021)
**ì œëª©**: Perceiver: General Perception with Iterative Attention

**ì €ì**: Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, Joao Carreira

**ì¶œíŒ**: ICML 2021

**arXiv**: https://arxiv.org/abs/2103.03206

**ê´€ë ¨ ë¸Œëœì¹˜**:
- âœ… ë¸Œëœì¹˜ 3: Perceiver + Music Transformer

**í•µì‹¬ ê¸°ì—¬**:
1. **Asymmetric Attention**
   - Input â†’ Latent: Cross-attention
   - ì…ë ¥ í¬ê¸°ì— ë¬´ê´€í•œ ë³µì¡ë„

2. **O(N) Complexity**
   - ì¼ë°˜ Transformer: O(NÂ²)
   - Perceiver: O(N) + O(MÂ²), M << N

3. **Modality-Agnostic**
   - ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ ëª¨ë‘ ì²˜ë¦¬
   - Symbolic musicë„ ê°€ëŠ¥

**ìˆ˜ì‹**:
```
ì¼ë°˜ Transformer:
  Self-Attention: O(NÂ²D)
  N = ì‹œí€€ìŠ¤ ê¸¸ì´, D = hidden dim

Perceiver:
  Cross-Attention (N â†’ M): O(NMD)
  Self-Attention (M): O(MÂ²D)
  M << Nì´ë©´ O(ND) + O(MÂ²D) â‰ˆ O(ND)

ì˜ˆì‹œ: N=50,000 (pixels), M=512 (latents)
      Transformer: 2.5B operations
      Perceiver: 25M operations (100ë°° ë¹ ë¦„!)
```

**ì™œ ì¤‘ìš”í•œê°€**:
- ê¸´ ì‹œí€€ìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬
- MIDIëŠ” ìˆ˜ì²œ~ìˆ˜ë§Œ ê°œ ì´ë²¤íŠ¸ â†’ Perceiver ì í•©
- ë¸Œëœì¹˜ 3ì—ì„œ ê°€ì¥ íš¨ìœ¨ì ì¸ ì´ìœ 

---

## ğŸ“Š ë…¼ë¬¸ë³„ ìš°ì„ ìˆœìœ„

### ğŸ”¥ í•„ìˆ˜ (ë°˜ë“œì‹œ ì½ê¸°)
1. **Attention Is All You Need** - Transformer ê¸°ë³¸
2. **LoRA** - Fine-tuning ê¸°ë³¸
3. **Music Transformer** - ìŒì•… ìƒì„± ê¸°ë³¸

### ğŸŒŸ ê°•ë ¥ ì¶”ì²œ (ì·¨ì—… ì¤€ë¹„ìƒ)
4. **QLoRA** - 2023-2025 SOTA
5. **Perceiver** - íš¨ìœ¨ì ì¸ ì•„í‚¤í…ì²˜

### ğŸ“š ì‹¬í™” (ì‹œê°„ ìˆìœ¼ë©´)
6. VQ-VAE (Diffusion ì´í•´)
7. Diffusion Transformer
8. BERT (Style Encoder ì´í•´)

---

## ğŸ¯ ë¸Œëœì¹˜ë³„ ê´€ë ¨ ë…¼ë¬¸

### ë¸Œëœì¹˜ 1: SCG + Transformer
**í•„ìˆ˜ ë…¼ë¬¸**:
- Attention Is All You Need (Transformer)
- VQ-VAE
- Denoising Diffusion Probabilistic Models (DDPM)

### ë¸Œëœì¹˜ 2: Moonbeam + LoRA
**í•„ìˆ˜ ë…¼ë¬¸**:
- Attention Is All You Need (Transformer)
- LoRA
- Music Transformer (ì°¸ê³ )

### ë¸Œëœì¹˜ 3: Perceiver + Music Transformer + QLoRA â­
**í•„ìˆ˜ ë…¼ë¬¸**:
- Attention Is All You Need (Transformer)
- Music Transformer
- Perceiver
- LoRA
- QLoRA

### ë¸Œëœì¹˜ 4: Production Transformer â­â­â­
**í•„ìˆ˜ ë…¼ë¬¸**:
- Attention Is All You Need (Transformer)
- Music Transformer
- LoRA
- QLoRA

---

## ğŸ“– ë…¼ë¬¸ ì½ëŠ” ë²•

### 1ì°¨ ë…ì„œ (30ë¶„)
1. **Abstract** - ë¬´ì—‡ì„ í•œ ë…¼ë¬¸ì¸ê°€?
2. **Introduction** - ì™œ ì´ê²Œ ì¤‘ìš”í•œê°€?
3. **Conclusion** - ê²°ê³¼ê°€ ì–´ë–¤ê°€?
4. **Figures** - ì‹œê°ì ìœ¼ë¡œ ì´í•´

### 2ì°¨ ë…ì„œ (2ì‹œê°„)
1. **Method** - ì–´ë–»ê²Œ êµ¬í˜„í–ˆë‚˜?
2. **Experiments** - ì–´ë–¤ ì‹¤í—˜ì„ í–ˆë‚˜?
3. **Related Work** - ë‹¤ë¥¸ ë°©ë²•ê³¼ì˜ ë¹„êµ

### 3ì°¨ ë…ì„œ (ì½”ë“œ ë³´ë©´ì„œ)
1. ë…¼ë¬¸ì˜ ìˆ˜ì‹ì„ ì½”ë“œë¡œ ëŒ€ì‘
2. Figureë¥¼ ì¬í˜„
3. Ablation study ì´í•´

---

## ğŸ’¡ ë©´ì ‘ ëŒ€ë¹„ í•µì‹¬ ì§ˆë¬¸

### Transformer
**Q**: Self-Attentionì˜ ì‹œê°„ ë³µì¡ë„ëŠ”?
**A**: O(NÂ²D). Nê°œ í† í°ì´ ê°ê° Nê°œë¥¼ attention â†’ NÂ². ì´ê²ƒì´ ê¸´ ì‹œí€€ìŠ¤ì˜ bottleneck.

**Q**: Positional Encodingì€ ì™œ í•„ìš”í•œê°€?
**A**: Attentionì€ ìˆœì„œ ì •ë³´ê°€ ì—†ìŒ (permutation invariant). PEë¥¼ ë”í•´ì„œ ìœ„ì¹˜ ì •ë³´ ì£¼ì….

### LoRA
**Q**: LoRAì˜ rank rì€ ì–´ë–»ê²Œ ì •í•˜ë‚˜?
**A**: ì‹¤í—˜ì ìœ¼ë¡œ ê²°ì •. ë³´í†µ r=8ì´ë©´ ì¶©ë¶„. ë„ˆë¬´ ì‘ìœ¼ë©´ í‘œí˜„ë ¥ ë¶€ì¡±, ë„ˆë¬´ í¬ë©´ íš¨ìœ¨ì„± ê°ì†Œ.

**Q**: LoRAë¥¼ ì–´ëŠ layerì— ì ìš©í•˜ë‚˜?
**A**: ë³´í†µ attentionì˜ Q, K, V projection. FFNì€ ì„ íƒì . ì‹¤í—˜ ê²°ê³¼ attentionë§Œìœ¼ë¡œ ì¶©ë¶„.

### Music Transformer
**Q**: Absolute vs Relative positionì˜ ì°¨ì´ëŠ”?
**A**: Absoluteì€ ì ˆëŒ€ ìœ„ì¹˜ (0, 1, 2, ...), RelativeëŠ” ìƒëŒ€ ê±°ë¦¬ (i-j). ìŒì•…ì€ ì „ì´ ë¶ˆë³€ì„±ì´ ì¤‘ìš”í•´ì„œ relativeê°€ ì í•©.

---

## ğŸ“¥ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ

ëª¨ë“  ë…¼ë¬¸ì€ **arXiv**ì—ì„œ ë¬´ë£Œë¡œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥:

```bash
# ì˜ˆì‹œ: LoRA ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ
wget https://arxiv.org/pdf/2106.09685.pdf -O LoRA.pdf

# QLoRA
wget https://arxiv.org/pdf/2305.14314.pdf -O QLoRA.pdf

# Music Transformer
wget https://arxiv.org/pdf/1809.04281.pdf -O MusicTransformer.pdf

# Attention Is All You Need
wget https://arxiv.org/pdf/1706.03762.pdf -O Transformer.pdf

# Perceiver
wget https://arxiv.org/pdf/2103.03206.pdf -O Perceiver.pdf
```

---

## ğŸ“ ì¶”ê°€ í•™ìŠµ ë¦¬ì†ŒìŠ¤

### ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Transformer ì‹œê°í™”
- [LoRA Explained](https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch) - LoRA ìƒì„¸ ì„¤ëª…
- [Annotated Music Transformer](https://gudgud96.github.io/2020/04/01/annotated-music-transformer/) - ì½”ë“œì™€ í•¨ê»˜

### ê°•ì˜
- Stanford CS224N - NLP with Deep Learning
- MIT 6.S191 - Introduction to Deep Learning
- Fast.ai - Practical Deep Learning

### êµ¬í˜„
- [HuggingFace Transformers Course](https://huggingface.co/course)
- [Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)

---

## âœ… í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Week 1: Transformer ê¸°ì´ˆ
- [ ] "Attention Is All You Need" 1ì°¨ ë…ì„œ
- [ ] Self-attention ìˆ˜ì‹ ì´í•´
- [ ] Multi-head attention êµ¬í˜„
- [ ] Positional encoding êµ¬í˜„

### Week 2: Music Transformer
- [ ] "Music Transformer" 1ì°¨ ë…ì„œ
- [ ] Relative attention ì´í•´
- [ ] Event-based MIDI representation êµ¬í˜„

### Week 3: LoRA
- [ ] "LoRA" ë…¼ë¬¸ ì½ê¸°
- [ ] Low-rank decomposition ì´í•´
- [ ] LoRA layer êµ¬í˜„

### Week 4: QLoRA
- [ ] "QLoRA" ë…¼ë¬¸ ì½ê¸°
- [ ] 4-bit quantization ì´í•´
- [ ] PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©

### Week 5: Perceiver (Optional)
- [ ] "Perceiver" ë…¼ë¬¸ ì½ê¸°
- [ ] Cross-attention ì´í•´
- [ ] Complexity ë¶„ì„

---

## ğŸ† ëª©í‘œ

ì´ ë…¼ë¬¸ë“¤ì„ ì½ê³  ë‚˜ë©´:

### ë©´ì ‘ì—ì„œ
âœ… Transformerë¥¼ ê¹Šì´ìˆê²Œ ì„¤ëª… ê°€ëŠ¥
âœ… ìµœì‹  fine-tuning ë°©ë²• (LoRA, QLoRA) ì´í•´
âœ… êµ¬í˜„ ê²°ì •ì„ ë…¼ë¬¸ ê¸°ë°˜ìœ¼ë¡œ ì •ë‹¹í™” ê°€ëŠ¥

### ì‹¤ë¬´ì—ì„œ
âœ… ìƒˆë¡œìš´ ëª¨ë¸ ë¹ ë¥´ê²Œ ì´í•´
âœ… ë…¼ë¬¸ì„ ì½”ë“œë¡œ êµ¬í˜„ ê°€ëŠ¥
âœ… ìµœì í™” ë°©í–¥ ê²°ì • ê°€ëŠ¥

### ì»¤ë¦¬ì–´ì—ì„œ
âœ… Research Engineer ì—­ëŸ‰
âœ… ML Architect ê¸°ì´ˆ
âœ… í‰ìƒ í•™ìŠµ ëŠ¥ë ¥

---

**"ë…¼ë¬¸ì„ ì½ëŠ” ê²ƒì€ ì–´ë µì§€ë§Œ, ì½ê³  ë‚˜ë©´ ì„¸ìƒì´ ë‹¤ë¥´ê²Œ ë³´ì…ë‹ˆë‹¤."** ğŸš€

ì§€ê¸ˆ ë°”ë¡œ ì‹œì‘í•˜ì„¸ìš”!
