# Phase 3: ë³¸ê²© í›ˆë ¨ ğŸš€

**ëª©í‘œ**: ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ í”„ë¡œë•ì…˜ í’ˆì§ˆ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤.

**ì˜ˆìƒ ì‹œê°„**: 1-2ì£¼ (GPU ì„±ëŠ¥ì— ë”°ë¼)
**ë‚œì´ë„**: â­â­â­â­â˜†

---

## ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] í”„ë¡œë•ì…˜ ì„¤ì • íŒŒì¼ ì‘ì„±
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
- [ ] AMP + EMA í™œì„±í™”
- [ ] í›ˆë ¨ ì‹œì‘ ë° ëª¨ë‹ˆí„°ë§
- [ ] ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
- [ ] Early stopping ì„¤ì •

---

## 1. í”„ë¡œë•ì…˜ ì„¤ì •

### config.yaml (Full Model)

```yaml
model:
  vocab_size: 2048
  hidden_dim: 512
  latent_dim: 256
  num_layers: 12
  num_heads: 8
  max_seq_len: 2048
  diffusion_steps: 1000
  num_style_dims: 64

training:
  batch_size: 8          # GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •
  epochs: 100
  learning_rate: 0.0001
  warmup_steps: 1000
  weight_decay: 0.01
  gradient_clip: 1.0
  use_amp: true          # 2x ë¹ ë¦„
  use_ema: true          # í’ˆì§ˆ í–¥ìƒ
  ema_decay: 0.9999

data:
  train_dir: 'data/art_tatum_midi/train'
  val_dir: 'data/art_tatum_midi/val'
  max_seq_len: 2048
  num_workers: 4

checkpoint:
  save_every: 1000       # steps
  keep_last_n: 5         # ìµœê·¼ 5ê°œë§Œ
  checkpoint_dir: 'checkpoints/production'

logging:
  log_every: 100
  tensorboard_dir: 'logs/tensorboard'
```

---

## 2. í›ˆë ¨ ì‹œì‘

### ê¸°ë³¸ í›ˆë ¨

```bash
python scripts/train_tatumflow.py --config config.yaml
```

### AMP + EMA í›ˆë ¨ (ê¶Œì¥)

```bash
python scripts/phase3_train_production.py \
  --config config.yaml \
  --use_amp \
  --use_ema \
  --device cuda
```

### ì˜ˆìƒ ì¶œë ¥

```
TatumFlow Training
==================
Model: 125M parameters
Data: 70 train files, 9 val files
GPU: NVIDIA A100 (40GB)
AMP: Enabled
EMA: Enabled (decay=0.9999)

Epoch 1/100
  Step 100: Loss=5.234, Recon=3.456, Diff=1.234, KL=0.012 (2.3s/step)
  Step 200: Loss=4.789, Recon=3.123, Diff=1.098, KL=0.011 (2.1s/step)
  ...
  Validation Loss: 4.123
  Checkpoint saved: checkpoints/production/step_1000.pt

Epoch 2/100
  Step 1100: Loss=4.234, Recon=2.789, Diff=0.987, KL=0.010 (2.0s/step)
  ...
```

---

## 3. ëª¨ë‹ˆí„°ë§

### TensorBoard

```bash
tensorboard --logdir=logs/tensorboard
```

**ì£¼ìš” ë©”íŠ¸ë¦­**:

1. **Loss/train_total**: ì „ì²´ í›ˆë ¨ ì†ì‹¤ (â†“)
2. **Loss/val_total**: ê²€ì¦ ì†ì‹¤ (â†“, í•˜ì§€ë§Œ trainë³´ë‹¤ ë†’ìŒ)
3. **Loss/reconstruction**: ì¬êµ¬ì„± ì†ì‹¤ (ë¹ ë¥´ê²Œ â†“)
4. **Loss/diffusion**: ë””í“¨ì „ ì†ì‹¤ (ì²œì²œíˆ â†“)
5. **Loss/kl_divergence**: KL ë°œì‚° (ë‚®ê²Œ ìœ ì§€)
6. **Learning_rate**: ì›Œë°ì—… í›„ ê°ì†Œ
7. **Grad_norm**: Gradient í¬ê¸° (1.0 ì´í•˜ë¡œ í´ë¦½ë¨)

### GPU ëª¨ë‹ˆí„°ë§

```bash
# ì‹¤ì‹œê°„ GPU ì‚¬ìš©ë¥ 
watch -n 1 nvidia-smi

# GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
# config.yamlì—ì„œ batch_size ì¤„ì´ê¸°
```

---

## 4. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

### Batch Size

**í° batch**:
- âœ… ë¹ ë¦„
- âœ… ì•ˆì •ì 
- âŒ ë©”ëª¨ë¦¬ ë§ì´ í•„ìš”

**ì‘ì€ batch**:
- âœ… ë©”ëª¨ë¦¬ ì ˆì•½
- âŒ ëŠë¦¼
- âŒ ë¶ˆì•ˆì •

**ê¶Œì¥**:
- A100 (40GB): batch_size=8-16
- V100 (16GB): batch_size=4-8
- T4 (16GB): batch_size=2-4

### Learning Rate

**ë„ˆë¬´ ë†’ìŒ** (>0.001):
- Loss ì§„ë™
- NaN ë°œìƒ

**ë„ˆë¬´ ë‚®ìŒ** (<0.00001):
- ëŠë¦° í•™ìŠµ
- ìˆ˜ë ´ ì•ˆë¨

**ê¶Œì¥**: 0.0001 (Adam ê¸°ì¤€)

### Warmup Steps

ì²˜ìŒ N stepsëŠ” LRì„ ì²œì²œíˆ ì˜¬ë¦¼

**íš¨ê³¼**:
- í›ˆë ¨ ì´ˆê¸° ì•ˆì •ì„± â†‘
- Loss spike ë°©ì§€

**ê¶Œì¥**: 1000 steps

---

## 5. Early Stopping

### ì–¸ì œ ë©ˆì¶œê¹Œ?

**ì¢‹ì€ ì‹ í˜¸** (ê³„ì†):
- Validation loss ê³„ì† ê°ì†Œ
- Train/Val gap ì‘ìŒ

**ë‚˜ìœ ì‹ í˜¸** (ì¤‘ë‹¨):
- Validation loss 5 epochs ì—°ì† ì¦ê°€ â†’ **ì˜¤ë²„í”¼íŒ…**
- Lossê°€ NaN â†’ **í„°ì§**
- Lossê°€ ì•ˆ ë–¨ì–´ì§ â†’ **í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •**

### ìë™ Early Stopping

```python
# scripts/phase3_train_production.pyì— ì¶”ê°€
patience = 5
best_val_loss = float('inf')
patience_counter = 0

for epoch in range(num_epochs):
    val_loss = validate(model, val_loader)

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        save_checkpoint('best.pt')
    else:
        patience_counter += 1

    if patience_counter >= patience:
        print(f"Early stopping at epoch {epoch}")
        break
```

---

## 6. ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬

### ì €ì¥ ì£¼ê¸°

- **ë„ˆë¬´ ìì£¼**: ë””ìŠ¤í¬ ë‚­ë¹„
- **ë„ˆë¬´ ì ê²Œ**: ì¢‹ì€ ëª¨ë¸ ë†“ì¹¨

**ê¶Œì¥**: 1000 stepsë§ˆë‹¤

### ì¤‘ìš” ì²´í¬í¬ì¸íŠ¸

1. **best.pt**: ìµœê³  ê²€ì¦ Loss
2. **latest.pt**: ê°€ì¥ ìµœê·¼
3. **epoch_X.pt**: Epochë³„

### ë””ìŠ¤í¬ ì ˆì•½

```yaml
checkpoint:
  keep_last_n: 5  # ìµœê·¼ 5ê°œë§Œ
  delete_old: true
```

---

## 7. í›ˆë ¨ ì¬ê°œ (Resume)

### ì¤‘ë‹¨ëœ í›ˆë ¨ ì´ì–´í•˜ê¸°

```bash
python scripts/phase3_train_production.py \
  --config config.yaml \
  --resume checkpoints/production/latest.pt
```

**ìë™ìœ¼ë¡œ ë³µì›**:
- ëª¨ë¸ ê°€ì¤‘ì¹˜
- Optimizer ìƒíƒœ
- Epoch ë²ˆí˜¸
- Learning rate scheduler

---

## ğŸ“ í•™ìŠµ ë‚´ìš©

### Mixed Precision (AMP)

**FP32** (ê¸°ë³¸):
- ì •ë°€ë„: ë†’ìŒ
- ì†ë„: ëŠë¦¼
- ë©”ëª¨ë¦¬: ë§ì´ ì‚¬ìš©

**FP16** (AMP):
- ì •ë°€ë„: ì•½ê°„ ë‚®ìŒ (ìŒì•…ì— ë¬´ì‹œ ê°€ëŠ¥)
- ì†ë„: **2ë°° ë¹ ë¦„**
- ë©”ëª¨ë¦¬: **50% ì ˆì•½**

**ë™ì‘ ì›ë¦¬**:
```python
with torch.cuda.amp.autocast():
    output = model(input)  # FP16 ì—°ì‚°
loss = criterion(output, target)
scaler.scale(loss).backward()  # FP32 gradient
```

### EMA (Exponential Moving Average)

ëª¨ë¸ ê°€ì¤‘ì¹˜ì˜ ì´ë™ í‰ê· ì„ ìœ ì§€í•©ë‹ˆë‹¤.

**íš¨ê³¼**:
- ìƒì„± í’ˆì§ˆ í–¥ìƒ
- í›ˆë ¨ ì•ˆì •ì„± ì¦ê°€

**ê³µì‹**:
```
Î¸_ema = decay * Î¸_ema + (1 - decay) * Î¸
```

**ê¶Œì¥ decay**: 0.9999

### Gradient Clipping

Gradientê°€ ë„ˆë¬´ í¬ë©´ clipí•©ë‹ˆë‹¤.

**ë¬¸ì œ**: Exploding gradient â†’ NaN
**í•´ê²°**: `gradient_clip=1.0`

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

---

## ğŸš¨ ë¬¸ì œ í•´ê²°

### ë¬¸ì œ 1: CUDA Out of Memory

**ì¦ìƒ**:
```
RuntimeError: CUDA out of memory
```

**í•´ê²°**:
```yaml
training:
  batch_size: 4  # ì¤„ì´ê¸°
  gradient_accumulation: 2  # ì¶”ê°€ (effective batch = 4*2=8)
```

### ë¬¸ì œ 2: Lossê°€ NaN

**ì›ì¸**: Exploding gradient

**í•´ê²°**:
```yaml
training:
  gradient_clip: 0.5  # ë” ë‚®ê²Œ
  learning_rate: 0.00005  # ì¤„ì´ê¸°
```

### ë¬¸ì œ 3: í›ˆë ¨ì´ ë„ˆë¬´ ëŠë¦¼

**ì²´í¬**:
1. GPU ì‚¬ìš©ë¥  100%ì¸ê°€? (`nvidia-smi`)
2. AMP í™œì„±í™”í–ˆë‚˜?
3. num_workers=4 ì„¤ì •í–ˆë‚˜?

**ê°œì„ **:
```yaml
data:
  num_workers: 4  # CPU ì½”ì–´ í™œìš©
training:
  use_amp: true   # 2x ë¹ ë¦„
```

---

## âœ… Phase 3 ì™„ë£Œ ì²´í¬

- [ ] 10+ epochs í›ˆë ¨ ì™„ë£Œ
- [ ] Validation loss < 2.0
- [ ] TensorBoardì—ì„œ ì •ìƒ í•™ìŠµ ê³¡ì„  í™•ì¸
- [ ] ì²´í¬í¬ì¸íŠ¸ ì—¬ëŸ¬ ê°œ ì €ì¥ë¨
- [ ] GPU íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš© (>80%)

---

## ë‹¤ìŒ ë‹¨ê³„

**Phase 4: í‰ê°€ ë° ê°œì„ **ìœ¼ë¡œ ì´ë™:
```bash
cat docs/phase4_evaluation.md
```

**ì˜ í•˜ì…¨ìŠµë‹ˆë‹¤! ì´ì œ ëª¨ë¸ì„ í‰ê°€í•´ë´…ì‹œë‹¤! ğŸ“Š**
