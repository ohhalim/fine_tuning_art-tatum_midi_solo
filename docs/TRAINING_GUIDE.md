# Brad Mehldau AI - Training Guide

Complete guide for training the SCG + Transformer hybrid model

## ğŸ“‹ ëª©ì°¨

1. [í™˜ê²½ ì„ íƒ](#í™˜ê²½-ì„ íƒ)
2. [Runpod í•™ìŠµ](#runpod-í•™ìŠµ)
3. [Google Colab í•™ìŠµ](#google-colab-í•™ìŠµ)
4. [ë¡œì»¬ í™˜ê²½ í•™ìŠµ](#ë¡œì»¬-í™˜ê²½-í•™ìŠµ)
5. [Training Timeline](#training-timeline)
6. [ë¹„ìš© ìµœì í™”](#ë¹„ìš©-ìµœì í™”)

---

## í™˜ê²½ ì„ íƒ

| í™˜ê²½ | GPU | ë¹„ìš© | ì†ë„ | ì¶”ì²œ |
|------|-----|------|------|------|
| **Runpod** | RTX 3090/4090 | $0.34-0.79/hr | â­â­â­â­â­ | Phase 3 (Brad fine-tuning) |
| **Colab Pro** | T4/V100 | $10/month | â­â­â­ | Phase 1-2 (ê¸°ë³¸ í•™ìŠµ) |
| **ë¡œì»¬ (M1/M2)** | M1/M2 GPU | Free | â­â­ | ê°œë°œ & ì¶”ë¡ ë§Œ |
| **ë¡œì»¬ (NVIDIA)** | RTX 3060+ | Free | â­â­â­â­ | ì‹œê°„ ì—¬ìœ  ìˆìœ¼ë©´ |

### ì¶”ì²œ ì „ëµ (ì˜ˆì‚° $20)

```
Week 1-4: Google Colab Pro ($10/month)
  - VQ-VAE ì‚¬ì „í•™ìŠµ
  - Style Encoder í•™ìŠµ
  - DiT ê¸°ë³¸ í•™ìŠµ

Week 5-6: Runpod ($10)
  - Brad Mehldau fine-tuning ONLY
  - RTX 3090 spot instance

Week 7+: ë¡œì»¬ í™˜ê²½
  - ì¶”ë¡  & FL Studio í†µí•©
```

---

## Runpod í•™ìŠµ

### Step 1: Runpod ê³„ì • ìƒì„±

1. https://runpod.io ì ‘ì†
2. ê³„ì • ìƒì„± & í¬ë ˆë”§ ì¶”ê°€ ($10-20)
3. GPU Pod ì„ íƒ:
   - **RTX 3090**: $0.34/hr (Spot), $0.44/hr (On-Demand)
   - **RTX 4090**: $0.69/hr (Spot), $0.79/hr (On-Demand)

### Step 2: Pod ìƒì„±

```bash
# Template ì„ íƒ
Template: PyTorch 2.0+
GPU: RTX 3090 (1x)
Disk: 50GB
Volume: 100GB (ì˜êµ¬ ì €ì¥ìš©)

# Auto-stop ì„¤ì • (ë¹„ìš© ì ˆì•½!)
Idle Timeout: 30 minutes
```

### Step 3: ì½”ë“œ í´ë¡  & ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# SSH ì ‘ì† í›„
cd /workspace

# í”„ë¡œì íŠ¸ í´ë¡ 
git clone https://github.com/yourusername/brad-mehldau-ai.git
cd brad-mehldau-ai

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# CUDA í™•ì¸
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"
```

### Step 4: ë°ì´í„° ë‹¤ìš´ë¡œë“œ

```bash
# MAESTRO ë°ì´í„° (VQ-VAE ì‚¬ì „í•™ìŠµìš©)
python scripts/download_data.py --dataset maestro --data_dir ./data

# PiJAMA ë°ì´í„° (Brad Mehldau)
python scripts/download_data.py --dataset pijama --data_dir ./data
```

### Step 5: VQ-VAE ì‚¬ì „í•™ìŠµ (~10ì‹œê°„)

```bash
# VQ-VAE í•™ìŠµ
python scripts/train_vqvae.py \
  --data_dir ./data/maestro \
  --save_dir ./checkpoints/vqvae \
  --epochs 50 \
  --batch_size 16 \
  --lr 1e-4 \
  --device cuda

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ + ë¡œê·¸ ì €ì¥
nohup python scripts/train_vqvae.py \
  --data_dir ./data/maestro \
  --save_dir ./checkpoints/vqvae \
  --epochs 50 \
  --batch_size 16 > vqvae_train.log 2>&1 &

# ë¡œê·¸ ëª¨ë‹ˆí„°ë§
tail -f vqvae_train.log
```

### Step 6: Checkpoint ë°±ì—… (ì¤‘ìš”!)

```bash
# rclone ì„¤ì • (Google Drive)
rclone config

# ì²´í¬í¬ì¸íŠ¸ ë°±ì—…
rclone copy ./checkpoints gdrive:brad-mehldau-checkpoints/ -P

# ìë™ ë°±ì—… ìŠ¤í¬ë¦½íŠ¸
while true; do
  rclone copy ./checkpoints gdrive:brad-mehldau-checkpoints/ -P
  sleep 3600  # 1ì‹œê°„ë§ˆë‹¤
done &
```

### Step 7: Brad Mehldau Fine-tuning (~15ì‹œê°„)

```bash
# Hybrid ëª¨ë¸ fine-tuning
python scripts/train_hybrid.py \
  --vqvae_ckpt ./checkpoints/vqvae/best.pt \
  --brad_data ./data/brad_mehldau \
  --epochs 50 \
  --batch_size 16 \
  --lr 5e-6 \
  --device cuda \
  --wandb_project "brad-scg-transformer"
```

### ë¹„ìš© ê³„ì‚°

```
VQ-VAE (10ì‹œê°„) + Fine-tuning (15ì‹œê°„) = 25ì‹œê°„
RTX 3090 Spot: 25 Ã— $0.34 = $8.5
RTX 4090 Spot: 25 Ã— $0.69 = $17.25

âš ï¸  Spot instanceëŠ” ì¤‘ê°„ì— ëŠê¸¸ ìˆ˜ ìˆìŒ
â†’ ìë™ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í•„ìˆ˜!
```

---

## Google Colab í•™ìŠµ

### Step 1: Colab Pro êµ¬ë…

- Colab Pro: $10/month
- GPU: T4 (ë¬´ë£Œ), V100 (Pro)
- ì—°ì† ì‹¤í–‰: ~12ì‹œê°„

### Step 2: Colab Notebook ì„¤ì •

```python
# GPU í™•ì¸
!nvidia-smi

# í”„ë¡œì íŠ¸ í´ë¡ 
!git clone https://github.com/yourusername/brad-mehldau-ai.git
%cd brad-mehldau-ai

# ì˜ì¡´ì„± ì„¤ì¹˜
!pip install -r requirements.txt

# Google Drive ë§ˆìš´íŠ¸ (ì²´í¬í¬ì¸íŠ¸ ì €ì¥ìš©)
from google.colab import drive
drive.mount('/content/drive')
```

### Step 3: ë°ì´í„° ë‹¤ìš´ë¡œë“œ

```python
# MAESTRO ë‹¤ìš´ë¡œë“œ
!python scripts/download_data.py --dataset maestro --data_dir ./data

# ë˜ëŠ” Google Driveì—ì„œ ë³µì‚¬
!cp -r /content/drive/MyDrive/brad-data/maestro ./data/
```

### Step 4: í•™ìŠµ ì‹¤í–‰

```python
# VQ-VAE í•™ìŠµ
!python scripts/train_vqvae.py \
  --data_dir ./data/maestro \
  --save_dir /content/drive/MyDrive/brad-checkpoints/vqvae \
  --epochs 30 \
  --batch_size 8 \
  --device cuda

# âš ï¸  12ì‹œê°„ ì œí•œ ì£¼ì˜!
# â†’ 30 epochsì”© ë‚˜ëˆ ì„œ í•™ìŠµ
```

### Step 5: Checkpoint ì €ì¥

```python
# ìë™ìœ¼ë¡œ Google Driveì— ì €ì¥
# save_dirì„ Drive ê²½ë¡œë¡œ ì„¤ì •

# í•™ìŠµ ì¤‘ê°„ì— ì €ì¥
import shutil
shutil.copy('./checkpoints/vqvae/best.pt',
            '/content/drive/MyDrive/brad-checkpoints/vqvae_backup.pt')
```

### Colab ì œí•œì‚¬í•­

```
âœ… ì¥ì :
- ì €ë ´ ($10/month)
- ì„¤ì • ê°„ë‹¨
- GPU ë¬´ë£Œ (ì œí•œì )

âŒ ë‹¨ì :
- 12ì‹œê°„ ì—°ì† ì‹¤í–‰ ì œí•œ
- ì¤‘ê°„ì— ëŠê¹€
- GPU í• ë‹¹ ë¶ˆí™•ì‹¤ (íŠ¹íˆ ë¬´ë£Œ)

ğŸ’¡ í•´ê²°ì±…:
- í•™ìŠµì„ 10-20 epoch ë‹¨ìœ„ë¡œ ë‚˜ëˆ”
- checkpointì—ì„œ resume ê¸°ëŠ¥ í•„ìˆ˜
- ìë™ ì €ì¥ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
```

---

## ë¡œì»¬ í™˜ê²½ í•™ìŠµ

### NVIDIA GPU (RTX 3060 ì´ìƒ)

```bash
# CUDA ì„¤ì¹˜ í™•ì¸
nvidia-smi

# PyTorch ì„¤ì¹˜ (CUDA 11.8)
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# í•™ìŠµ ì‹¤í–‰
python scripts/train_vqvae.py \
  --data_dir ./data/maestro \
  --save_dir ./checkpoints/vqvae \
  --epochs 50 \
  --batch_size 8 \
  --device cuda
```

### Apple Silicon (M1/M2)

```bash
# PyTorch MPS ì§€ì› í™•ì¸
python -c "import torch; print(torch.backends.mps.is_available())"

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# í•™ìŠµ ì‹¤í–‰ (MPS)
python scripts/train_vqvae.py \
  --data_dir ./data/maestro \
  --save_dir ./checkpoints/vqvae \
  --epochs 50 \
  --batch_size 4 \
  --device mps

# âš ï¸  MPSëŠ” CUDAë³´ë‹¤ ëŠë¦¼ (2-3ë°°)
# â†’ ê°œë°œìš©ìœ¼ë¡œë§Œ ê¶Œì¥
```

---

## Training Timeline

### Phase 1: VQ-VAE ì‚¬ì „í•™ìŠµ (Week 1-2)

```bash
# MAESTRO ë°ì´í„° ë‹¤ìš´ë¡œë“œ
python scripts/download_data.py --dataset maestro

# VQ-VAE í•™ìŠµ (RTX 3090 ê¸°ì¤€: 8-10ì‹œê°„)
python scripts/train_vqvae.py \
  --data_dir ./data/maestro \
  --epochs 50 \
  --batch_size 16

# ì²´í¬í¬ì¸íŠ¸: ./checkpoints/vqvae/best.pt
```

### Phase 2: Style Encoder ì‚¬ì „í•™ìŠµ (Week 3-4)

```bash
# PiJAMA ë°ì´í„° ë‹¤ìš´ë¡œë“œ
python scripts/download_data.py --dataset pijama

# Style Encoder í•™ìŠµ (RTX 3090: 8-10ì‹œê°„)
python scripts/train_style_encoder.py \
  --data_dir ./data/pijama \
  --epochs 50 \
  --batch_size 32

# ì²´í¬í¬ì¸íŠ¸: ./checkpoints/style_encoder/best.pt
```

### Phase 3: Brad Mehldau Fine-tuning (Week 5-6)

```bash
# Brad Mehldau ë°ì´í„° í•„í„°ë§
python scripts/filter_brad_mehldau.py

# Hybrid ëª¨ë¸ fine-tuning (RTX 3090: 10-15ì‹œê°„)
python scripts/train_hybrid.py \
  --vqvae_ckpt ./checkpoints/vqvae/best.pt \
  --style_encoder_ckpt ./checkpoints/style_encoder/best.pt \
  --brad_data ./data/brad_mehldau \
  --epochs 50 \
  --batch_size 16

# ìµœì¢… ì²´í¬í¬ì¸íŠ¸: ./checkpoints/brad_final/best.pt
```

---

## ë¹„ìš© ìµœì í™”

### 1. Spot Instance ì‚¬ìš© (50% ì ˆê°)

```bash
# Runpod Spot instance
RTX 3090: $0.34/hr (vs $0.44 On-Demand)

# ì£¼ì˜: ì¤‘ê°„ì— ëŠê¸¸ ìˆ˜ ìˆìŒ
â†’ ìë™ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í•„ìˆ˜
```

### 2. Mixed Precision Training (2ë°° ë¹ ë¦„)

```python
# train_vqvae.pyì— ì¶”ê°€
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    recon, vq_loss, perplexity = model(piano_roll)
    loss = recon_loss + vq_loss

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 3. Gradient Accumulation (í° batch size íš¨ê³¼)

```python
# ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
accumulation_steps = 4
batch_size = 4  # effective batch = 16

for i, batch in enumerate(train_loader):
    loss = train_step(batch)
    loss = loss / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### 4. Checkpoint Pruning (ì €ì¥ ê³µê°„ ì ˆì•½)

```python
# ìµœê·¼ 5ê°œ checkpointë§Œ ìœ ì§€
import glob
checkpoints = sorted(glob.glob('./checkpoints/*.pt'))
if len(checkpoints) > 5:
    os.remove(checkpoints[0])  # ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ ì‚­ì œ
```

---

## ë¬¸ì œ í•´ê²°

### Out of Memory (OOM)

```python
# batch_size ì¤„ì´ê¸°
--batch_size 8  # ë˜ëŠ” 4

# gradient accumulation ì‚¬ìš©
--gradient_accumulation 2

# mixed precision training
--mixed_precision fp16
```

### Runpod Pod ëŠê¹€

```bash
# ìë™ ì¬ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
#!/bin/bash
while true; do
    python scripts/train_hybrid.py \
      --resume ./checkpoints/brad_final/latest.pt \
      ...

    if [ $? -eq 0 ]; then
        break
    fi

    echo "Training interrupted, restarting in 10s..."
    sleep 10
done
```

### í•™ìŠµ ëŠë¦¼

```bash
# DataLoader workers ëŠ˜ë¦¬ê¸°
num_workers=4  # ë˜ëŠ” 8

# Pin memory ì‚¬ìš©
pin_memory=True

# Prefetch factor
prefetch_factor=2
```

---

## ë‹¤ìŒ ë‹¨ê³„

í•™ìŠµ ì™„ë£Œ í›„:

1. **ëª¨ë¸ ê²€ì¦**: `scripts/evaluate.py`ë¡œ ì„±ëŠ¥ í‰ê°€
2. **ì¶”ë¡  í…ŒìŠ¤íŠ¸**: `server/inference_server.py`ë¡œ ìƒì„± í…ŒìŠ¤íŠ¸
3. **FL Studio í†µí•©**: `docs/FL_STUDIO_GUIDE.md` ì°¸ê³ 

---

**Questions?** GitHub Issuesì— ë¬¸ì˜í•´ì£¼ì„¸ìš”!
