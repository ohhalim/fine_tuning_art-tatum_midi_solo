# í•™ìŠµ ë¡œë“œë§µ: "ë‚˜ + AI(ë‚˜) = JAM!" ì‹¤í˜„í•˜ê¸°

**ëª©í‘œ**: ë‚´ ì—°ì£¼ ìŠ¤íƒ€ì¼ì„ í•™ìŠµí•œ AIì™€ ì‹¤ì‹œê°„ ì¦‰í¥ì—°ì£¼í•˜ëŠ” ì‹œìŠ¤í…œ êµ¬ì¶•

**ê¸°ê°„**: 3ê°œì›” (ì§‘ì¤‘ í•™ìŠµ) + 3ê°œì›” (êµ¬í˜„ & ì‹¤í—˜)

---

## ğŸ¯ ìµœì¢… ëª©í‘œ ë¶„í•´

```
ë‚˜ì™€ ê°€ìƒì˜ ë‚´ê°€ JAM!
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. AIê°€ ë‚´ ìŠ¤íƒ€ì¼ í•™ìŠµ                  â”‚
â”‚    â†’ Fine-tuning ê¸°ìˆ  í•„ìš”              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. ì‹¤ì‹œê°„ ìƒì„±                          â”‚
â”‚    â†’ Real-time generation ì´í•´ í•„ìš”     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. MIDIë¡œ ì‘ë™                          â”‚
â”‚    â†’ Audio â†’ MIDI ë³€í™˜ ê¸°ìˆ  í•„ìš”        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. ìƒí˜¸ì‘ìš©                             â”‚
â”‚    â†’ Audio/MIDI injection êµ¬í˜„ í•„ìš”     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š Phase 1: ê¸°ì´ˆ ì´ë¡  (2ì£¼)

### 1.1 Transformer ì™„ë²½ ì´í•´ â­â­â­

**ì™œ ë°°ì›Œì•¼?**
- Magenta RTì˜ í•µì‹¬ = Transformer
- Fine-tuning ì´í•´í•˜ë ¤ë©´ ê¸°ë³¸ êµ¬ì¡° í•„ìˆ˜
- ëª¨ë“  LLM/ìƒì„± ëª¨ë¸ì˜ ê¸°ì´ˆ

**ë¬´ì—‡ì„ ë°°ìš¸ê¹Œ?**

```python
# 1. Self-Attention ë©”ì»¤ë‹ˆì¦˜
Q, K, V = query, key, value
Attention(Q,K,V) = softmax(QK^T / âˆšd_k)V

# ì™œ ì¤‘ìš”?
# â†’ ìŒì•…ì˜ "ë§¥ë½"ì„ ì´í•´í•˜ëŠ” í•µì‹¬!
# â†’ "ì´ì „ 4ë§ˆë””ë¥¼ ê¸°ì–µí•˜ê³  ë‹¤ìŒ í”„ë ˆì´ì¦ˆ ìƒì„±"

# 2. Multi-Head Attention
# ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— ë¶„ì„
head_1 = Attention(Q1, K1, V1)  # ë©œë¡œë”” ê´€ê³„
head_2 = Attention(Q2, K2, V2)  # í™”ì„± ê´€ê³„
head_3 = Attention(Q3, K3, V3)  # ë¦¬ë“¬ ê´€ê³„

# 3. Positional Encoding
# í† í°ì˜ ìˆœì„œ ì •ë³´ ì¶”ê°€
# â†’ ìŒì•…ì—ì„œ íƒ€ì´ë°ì´ ì¤‘ìš”!

# 4. Encoder-Decoder êµ¬ì¡°
Encoder: ì…ë ¥ ì²˜ë¦¬ (ê³¼ê±° 10ì´ˆ ìŒì•…)
Decoder: ìƒì„± (ë‹¤ìŒ 2ì´ˆ ì˜ˆì¸¡)
```

**í•™ìŠµ ë¦¬ì†ŒìŠ¤**:
```
1. ë…¼ë¬¸ ì½ê¸°:
   - "Attention Is All You Need" (Vaswani et al.)
   - PAPERS_TO_READ.md ì°¸ì¡°

2. ì½”ë“œ ì‹¤ìŠµ:
   - PyTorch Transformer tutorial
   - HuggingFace Transformers ê¸°ë³¸ ì˜ˆì œ

3. ì‹œê°í™”:
   - http://jalammar.github.io/illustrated-transformer/
   - í•œêµ­ì–´: Transformer ì„¤ëª… ë¸”ë¡œê·¸ë“¤

4. ì‹¤ìŠµ ê³¼ì œ:
   - ê°„ë‹¨í•œ seq2seq Transformer êµ¬í˜„
   - Text generationìœ¼ë¡œ ë¨¼ì € ì—°ìŠµ
   - ê·¸ ë‹¤ìŒ MIDI sequence generation
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] Self-attention ìˆ˜ì‹ ìœ ë„ ê°€ëŠ¥
- [ ] Multi-head attention êµ¬ì¡° ê·¸ë¦´ ìˆ˜ ìˆìŒ
- [ ] PyTorchë¡œ ê°„ë‹¨í•œ Transformer êµ¬í˜„
- [ ] Positional encoding í•„ìš”ì„± ì„¤ëª… ê°€ëŠ¥
- [ ] Encoder-decoder ì°¨ì´ ëª…í™•íˆ ì´í•´

**ì˜ˆìƒ ì‹œê°„**: 1ì£¼ (ë§¤ì¼ 2-3ì‹œê°„)

---

### 1.2 Audio/MIDI Tokenization â­â­â­

**ì™œ ë°°ì›Œì•¼?**
- ì˜¤ë””ì˜¤ë¥¼ Transformerê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜
- SpectroStream (Audio) ì´í•´ â†’ MIDI tokenizer ê°œë°œ

**ë¬´ì—‡ì„ ë°°ìš¸ê¹Œ?**

```python
# 1. Audio Tokenization (SpectroStream)

# Audio waveform â†’ Discrete tokens
audio_48khz = [48000 samples/sec Ã— 2 channels]
    â†“ Encoder (Neural codec)
audio_tokens = [25 frames/sec Ã— 64 RVQ levels]
    â†“ Decoder
reconstructed_audio â‰ˆ original_audio

# í•µì‹¬ ê°œë…:
# - RVQ (Residual Vector Quantization)
# - Codebook (vocabulary of audio patterns)
# - Perceptual loss (ì‚¬ëŒì´ ë“£ê¸°ì— ìì—°ìŠ¤ëŸ½ê²Œ)

# 2. MIDI Tokenization (ë‚´ê°€ êµ¬í˜„í•  ê²ƒ!)

# Event-based representation:
midi_events = [
    NOTE_ON(pitch=60, velocity=80, time=0.0),
    NOTE_OFF(pitch=60, time=0.5),
    NOTE_ON(pitch=64, velocity=75, time=0.5),
    NOTE_OFF(pitch=64, time=1.0),
]
    â†“ Tokenize
tokens = [
    TOKEN_NOTE_ON_60,
    TOKEN_VELOCITY_80,
    TOKEN_TIME_SHIFT_500ms,
    TOKEN_NOTE_OFF_60,
    TOKEN_NOTE_ON_64,
    ...
]

# REMI (REpresentation of MIDi):
tokens = [
    BAR_START,
    POSITION_0,
    PITCH_60,
    VELOCITY_80,
    DURATION_8,  # 8ë¶„ìŒí‘œ
    POSITION_2,
    PITCH_64,
    ...
]

# 3. ë¹„êµ: Audio vs MIDI tokens

Audio (SpectroStream):
  - 2ì´ˆ = 50 frames Ã— 64 RVQ = 3,200 tokens
  - ëª¨ë“  ìŒí–¥ ì •ë³´ í¬í•¨ (ìŒìƒ‰, ì”í–¥, ë…¸ì´ì¦ˆ)
  - ë¬´ê±°ì›€

MIDI:
  - 2ì´ˆ = ~100 events = ~100 tokens
  - Note on/off, velocity, timingë§Œ
  - ê°€ë²¼ì›€ (30ë°°!)
  - â†’ ë” ë¹ ë¥¸ ìƒì„± ê°€ëŠ¥!
```

**í•™ìŠµ ë¦¬ì†ŒìŠ¤**:
```
1. Audio Codec:
   - SoundStream paper (Google)
   - EnCodec paper (Meta)
   - SpectroStream code ë¶„ì„

2. MIDI Tokenization:
   - Miditok library ë¬¸ì„œ
   - "This Time with Feeling" (MIDI tokenization survey)
   - Music Transformer paper (REMI representation)

3. ì½”ë“œ ì‹¤ìŠµ:
   - Miditok ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©í•´ë³´ê¸°
   - MIDI íŒŒì¼ â†’ tokens â†’ MIDI ë³µì›
   - ë‹¤ì–‘í•œ tokenization ë°©ì‹ ë¹„êµ

4. ì‹¤ìŠµ ê³¼ì œ:
   - ë‚´ MIDI íŒŒì¼ tokenizeí•´ë³´ê¸°
   - Token vocabulary í¬ê¸° ë¶„ì„
   - Reconstruction í’ˆì§ˆ í‰ê°€
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] RVQ ê°œë… ì„¤ëª… ê°€ëŠ¥
- [ ] Audio codecì´ ì™œ í•„ìš”í•œì§€ ì´í•´
- [ ] MIDI event-based vs REMI ë¹„êµ ê°€ëŠ¥
- [ ] Miditokìœ¼ë¡œ MIDI tokenize/detokenize ê°€ëŠ¥
- [ ] ë‚´ í”„ë¡œì íŠ¸ì— ë§ëŠ” tokenization ë°©ì‹ ì„ íƒ

**ì˜ˆìƒ ì‹œê°„**: 3-4ì¼ (ë§¤ì¼ 2-3ì‹œê°„)

---

### 1.3 Music Generation ê¸°ì´ˆ â­â­

**ì™œ ë°°ì›Œì•¼?**
- ìŒì•… ìƒì„±ì˜ ë„ë©”ì¸ ì§€ì‹
- Magenta RTê°€ í•´ê²°í•œ ë¬¸ì œë“¤ ì´í•´

**ë¬´ì—‡ì„ ë°°ìš¸ê¹Œ?**

```python
# 1. Music Language Modeling

# Text LMê³¼ ìœ ì‚¬:
"The cat sat on the ___" â†’ "mat" (ì˜ˆì¸¡)

# Music LM:
[C, E, G, ___ ] â†’ "C" or "E" (ì˜ˆì¸¡)
# í•˜ì§€ë§Œ ìŒì•…ì€:
# - Polyphonic (ì—¬ëŸ¬ ìŒì´ ë™ì‹œì—)
# - Hierarchical (ë©œë¡œë”” + í™”ì„± + ë¦¬ë“¬)
# - Long-term structure (16ë§ˆë””, 32ë§ˆë”” êµ¬ì¡°)

# 2. ìŒì•… ìƒì„±ì˜ ë„ì „ê³¼ì œ

# Challenge 1: Long-term coherence
# â†’ 4ë§ˆë””ëŠ” ê´œì°®ì€ë° 32ë§ˆë””ëŠ” ì‚°ë§Œí•´ì§
# â†’ Solution: Hierarchical generation, Planning

# Challenge 2: Multiple attributes
# â†’ ë©œë¡œë””, í™”ì„±, ë¦¬ë“¬ ë™ì‹œ ì»¨íŠ¸ë¡¤
# â†’ Solution: Multi-conditioning, Disentanglement

# Challenge 3: Evaluation
# â†’ "ì¢‹ì€ ìŒì•…"ì„ ì–´ë–»ê²Œ ì¸¡ì •?
# â†’ Solution: Perplexity, ì‚¬ëŒ í‰ê°€, Musicality metrics

# 3. Conditioning (ì»¨ë””ì…”ë‹)

# Unconditional:
model.generate()  # ëœë¤ ìƒì„±

# Conditional:
model.generate(
    genre="jazz",
    tempo=120,
    key="C major",
    style_embedding=my_style
)

# 4. Sampling strategies

# Greedy: í•­ìƒ ê°€ì¥ í™•ë¥  ë†’ì€ ê²ƒ
# â†’ ì•ˆì „í•˜ì§€ë§Œ ì§€ë£¨í•¨

# Top-k: ìƒìœ„ kê°œ ì¤‘ ìƒ˜í”Œë§
# â†’ ì ë‹¹í•œ ë‹¤ì–‘ì„±

# Temperature: í™•ë¥  ë¶„í¬ ì¡°ì ˆ
# temp=0.1: ë³´ìˆ˜ì  (ì•ˆì „)
# temp=1.0: í‘œì¤€
# temp=2.0: ëª¨í—˜ì  (ëœë¤)
```

**í•™ìŠµ ë¦¬ì†ŒìŠ¤**:
```
1. ë…¼ë¬¸:
   - Music Transformer (Google Magenta)
   - MuseNet (OpenAI)
   - Jukebox (OpenAI)

2. ì‹¤ìŠµ:
   - Magenta.js ì›¹ ë°ëª¨ë“¤ ì²´í—˜
   - Music Transformer Colab ì‹¤í–‰
   - ë‹¤ì–‘í•œ sampling parameter ì‹¤í—˜

3. ìŒì•… ì´ë¡ :
   - ê¸°ë³¸ í™”ì„±í•™ (ì½”ë“œ ì§„í–‰)
   - ë¦¬ë“¬ íŒ¨í„´
   - ê³¡ êµ¬ì¡° (AABA, verse-chorus)
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] Music LM vs Text LM ì°¨ì´ ì„¤ëª… ê°€ëŠ¥
- [ ] Polyphonic music ìƒì„±ì˜ ì–´ë ¤ì›€ ì´í•´
- [ ] Conditioning ë°©ì‹ë“¤ ë¹„êµ ê°€ëŠ¥
- [ ] Temperature, top-kì˜ íš¨ê³¼ ì²´í—˜
- [ ] ìŒì•… ìƒì„± ìƒ˜í”Œ í’ˆì§ˆ í‰ê°€ ê°€ëŠ¥

**ì˜ˆìƒ ì‹œê°„**: 3-4ì¼

---

## ğŸ“š Phase 2: Magenta RealTime ê¹Šì´ ì´í•´ (2ì£¼)

### 2.1 Magenta RT Architecture ì™„ë²½ ë¶„í•´ â­â­â­

**ì™œ ë°°ì›Œì•¼?**
- ì´ ì‹œìŠ¤í…œì„ MIDIë¡œ ê°œì¡°í•´ì•¼ í•¨
- ê° ì»´í¬ë„ŒíŠ¸ì˜ ì—­í•  ì´í•´ í•„ìˆ˜

**ë¬´ì—‡ì„ ë°°ìš¸ê¹Œ?**

```python
# ì „ì²´ íŒŒì´í”„ë¼ì¸ ë¶„í•´

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Component 1: MusicCoCa (Style Embedding)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MusicCoCa:
    """
    ëª©ì : Textì™€ Audioë¥¼ ê°™ì€ ê³µê°„ì— ì„ë² ë”©

    ì™œ ì¤‘ìš”?
    - "jazz piano" (text) = my_recording.wav (audio)
    - ë‚´ ì—°ì£¼ë¥¼ promptë¡œ ì‚¬ìš© ê°€ëŠ¥!
    """

    def __init__(self):
        self.audio_encoder = ViT(12_layers)  # Vision Transformer
        self.text_encoder = Transformer(12_layers)
        self.text_decoder = Transformer(3_layers)  # Regularization

    def encode_audio(self, audio_10s):
        # Audio â†’ Log-mel spectrogram
        spectrogram = to_mel(audio_10s)  # 128 channels Ã— 992 frames

        # ViTë¡œ ì²˜ë¦¬
        embedding_768d = self.audio_encoder(spectrogram)

        # Quantize to 12 tokens
        tokens_12 = self.quantize(embedding_768d)
        return tokens_12

    def encode_text(self, text):
        # Text â†’ Tokens
        text_tokens = tokenize(text)  # max 128 tokens

        # Transformerë¡œ ì²˜ë¦¬
        embedding_768d = self.text_encoder(text_tokens)

        # Quantize to 12 tokens
        tokens_12 = self.quantize(embedding_768d)
        return tokens_12

    def blend_prompts(self, prompts):
        """Multiple prompts weighted average"""
        embeddings = []
        weights = []

        for prompt, weight in prompts:
            if isinstance(prompt, str):
                emb = self.encode_text(prompt)
            else:
                emb = self.encode_audio(prompt)
            embeddings.append(emb)
            weights.append(weight)

        # Weighted average
        blended = sum(w * e for w, e in zip(weights, embeddings))
        blended = blended / sum(weights)
        return blended

# ë°°ì›Œì•¼ í•  ê²ƒ:
# - Contrastive learning (CoCa)
# - Joint embedding space
# - Quantization (768D â†’ 12 tokens)
# - Attention pooling

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Component 2: SpectroStream (Audio Codec)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SpectroStream:
    """
    ëª©ì : Audio â†” Discrete tokens

    ì™œ ì¤‘ìš”?
    - TransformerëŠ” discrete tokens ì²˜ë¦¬
    - Continuous audioë¥¼ language-likeë¡œ ë³€í™˜
    """

    def encode(self, audio_2s):
        # Audio â†’ Latent
        latent = self.encoder_nn(audio_2s)

        # Latent â†’ RVQ tokens
        tokens = []
        residual = latent
        for level in range(64):  # 64 RVQ levels
            codes, residual = self.vq_layers[level](residual)
            tokens.append(codes)

        # Shape: [50 frames, 64 RVQ levels]
        # â†’ 3,200 tokens for 2 seconds
        return tokens

    def decode(self, tokens):
        # RVQ tokens â†’ Latent
        latent = 0
        for level, codes in enumerate(tokens):
            latent += self.vq_layers[level].lookup(codes)

        # Latent â†’ Audio
        audio = self.decoder_nn(latent)
        return audio

# ë°°ì›Œì•¼ í•  ê²ƒ:
# - RVQ (Residual Vector Quantization)
# - Codebook learning
# - Perceptual loss
# - Hierarchical structure (coarse â†’ fine)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Component 3: Encoder-Decoder Transformer
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MagentaRTTransformer:
    """
    ëª©ì : ê³¼ê±° context â†’ ë‹¤ìŒ 2ì´ˆ ìƒì„±
    """

    def __init__(self, config):
        self.encoder = T5Encoder(config)  # Bidirectional
        self.decoder = T5Decoder(config)  # Causal

    def generate_chunk(self, history_10s, style_12tokens):
        """
        Inputs:
        - history_10s: 5 chunks Ã— 50 frames Ã— 4 RVQ = 1000 tokens
        - style_12tokens: 12 tokens

        Output:
        - next_chunk: 50 frames Ã— 16 RVQ = 800 tokens
        """

        # 1. Encoder: Process context
        encoder_input = torch.cat([
            history_10s,     # 1000 tokens (coarse)
            style_12tokens   # 12 tokens
        ])  # Total: 1012 tokens

        encoder_output = self.encoder(encoder_input)

        # 2. Decoder: Generate next chunk
        # Two-stage architecture:

        # Stage 1: Temporal module (frame-level)
        temporal_context = []
        for frame_idx in range(50):  # 2s = 50 frames
            frame_emb = self.temporal_module(
                encoder_output,
                frame_idx
            )
            temporal_context.append(frame_emb)

        # Stage 2: Depth module (RVQ-level)
        chunk_tokens = []
        for frame_idx in range(50):
            frame_tokens = []
            for rvq_level in range(16):
                token = self.depth_module(
                    temporal_context[frame_idx],
                    previous_rvq_tokens=frame_tokens
                )
                frame_tokens.append(token)
            chunk_tokens.append(frame_tokens)

        return chunk_tokens  # [50, 16]

# ë°°ì›Œì•¼ í•  ê²ƒ:
# - T5 architecture
# - Bidirectional vs Causal attention
# - Two-stage decoding (temporal + depth)
# - KV-cache for efficiency

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Component 4: Chunk-based Generation Loop
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class StreamingGenerator:
    """
    ëª©ì : ë¬´í•œ ìŠ¤íŠ¸ë¦¼ ìƒì„±
    """

    def generate_stream(self, initial_style):
        state = {
            'chunks': [],  # ìµœê·¼ 5 chunks ìœ ì§€
            'style': initial_style
        }

        while True:  # ë¬´í•œ ìƒì„±!
            # 1. ê³¼ê±° 10ì´ˆ ì¶”ì¶œ (coarse)
            if len(state['chunks']) < 5:
                # Cold start: padding
                history = pad_to_5chunks(state['chunks'])
            else:
                # ìµœê·¼ 5 chunks
                history = state['chunks'][-5:]

            # Coarse context (4 RVQ levelsë§Œ)
            history_coarse = [
                chunk[:, :, :4]  # [frames, RVQ] â†’ [frames, 4]
                for chunk in history
            ]

            # 2. ë‹¤ìŒ 2ì´ˆ ìƒì„± (16 RVQ levels)
            next_chunk = self.model.generate_chunk(
                history_coarse,
                state['style']
            )

            # 3. Audioë¡œ ë³€í™˜ & ì¬ìƒ
            audio_2s = self.codec.decode(next_chunk)
            play(audio_2s)

            # 4. State ì—…ë°ì´íŠ¸
            state['chunks'].append(next_chunk)

            # Sliding window (ìµœëŒ€ 5 chunks)
            if len(state['chunks']) > 5:
                state['chunks'].pop(0)

            # 5. Style ì—…ë°ì´íŠ¸ (ì‚¬ìš©ìê°€ ë³€ê²½í–ˆë‹¤ë©´)
            if user_changed_style:
                state['style'] = new_style

# ë°°ì›Œì•¼ í•  ê²ƒ:
# - Stateless generation
# - Sliding window context
# - Cold start handling
# - Style transition smoothing
```

**í•™ìŠµ ë¦¬ì†ŒìŠ¤**:
```
1. ë…¼ë¬¸ ì •ë…:
   - Live Music Models (arxiv 2508.04651)
   - LIVE_MUSIC_MODELS_ANALYSIS.md

2. ì½”ë“œ ë¶„ì„:
   - github.com/magenta/magenta-realtime
   - ê° ì»´í¬ë„ŒíŠ¸ ì½”ë“œ ì½ê¸°
   - ë°ì´í„° í”Œë¡œìš° ì¶”ì 

3. Colab ì‹¤ìŠµ:
   - ê³µì‹ Colab ë°ëª¨ ì‹¤í–‰
   - ê° ë‹¨ê³„ë³„ intermediate ê²°ê³¼ ì¶œë ¥
   - Parameter ë³€ê²½í•´ë³´ë©° íš¨ê³¼ ê´€ì°°

4. ì‹œê°í™”:
   - Architecture diagram ì§ì ‘ ê·¸ë¦¬ê¸°
   - Data flow ì°¨íŠ¸ ì‘ì„±
   - Tensor shapes ì¶”ì 
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] MusicCoCaì˜ 3ê°œ ì»´í¬ë„ŒíŠ¸ ì„¤ëª… ê°€ëŠ¥
- [ ] SpectroStream encoding/decoding ê³¼ì • ì´í•´
- [ ] Two-stage decoder êµ¬ì¡° ê·¸ë¦´ ìˆ˜ ìˆìŒ
- [ ] Chunk-based generation loop ì½”ë“œ ì‘ì„± ê°€ëŠ¥
- [ ] Coarse context vs full generation ì°¨ì´ ì„¤ëª… ê°€ëŠ¥
- [ ] ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ìˆœì„œë„ë¡œ ê·¸ë¦´ ìˆ˜ ìˆìŒ

**ì˜ˆìƒ ì‹œê°„**: 1ì£¼ (ë§¤ì¼ 3-4ì‹œê°„)

---

### 2.2 Real-time Generation ê¸°ìˆ  â­â­â­

**ì™œ ë°°ì›Œì•¼?**
- ì‹¤ì‹œê°„ ë“€ì—£ì˜ í•µì‹¬ = ë‚®ì€ ë ˆì´í„´ì‹œ
- RTF (Real-Time Factor) ìµœì í™” í•„ìš”

**ë¬´ì—‡ì„ ë°°ìš¸ê¹Œ?**

```python
# 1. Latency ë¶„ì„

total_latency = (
    encoding_time +      # Audio â†’ Tokens
    model_inference +    # Tokens â†’ Next tokens
    decoding_time +      # Tokens â†’ Audio
    audio_buffer         # ì¬ìƒ ë²„í¼
)

# ëª©í‘œ:
# Audio: ~800ms (Magenta RT)
# MIDI: <50ms (ìš°ë¦¬ ëª©í‘œ!)

# 2. Optimization ê¸°ë²•ë“¤

# â•â•â• Model Optimization â•â•â•

# A. Quantization
model_fp32 = load_model()  # 32-bit float
model_fp16 = quantize(model_fp32, 'fp16')  # 16-bit: 2ë°° ë¹ ë¦„
model_int8 = quantize(model_fp32, 'int8')  # 8-bit: 4ë°° ë¹ ë¦„

# B. KV-Cache (Transformer ìµœì í™”)
class TransformerWithCache:
    def forward(self, x, cache=None):
        if cache is None:
            # First step: compute all
            k = self.compute_keys(x)
            v = self.compute_values(x)
        else:
            # Subsequent steps: reuse cache
            k = torch.cat([cache['k'], self.compute_keys(x[-1:])])
            v = torch.cat([cache['v'], self.compute_values(x[-1:])])

        attention = self.attention(q, k, v)
        return attention, {'k': k, 'v': v}

# â†’ ë§¤ë²ˆ ì „ì²´ ê³„ì‚° ì•ˆ í•¨! (10ë°° ë¹ ë¦„)

# C. Model Compilation
import torch
model = torch.compile(model, mode='reduce-overhead')
# â†’ PyTorch 2.0 compilation (2ë°° ë¹ ë¦„)

# D. Batch Size = 1
# Real-timeì—ì„œëŠ” batching ë¶ˆê°€ëŠ¥
# â†’ Single sample inference ìµœì í™” í•„ìš”

# â•â•â• Hardware Optimization â•â•â•

# A. GPU Selection
# RTX 3060 (8GB): ê´œì°®ìŒ
# RTX 3090 (24GB): ì™„ë²½
# TPU v2-8: Colab ë¬´ë£Œ!

# B. Mixed Precision Training
from torch.cuda.amp import autocast
with autocast():
    output = model(input)
# â†’ FP16 ì—°ì‚°ìœ¼ë¡œ 2ë°° ë¹ ë¦„

# â•â•â• Algorithmic Optimization â•â•â•

# A. Coarse Context (Magenta RT í•µì‹¬!)
# Context: 4 RVQ levels (coarse)
# Generation: 16 RVQ levels (fine)
# â†’ 4ë°° ë©”ëª¨ë¦¬ ì ˆì•½, ì†ë„ í–¥ìƒ

# B. Chunk Size Tuning
# ì‘ì€ chunk: ë‚®ì€ latency, ë¶ˆì•ˆì •
# í° chunk: ë†’ì€ latency, ì•ˆì •ì 
# Magenta RT: 2ì´ˆ (ìµœì  ì§€ì )

# 3. MIDIì˜ ì´ì  (ìš°ë¦¬ í”„ë¡œì íŠ¸!)

# Audio tokenization:
audio_2s = 50 frames Ã— 64 RVQ = 3,200 tokens
encoding_time = 50ms
decoding_time = 100ms

# MIDI tokenization:
midi_2s = ~100 events = 100 tokens
encoding_time = 5ms (10ë°° ë¹ ë¦„!)
decoding_time = 10ms (10ë°° ë¹ ë¦„!)

# â†’ Total latency: 800ms â†’ <50ms ê°€ëŠ¥!

# 4. Profiling & Benchmarking

import time

def profile_model():
    times = {
        'encoding': [],
        'model': [],
        'decoding': []
    }

    for _ in range(100):
        # Encoding
        t0 = time.time()
        tokens = encode(audio)
        times['encoding'].append(time.time() - t0)

        # Model inference
        t0 = time.time()
        output = model(tokens)
        times['model'].append(time.time() - t0)

        # Decoding
        t0 = time.time()
        audio = decode(output)
        times['decoding'].append(time.time() - t0)

    # ë¶„ì„
    for stage, ts in times.items():
        print(f"{stage}: {np.mean(ts)*1000:.1f}ms Â± {np.std(ts)*1000:.1f}ms")

    # RTF ê³„ì‚°
    chunk_duration = 2.0  # seconds
    total_time = sum(np.mean(ts) for ts in times.values())
    rtf = chunk_duration / total_time
    print(f"RTF: {rtf:.2f}x")

# 5. Cold Start ë¬¸ì œ

# ì²« ë²ˆì§¸ chunk ìƒì„± ì‹œ:
# - Contextê°€ ì—†ìŒ
# - Modelì„ GPUë¡œ ë¡œë”©
# - ì²« inferenceëŠ” ëŠë¦¼

# í•´ê²°:
def warm_up_model(model):
    """Model warm-upìœ¼ë¡œ ì²« latency ì¤„ì´ê¸°"""
    dummy_input = torch.zeros(1, 1012).to('cuda')
    for _ in range(5):
        _ = model(dummy_input)
    # â†’ ì´í›„ inferenceëŠ” ë¹ ë¦„!
```

**í•™ìŠµ ë¦¬ì†ŒìŠ¤**:
```
1. ìµœì í™” ê¸°ìˆ :
   - PyTorch Performance Tuning Guide
   - NVIDIA TensorRT ë¬¸ì„œ
   - torch.compile ê°€ì´ë“œ

2. Profiling ë„êµ¬:
   - PyTorch Profiler
   - NVIDIA Nsight
   - Python cProfile

3. ì‹¤ìŠµ:
   - Magenta RT ì½”ë“œ profiling
   - ê° ë‹¨ê³„ë³„ ì‹œê°„ ì¸¡ì •
   - Bottleneck ì°¾ê¸°
   - Optimization ì ìš© & ì¬ì¸¡ì •

4. ë²¤ì¹˜ë§ˆí¬:
   - ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸° ë¹„êµ
   - GPUë³„ ì„±ëŠ¥ ë¹„êµ
   - Quantization íš¨ê³¼ ì¸¡ì •
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] Latency êµ¬ì„± ìš”ì†Œ ì„¤ëª… ê°€ëŠ¥
- [ ] RTF ê³„ì‚° ë° í•´ì„ ê°€ëŠ¥
- [ ] Quantization (FP16, INT8) ì ìš© ê°€ëŠ¥
- [ ] KV-cache êµ¬í˜„ ì´í•´
- [ ] ëª¨ë¸ profiling ì‹¤í–‰ ê°€ëŠ¥
- [ ] Optimization ì „í›„ ì„±ëŠ¥ ë¹„êµ ê°€ëŠ¥
- [ ] MIDIê°€ Audioë³´ë‹¤ ë¹ ë¥¸ ì´ìœ  ì„¤ëª… ê°€ëŠ¥

**ì˜ˆìƒ ì‹œê°„**: 4-5ì¼

---

### 2.3 Audio Injection ë©”ì»¤ë‹ˆì¦˜ â­â­

**ì™œ ë°°ì›Œì•¼?**
- ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©ì˜ í•µì‹¬!
- MIDI injection êµ¬í˜„ ì‹œ í•„ìš”

**ë¬´ì—‡ì„ ë°°ìš¸ê¹Œ?**

```python
# Audio Injectionì˜ ì‘ë™ ì›ë¦¬

class AudioInjectionGenerator:
    """
    ëª©ì : ì‚¬ìš©ì ì…ë ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë¸ì— ì£¼ì…

    í•µì‹¬ ì•„ì´ë””ì–´:
    1. ì‚¬ìš©ì audioë¥¼ ìº¡ì²˜
    2. ëª¨ë¸ outputê³¼ ë¯¹ì‹±
    3. ë¯¹ì‹±ëœ ê²ƒì„ ë‹¤ìŒ contextë¡œ ì‚¬ìš©
    4. ëª¨ë¸ì´ ì‚¬ìš©ì ì…ë ¥ì— "ë°˜ì‘"
    """

    def __init__(self):
        self.model = MagentaRT()
        self.codec = SpectroStream()
        self.audio_buffer = []

    def inject_and_generate(self, user_audio, mix_ratio=0.3):
        """
        Args:
            user_audio: ì‚¬ìš©ìê°€ ì—°ì£¼í•œ audio (2ì´ˆ)
            mix_ratio: 0.0-1.0, ì‚¬ìš©ì audio ë¹„ìœ¨
        """

        # 1. í˜„ì¬ context (ê³¼ê±° 10ì´ˆ)
        context = self.get_context()

        # 2. ëª¨ë¸ë¡œ ë‹¤ìŒ 2ì´ˆ ìƒì„±
        model_output = self.model.generate_chunk(context)
        model_audio = self.codec.decode(model_output)

        # 3. ì‚¬ìš©ì audioì™€ ë¯¹ì‹±
        mixed_audio = (
            mix_ratio * user_audio +
            (1 - mix_ratio) * model_audio
        )

        # âš ï¸ ì¤‘ìš”: ì‚¬ìš©ì audioëŠ” ì§ì ‘ ì¬ìƒ ì•ˆ ë¨!
        # ëŒ€ì‹  mixed_audioë¥¼ ë‹¤ìŒ contextë¡œ ì‚¬ìš©

        # 4. Mixed audioë¥¼ tokenize
        mixed_tokens = self.codec.encode(mixed_audio)

        # 5. Context ì—…ë°ì´íŠ¸
        self.audio_buffer.append(mixed_tokens)

        # 6. Model output ì¬ìƒ (not mixed!)
        play(model_audio)

        return model_audio

# ì™œ ì´ë ‡ê²Œ ë³µì¡í•˜ê²Œ?

# â•â•â• Naive approach (ì•ˆ ì¢‹ìŒ) â•â•â•
# User plays â†’ Encode â†’ Add to prompt
# â†’ ëª¨ë¸ì´ ì‚¬ìš©ì ì…ë ¥ì„ "ë”°ë¼í•˜ê¸°ë§Œ" í•¨
# â†’ ë°˜ì‘ì´ ì•„ë‹Œ ëª¨ë°©

# â•â•â• Audio injection (ì¢‹ìŒ!) â•â•â•
# User plays â†’ Mix with model output â†’ Context
# â†’ ëª¨ë¸ì´ "ë‚´ê°€ ë°©ê¸ˆ ì‚¬ìš©ìì™€ í•¨ê»˜ ì—°ì£¼í–ˆë‹¤"ê³  ì¸ì‹
# â†’ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”/ë°˜ì‘

# ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ:

def interactive_session():
    generator = AudioInjectionGenerator()

    while True:
        # 1. ì‚¬ìš©ì ì…ë ¥ ìº¡ì²˜ (ë§ˆì´í¬)
        user_audio = capture_from_mic(duration=2.0)

        # 2. AI ìƒì„± + ë¯¹ì‹±
        ai_audio = generator.inject_and_generate(
            user_audio,
            mix_ratio=0.3  # 30% ì‚¬ìš©ì, 70% AI
        )

        # 3. AIë§Œ ì¬ìƒ (ì‚¬ìš©ìëŠ” ì´ë¯¸ ë“¤ìŒ)
        play_to_speaker(ai_audio)

        # 4. ë‹¤ìŒ ë°˜ë³µ
        # â†’ AIëŠ” "user + ì´ì „ AI"ë¥¼ contextë¡œ ë³¸ ìƒíƒœ
        # â†’ ì‚¬ìš©ì ì…ë ¥ì— ì˜í–¥ë°›ì€ ë‹¤ìŒ ì¶œë ¥!

# MIDI Injection êµ¬ìƒ (ìš°ë¦¬ê°€ êµ¬í˜„í•  ê²ƒ!)

class MIDIInjectionGenerator:
    """
    Audio injection â†’ MIDI injection ë³€í™˜
    """

    def inject_and_generate(self, user_midi_events, mix_ratio=0.3):
        """
        Args:
            user_midi_events: ì‚¬ìš©ì MIDI ì…ë ¥ (2ì´ˆ)
            mix_ratio: ì‚¬ìš©ì ì…ë ¥ ë°˜ì˜ ë¹„ìœ¨
        """

        # 1. Context
        context = self.get_midi_context()  # ê³¼ê±° 10ì´ˆ MIDI

        # 2. AI ìƒì„±
        ai_midi = self.model.generate_chunk(context)

        # 3. "ë¯¹ì‹±" (MIDIì˜ ê²½ìš°)
        # Audioì²˜ëŸ¼ ë‹¨ìˆœ mix ë¶ˆê°€ëŠ¥
        # â†’ ë‘ ê°€ì§€ ë°©ë²•:

        # Method A: Interleaving (êµì°¨)
        mixed_midi = []
        for user_note, ai_note in zip(user_midi_events, ai_midi):
            if random.random() < mix_ratio:
                mixed_midi.append(user_note)
            else:
                mixed_midi.append(ai_note)

        # Method B: Harmonic blending
        mixed_midi = []
        for user_note, ai_note in zip(user_midi_events, ai_midi):
            # ì‚¬ìš©ì ìŒì— AIê°€ í™”ì„± ì¶”ê°€
            mixed_midi.append(user_note)
            if is_harmonically_compatible(user_note, ai_note):
                mixed_midi.append(ai_note)

        # Method C: Velocity blending
        mixed_midi = []
        for t in time_steps:
            user_notes = get_notes_at(user_midi_events, t)
            ai_notes = get_notes_at(ai_midi, t)

            # ì‚¬ìš©ì ìŒ: ê·¸ëŒ€ë¡œ
            # AI ìŒ: velocity ë‚®ì¶°ì„œ background
            for note in user_notes:
                mixed_midi.append(note)
            for note in ai_notes:
                if note not in user_notes:
                    note.velocity *= 0.5  # ë°°ê²½ìœ¼ë¡œ
                    mixed_midi.append(note)

        # 4. Context ì—…ë°ì´íŠ¸
        self.midi_buffer.append(mixed_midi)

        # 5. AIë§Œ ì¶œë ¥
        return ai_midi

# í•µì‹¬ í†µì°°:

# Audio injection:
# - ë¬¼ë¦¬ì  ë¯¹ì‹± ê°€ëŠ¥ (waveform ë”í•˜ê¸°)
# - ìì—°ìŠ¤ëŸ¬ì›€

# MIDI injection:
# - ë¬¼ë¦¬ì  ë¯¹ì‹± ë¶ˆê°€ëŠ¥ (discrete events)
# - ìŒì•…ì  ê·œì¹™ í•„ìš” (í™”ì„±, íƒ€ì´ë°)
# - ë” ì°½ì˜ì  ì ‘ê·¼ í•„ìš”!

# ë°°ì›Œì•¼ í•  ê²ƒ:
# 1. Audio mixing ì›ë¦¬
# 2. Context management (sliding window)
# 3. MIDI event merging ì „ëµ
# 4. Harmonic compatibility íŒë‹¨
# 5. Real-time MIDI routing
```

**í•™ìŠµ ë¦¬ì†ŒìŠ¤**:
```
1. Audio processing:
   - Librosa ë¼ì´ë¸ŒëŸ¬ë¦¬
   - Audio mixing ê¸°ì´ˆ
   - Real-time audio I/O (PyAudio)

2. MIDI processing:
   - Mido ë¼ì´ë¸ŒëŸ¬ë¦¬
   - Real-time MIDI I/O
   - MIDI event timing

3. ì‹¤ìŠµ:
   - Magenta RT audio injection ë°ëª¨
   - ì§ì ‘ audio mixing ì½”ë“œ ì‘ì„±
   - MIDI event merging ì‹¤í—˜
   - Harmonic compatibility í•¨ìˆ˜ ì‘ì„±

4. ìŒì•… ì´ë¡ :
   - í™”ì„± ì´ë¡  (ì–´ë–¤ ìŒì´ í•¨ê»˜ ì–´ìš¸ë¦¬ëŠ”ê°€)
   - Voicing (ìŒ ë°°ì¹˜)
   - Call-response íŒ¨í„´
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] Audio injection ì‘ë™ ì›ë¦¬ ì„¤ëª… ê°€ëŠ¥
- [ ] ì™œ mixingì´ í•„ìš”í•œì§€ ì´í•´
- [ ] Audio mixing ì½”ë“œ ì‘ì„± ê°€ëŠ¥
- [ ] MIDI event merging 3ê°€ì§€ ë°©ë²• ì„¤ëª… ê°€ëŠ¥
- [ ] Harmonic compatibility íŒë‹¨ í•¨ìˆ˜ ì‘ì„±
- [ ] Real-time MIDI I/O ì½”ë“œ ì‘ì„± ê°€ëŠ¥

**ì˜ˆìƒ ì‹œê°„**: 3-4ì¼

---

## ğŸ“š Phase 3: Fine-tuning ê¸°ìˆ  (1.5ì£¼)

### 3.1 Transfer Learning & Fine-tuning ê¸°ì´ˆ â­â­â­

**ì™œ ë°°ì›Œì•¼?**
- Magenta RTë¥¼ "ë‚´ ìŠ¤íƒ€ì¼"ë¡œ ë§Œë“¤ê¸°
- ì²˜ìŒë¶€í„° í•™ìŠµ vs Fine-tuning ì°¨ì´ ì´í•´

**ë¬´ì—‡ì„ ë°°ìš¸ê¹Œ?**

```python
# Transfer Learningì˜ ì² í•™

# â•â•â• From Scratch (ì²˜ìŒë¶€í„°) â•â•â•
model = MagentaRT(random_weights=True)
model.train(
    data=my_100_hours_recordings,
    epochs=1000,
    time=3_months,
    cost=$10000
)
# â†’ ë°ì´í„°, ì‹œê°„, ë¹„ìš© ì—„ì²­ë‚¨!

# â•â•â• Transfer Learning (ì „ì´ í•™ìŠµ) â•â•â•
pretrained_model = MagentaRT.from_pretrained(
    'magenta-rt-large',  # 190,000ì‹œê°„ ë°ì´í„°ë¡œ í•™ìŠµë¨!
)
my_model = pretrained_model.finetune(
    data=my_10_hours_recordings,  # 100ì‹œê°„ â†’ 10ì‹œê°„!
    epochs=50,                     # 1000 â†’ 50!
    time=1_week,                   # 3ê°œì›” â†’ 1ì£¼!
    cost=$50                       # $10000 â†’ $50!
)
# â†’ ì••ë„ì ìœ¼ë¡œ íš¨ìœ¨ì !

# ì™œ ê°€ëŠ¥í•œê°€?

# Pretrained modelì´ ì´ë¯¸ ë°°ìš´ ê²ƒ:
# - ìŒì•…ì˜ ê¸°ë³¸ êµ¬ì¡° (ë©œë¡œë””, í™”ì„±, ë¦¬ë“¬)
# - ì¼ë°˜ì ì¸ íŒ¨í„´ (ì½”ë“œ ì§„í–‰, ë¦¬ë“¬ íŒ¨í„´)
# - Audio tokenization (SpectroStream)
# - Long-term dependencies

# Fine-tuningìœ¼ë¡œ ì¶”ê°€ë¡œ ë°°ìš°ëŠ” ê²ƒ:
# - ë‚˜ë§Œì˜ voicing ìŠµê´€
# - ë‚˜ë§Œì˜ ë¦¬ë“¬ íŒ¨í„´
# - ë‚˜ë§Œì˜ í”„ë ˆì´ì¦ˆ
# - ë‚˜ë§Œì˜ "ì–¸ì–´"

# â•â•â• Fine-tuning Strategies â•â•â•

# Strategy 1: Freeze lower layers
pretrained = load_pretrained()
for layer in pretrained.encoder.layers[:8]:  # í•˜ìœ„ 8ê°œ layer
    layer.requires_grad = False  # ì–¼ë¦¼ (í•™ìŠµ ì•ˆ í•¨)

# ìƒìœ„ 4ê°œ layerë§Œ í•™ìŠµ
for layer in pretrained.encoder.layers[8:]:
    layer.requires_grad = True

# ì™œ?
# - í•˜ìœ„ layer: ì¼ë°˜ì ì¸ íŠ¹ì§• (ëª¨ë“  ìŒì•…ì— ê³µí†µ)
# - ìƒìœ„ layer: êµ¬ì²´ì ì¸ íŠ¹ì§• (ìŠ¤íƒ€ì¼ íŠ¹í™”)
# - í•˜ìœ„ëŠ” ìœ ì§€, ìƒìœ„ë§Œ ë‚´ ìŠ¤íƒ€ì¼ë¡œ!

# Strategy 2: Different learning rates
optimizer = AdamW([
    {'params': pretrained.encoder.parameters(), 'lr': 1e-5},  # ì‘ê²Œ
    {'params': pretrained.decoder.parameters(), 'lr': 1e-4},  # í¬ê²Œ
])

# ì™œ?
# - Encoder: ì•½ê°„ë§Œ ìˆ˜ì • (ì¼ë°˜ ì§€ì‹ ìœ ì§€)
# - Decoder: ë§ì´ ìˆ˜ì • (ë‚´ ìŠ¤íƒ€ì¼ í•™ìŠµ)

# Strategy 3: Gradual unfreezing
# Epoch 1-10: ìƒìœ„ 2ê°œ layerë§Œ
# Epoch 11-20: ìƒìœ„ 4ê°œ layer
# Epoch 21-30: ìƒìœ„ 6ê°œ layer
# ...
# â†’ ì ì§„ì ìœ¼ë¡œ ë” ë§ì´ í•™ìŠµ

# â•â•â• Catastrophic Forgetting ë°©ì§€ â•â•â•

# ë¬¸ì œ:
pretrained_model.quality = 90/100  # í›Œë¥­í•œ ì¼ë°˜ ìŒì•… ìƒì„±
my_finetuned.quality_on_my_style = 95/100  # ë‚´ ìŠ¤íƒ€ì¼ ì™„ë²½
my_finetuned.quality_on_general = 30/100   # ì¼ë°˜ ìŒì•…ì€ ëª» ë§Œë“¦!
# â†’ ì¼ë°˜ ëŠ¥ë ¥ì„ "ìŠì–´ë²„ë¦¼" (catastrophic forgetting)

# í•´ê²°ì±… 1: Regularization
loss = (
    style_loss(output, my_data) +           # ë‚´ ìŠ¤íƒ€ì¼ í•™ìŠµ
    0.1 * general_loss(output, general_data)  # ì¼ë°˜ ëŠ¥ë ¥ ìœ ì§€
)

# í•´ê²°ì±… 2: Small learning rate
lr = 1e-5  # ì²œì²œíˆ í•™ìŠµ (ê¸‰ê²©í•œ ë³€í™” ë°©ì§€)

# í•´ê²°ì±… 3: Early stopping
# Validation loss ì¦ê°€í•˜ë©´ ë©ˆì¶¤
# â†’ ê³¼ì í•© ë°©ì§€

# â•â•â• Data Augmentation (ë‚´ ë°ì´í„° ëŠ˜ë¦¬ê¸°) â•â•â•

# ë‚´ ë…¹ìŒ: 10ì‹œê°„ â†’ ë¶€ì¡±!
# Data augmentationìœ¼ë¡œ í™•ì¥:

def augment_midi(midi_file):
    """1ê°œ MIDI â†’ 36ê°œë¡œ ì¦ì‹!"""
    augmented = []

    # 1. Transposition (ì¡°ì˜®ê¹€)
    for semitones in range(-2, 3):  # -2 ~ +2 semitones
        transposed = transpose(midi_file, semitones)
        augmented.append(transposed)
    # â†’ 5ë°°

    # 2. Tempo variation (í…œí¬ ë³€í™”)
    for tempo_ratio in [0.9, 1.0, 1.1]:
        tempo_changed = change_tempo(midi_file, tempo_ratio)
        augmented.append(tempo_changed)
    # â†’ 3ë°°

    # 3. ì¡°í•©
    for trans in range(-2, 3):
        for tempo in [0.9, 1.0, 1.1]:
            aug = transpose(midi_file, trans)
            aug = change_tempo(aug, tempo)
            augmented.append(aug)
    # â†’ 15ë°°

    return augmented
# 10ì‹œê°„ Ã— 36 = 360ì‹œê°„ equivalent!

# ì£¼ì˜: ê³¼ë„í•œ augmentationì€ ì—­íš¨ê³¼
# - Transposition: Â±2 semitones (ìì—°ìŠ¤ëŸ¬ì›€)
# - Tempo: Â±10% (ìì—°ìŠ¤ëŸ¬ì›€)
# - ë„ˆë¬´ ë§ìœ¼ë©´ ë‚´ "ì§„ì§œ" ìŠ¤íƒ€ì¼ í¬ì„
```

**í•™ìŠµ ë¦¬ì†ŒìŠ¤**:
```
1. ì´ë¡ :
   - Transfer learning ê¸°ì´ˆ (CS231n Lecture)
   - Fine-tuning best practices
   - Catastrophic forgetting ë…¼ë¬¸

2. ì‹¤ìŠµ:
   - HuggingFace ëª¨ë¸ fine-tuning íŠœí† ë¦¬ì–¼
   - PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ˆì œ
   - ì‘ì€ ëª¨ë¸ë¡œ ì‹¤í—˜ (GPT-2 ë“±)

3. ë„ë©”ì¸ ì§€ì‹:
   - ìŒì•… ë°ì´í„° augmentation ê¸°ë²•
   - Style transfer in music
   - Personalization in generative models

4. ì‹¤ìŠµ ê³¼ì œ:
   - Pretrained ëª¨ë¸ ë¡œë“œ
   - ì¼ë¶€ layer freeze
   - ì‘ì€ ë°ì´í„°ë¡œ fine-tune
   - í’ˆì§ˆ ë¹„êµ (before/after)
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] Transfer learning ì¥ì  ì„¤ëª… ê°€ëŠ¥
- [ ] Layer freezing ì „ëµ ì´í•´
- [ ] Learning rate ì°¨ë³„í™” ì´ìœ  ì„¤ëª…
- [ ] Catastrophic forgetting ë°©ì§€ë²• 3ê°€ì§€
- [ ] Data augmentation ì½”ë“œ ì‘ì„±
- [ ] Fine-tuning ì‹¤í—˜ ì‹¤í–‰ ê°€ëŠ¥

**ì˜ˆìƒ ì‹œê°„**: 4-5ì¼

---

### 3.2 LoRA & QLoRA ì™„ë²½ ì´í•´ â­â­â­

**ì™œ ë°°ì›Œì•¼?**
- GPU ë©”ëª¨ë¦¬ ë¶€ì¡± í•´ê²°
- Fine-tuning íš¨ìœ¨ 10,000ë°° í–¥ìƒ!
- Magenta RTë¥¼ RTX 3060ì—ì„œ í•™ìŠµ ê°€ëŠ¥

**ë¬´ì—‡ì„ ë°°ìš¸ê¹Œ?**

```python
# LoRA: Low-Rank Adaptation

# â•â•â• ê¸°ì¡´ Fine-tuning (Full) â•â•â•

# Magenta RT Large: 760M parameters
# Fine-tuning: ëª¨ë“  760M íŒŒë¼ë¯¸í„° í•™ìŠµ
# GPU ë©”ëª¨ë¦¬: 40GB+ í•„ìš”
# í•™ìŠµ ì‹œê°„: ë©°ì¹ 
# ë¹„ìš©: $$$

class FullFineTuning:
    def __init__(self, pretrained_model):
        self.model = pretrained_model  # 760M params

    def train(self, my_data):
        # ëª¨ë“  íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸!
        for param in self.model.parameters():
            param.requires_grad = True  # 760Mê°œ ëª¨ë‘!

        optimizer = AdamW(self.model.parameters())
        # Optimizer state: 760M Ã— 2 = 1.5B values
        # â†’ ë©”ëª¨ë¦¬ í­ë°œ! ğŸ’¥

# â•â•â• LoRA (Low-Rank Adaptation) â•â•â•

# í•µì‹¬ ì•„ì´ë””ì–´:
# "Full fine-tuningì€ ì‚¬ì‹¤ low-rank spaceì—ì„œ ì¼ì–´ë‚œë‹¤"
# â†’ ì‹¤ì œë¡œëŠ” ì‘ì€ ë¶€ë¶„ê³µê°„ë§Œ ë³€í™”
# â†’ ê·¸ ë¶€ë¶„ê³µê°„ë§Œ í•™ìŠµí•˜ì!

class LoRALayer:
    """
    Original: W (d Ã— k)
    LoRA: W + Î”W, where Î”W = A @ B
    - A: (d Ã— r)
    - B: (r Ã— k)
    - r << d, k (rankê°€ ì‘ìŒ!)
    """

    def __init__(self, original_layer, rank=8):
        self.W = original_layer.weight  # (d, k)
        self.d, self.k = self.W.shape
        self.r = rank

        # LoRA matrices (trainable!)
        self.A = nn.Parameter(torch.randn(self.d, self.r))
        self.B = nn.Parameter(torch.randn(self.r, self.k))

        # Originalì€ freeze
        self.W.requires_grad = False

    def forward(self, x):
        # Original output + LoRA adaptation
        return x @ self.W + x @ self.A @ self.B
        #      ^^^^^^^^     ^^^^^^^^^^^^^^
        #      Frozen       Trainable!

# ì˜ˆì‹œ: Transformer attention layer

# Original:
W_q = torch.randn(768, 768)  # 768 Ã— 768 = 589,824 params

# LoRA with rank=8:
A = torch.randn(768, 8)  # 768 Ã— 8 = 6,144 params
B = torch.randn(8, 768)  # 8 Ã— 768 = 6,144 params
# Total: 12,288 params (98% ê°ì†Œ!)

# Full model:
# Magenta RT: 760M params to train

# With LoRA (r=8):
# Only: ~2M params to train (99.7% ê°ì†Œ!)
# â†’ GPU ë©”ëª¨ë¦¬: 40GB â†’ 8GB
# â†’ í•™ìŠµ ì†ë„: 3ë°° ë¹ ë¦„
# â†’ RTX 3060ìœ¼ë¡œ ê°€ëŠ¥! âœ…

# LoRA ì ìš© ì½”ë“œ:

from peft import LoraConfig, get_peft_model

# 1. Config
lora_config = LoraConfig(
    r=8,                    # Rank (í•µì‹¬ íŒŒë¼ë¯¸í„°!)
    lora_alpha=16,          # Scaling factor
    target_modules=[        # ì–´ëŠ layerì— ì ìš©?
        "q_proj",           # Query projection
        "v_proj",           # Value projection
        "k_proj",           # Key projection (optional)
        "o_proj",           # Output projection (optional)
    ],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

# 2. ì ìš©
model = MagentaRT.from_pretrained('large')
model = get_peft_model(model, lora_config)

# 3. í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸
model.print_trainable_parameters()
# Output:
# trainable params: 2,097,152 / 760,000,000 = 0.28%

# 4. í•™ìŠµ (ì¼ë°˜ fine-tuningê³¼ ë™ì¼!)
optimizer = AdamW(model.parameters(), lr=1e-4)
for batch in dataloader:
    loss = model(batch)
    loss.backward()
    optimizer.step()

# 5. ì €ì¥ (ì‘ìŒ!)
model.save_pretrained('my_lora_weights')
# File size: ~10MB (vs 3GB for full model!)

# â•â•â• QLoRA (Quantized LoRA) â•â•â•

# LoRA + Quantization = ê·¹í•œì˜ íš¨ìœ¨!

# Quantization:
# - FP32 (32-bit): 1 param = 4 bytes
# - FP16 (16-bit): 1 param = 2 bytes
# - INT8 (8-bit): 1 param = 1 byte
# - INT4 (4-bit): 1 param = 0.5 bytes

# QLoRA = 4-bit quantization + LoRA

from transformers import BitsAndBytesConfig

qlora_config = BitsAndBytesConfig(
    load_in_4bit=True,              # 4-bit quantization
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",      # Normal Float 4
    bnb_4bit_use_double_quant=True
)

model = MagentaRT.from_pretrained(
    'large',
    quantization_config=qlora_config,
    device_map='auto'
)
# â†’ Model size: 3GB â†’ 750MB (75% ê°ì†Œ!)

model = get_peft_model(model, lora_config)

# Total GPU memory:
# Model (quantized): 750MB
# LoRA params: 10MB
# Optimizer: 20MB
# Activations: ~3GB
# Total: ~4GB
# â†’ RTX 3060 (8GB) ì¶©ë¶„! âœ…âœ…âœ…

# â•â•â• Rank ì„ íƒ ê°€ì´ë“œ â•â•â•

# Rank = LoRAì˜ í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°

# r=4: ë§¤ìš° ì‘ìŒ
# - ë©”ëª¨ë¦¬: ìµœì†Œ
# - í•™ìŠµ ë°ì´í„°: 1-5ì‹œê°„
# - ìŠ¤íƒ€ì¼ í•™ìŠµ: ì œí•œì 
# - ì‚¬ìš© ì¼€ì´ìŠ¤: ë¹ ë¥¸ ì‹¤í—˜

# r=8: í‘œì¤€ (ì¶”ì²œ!)
# - ë©”ëª¨ë¦¬: ì‘ìŒ
# - í•™ìŠµ ë°ì´í„°: 10-50ì‹œê°„
# - ìŠ¤íƒ€ì¼ í•™ìŠµ: ì¶©ë¶„
# - ì‚¬ìš© ì¼€ì´ìŠ¤: ëŒ€ë¶€ë¶„ì˜ ê²½ìš°

# r=16: í¼
# - ë©”ëª¨ë¦¬: ì¤‘ê°„
# - í•™ìŠµ ë°ì´í„°: 50-100ì‹œê°„
# - ìŠ¤íƒ€ì¼ í•™ìŠµ: ë§¤ìš° ì •êµ
# - ì‚¬ìš© ì¼€ì´ìŠ¤: ê³ í’ˆì§ˆ í•„ìš” ì‹œ

# r=32: ë§¤ìš° í¼
# - ë©”ëª¨ë¦¬: í¼
# - í•™ìŠµ ë°ì´í„°: 100ì‹œê°„+
# - ìŠ¤íƒ€ì¼ í•™ìŠµ: ê³¼ì í•© ìœ„í—˜
# - ì‚¬ìš© ì¼€ì´ìŠ¤: íŠ¹ìˆ˜í•œ ê²½ìš°

# ì‹¤í—˜:
# 1. r=8ë¡œ ì‹œì‘
# 2. Validation loss í™•ì¸
# 3. ë„ˆë¬´ ë†’ìœ¼ë©´ r ì¦ê°€
# 4. ê³¼ì í•©ë˜ë©´ r ê°ì†Œ

# â•â•â• LoRA Merging (ë°°í¬ ì‹œ) â•â•â•

# Fine-tuning í›„:
# Base model (3GB) + LoRA weights (10MB)

# Inference ì‹œ ë‘ ê°€ì§€ ì˜µì…˜:

# Option 1: ë¶„ë¦¬ (ê°œë°œ/ì‹¤í—˜)
base_model = load('magenta-rt-large')
lora_weights = load('my_lora_weights')
output = base_model(input) + lora_weights(input)

# Option 2: Merge (ë°°í¬)
model = merge_lora_weights(base_model, lora_weights)
# â†’ Single model (3GB)
# â†’ ë” ë¹ ë¥¸ inference
output = model(input)

# Merge ì½”ë“œ:
def merge_lora():
    model = MagentaRT.from_pretrained('large')
    model = PeftModel.from_pretrained(model, 'my_lora_weights')

    # Merge!
    merged = model.merge_and_unload()
    merged.save_pretrained('my_finetuned_model')

# â•â•â• ì‹¤ì „ ì˜ˆì‹œ: ë‚´ í”„ë¡œì íŠ¸ â•â•â•

# ë‚´ í™˜ê²½:
# - GPU: RTX 3060 (8GB)
# - ë°ì´í„°: ë‚´ ì—°ì£¼ 20ì‹œê°„
# - ëª©í‘œ: "ohhalim style" í•™ìŠµ

# 1. QLoRA config (4-bit)
qlora_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
)

# 2. LoRA config (r=8)
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
)

# 3. Model ë¡œë“œ
model = MagentaRT.from_pretrained(
    'large',
    quantization_config=qlora_config
)
model = get_peft_model(model, lora_config)

# 4. í•™ìŠµ ë°ì´í„°
my_data = load_my_recordings('my_20_hours/')
my_data = augment(my_data)  # 20ì‹œê°„ â†’ 200ì‹œê°„ equivalent

# 5. í•™ìŠµ!
trainer = Trainer(
    model=model,
    train_dataset=my_data,
    args=TrainingArguments(
        output_dir='ohhalim-style',
        num_train_epochs=50,
        per_device_train_batch_size=1,  # GPU ë©”ëª¨ë¦¬ ê³ ë ¤
        learning_rate=1e-4,
        save_steps=1000,
    )
)
trainer.train()

# 6. ì €ì¥
model.save_pretrained('ohhalim-lora')
# â†’ 10MB file! (ê³µìœ  ì‰¬ì›€)

# 7. ì‚¬ìš©
my_model = load_model_with_lora('magenta-rt-large', 'ohhalim-lora')
output = my_model.generate(style_prompt="ohhalim style")
# â†’ ë‚´ ìŠ¤íƒ€ì¼ë¡œ ì—°ì£¼! ğŸ¹
```

**í•™ìŠµ ë¦¬ì†ŒìŠ¤**:
```
1. ë…¼ë¬¸:
   - LoRA (Hu et al., 2021) - PAPERS_TO_READ.md
   - QLoRA (Dettmers et al., 2023)

2. ì½”ë“œ:
   - HuggingFace PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬
   - QLoRA GitHub repository
   - LoRA íŠœí† ë¦¬ì–¼ë“¤

3. ì‹¤ìŠµ:
   - ì‘ì€ ëª¨ë¸ë¡œ LoRA ì‹¤í—˜ (GPT-2)
   - Rank ë³€í™”ì— ë”°ë¥¸ íš¨ê³¼ ì¸¡ì •
   - QLoRAë¡œ í° ëª¨ë¸ í•™ìŠµ (Llama ë“±)

4. ë¹„êµ ì‹¤í—˜:
   - Full fine-tuning vs LoRA
   - LoRA vs QLoRA
   - ë‹¤ì–‘í•œ rank (r=4, 8, 16, 32)
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] LoRA ìˆ˜ì‹ ì´í•´ ë° ìœ ë„ ê°€ëŠ¥
- [ ] Low-rank ê°œë… ì„¤ëª… ê°€ëŠ¥
- [ ] Rank ì„ íƒ ê°€ì´ë“œë¼ì¸ ì´í•´
- [ ] QLoRA = 4-bit + LoRA ì„¤ëª… ê°€ëŠ¥
- [ ] PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ LoRA ì ìš© ê°€ëŠ¥
- [ ] Trainable params ê³„ì‚° ê°€ëŠ¥
- [ ] LoRA merge ì½”ë“œ ì‘ì„± ê°€ëŠ¥
- [ ] ë©”ëª¨ë¦¬ ì ˆê° ê³„ì‚° ê°€ëŠ¥ (760M â†’ 2M)

**ì˜ˆìƒ ì‹œê°„**: 5-6ì¼

---

## ğŸ“š Phase 4: ì‹¤ì „ êµ¬í˜„ (2ì£¼)

### 4.1 MIDI Tokenization ì„¤ê³„ & êµ¬í˜„ â­â­â­

**ì™œ ë°°ì›Œì•¼?**
- SpectroStream â†’ MIDI tokenizer ëŒ€ì²´
- 30ë°° íš¨ìœ¨ í–¥ìƒì˜ í•µì‹¬!

**ë¬´ì—‡ì„ ë°°ìš¸ê¹Œ?**

```python
# MIDI Tokenizer ì„¤ê³„

# â•â•â• ìš”êµ¬ì‚¬í•­ ë¶„ì„ â•â•â•

# SpectroStream (Audio):
# - 2ì´ˆ = 50 frames Ã— 64 RVQ = 3,200 tokens
# - Vocabulary: 1,024 per RVQ level
# - Hierarchical (coarse â†’ fine)

# Our MIDI Tokenizer:
# - 2ì´ˆ = ~100 events = ~100 tokens (30ë°° ì ìŒ!)
# - Vocabulary: ~500 tokens
# - Expressive (velocity, timing, pedal)

# â•â•â• Tokenization ë°©ì‹ ë¹„êµ â•â•â•

# Method 1: Event-based (ì¶”ì²œ!)
# ì¥ì : ì§ê´€ì , í‘œí˜„ë ¥ ë†’ìŒ
# ë‹¨ì : Variable length

# Example:
events = [
    ("NOTE_ON", 60, 80, 0.0),      # (type, pitch, velocity, time)
    ("NOTE_ON", 64, 75, 0.0),      # Chord: C-E
    ("NOTE_ON", 67, 75, 0.0),      # Chord: C-E-G
    ("NOTE_OFF", 60, 0, 0.5),
    ("NOTE_OFF", 64, 0, 0.5),
    ("NOTE_OFF", 67, 0, 0.5),
    ("NOTE_ON", 65, 70, 0.5),      # F
    ("NOTE_OFF", 65, 0, 1.0),
]

# Tokenize:
tokens = [
    TOKEN_NOTE_ON_60,    # Base: 0-127
    TOKEN_VELOCITY_80,   # Base: 128-255
    TOKEN_TIME_0,        # Base: 256
    TOKEN_NOTE_ON_64,
    TOKEN_VELOCITY_75,
    TOKEN_TIME_0,
    TOKEN_NOTE_ON_67,
    TOKEN_VELOCITY_75,
    TOKEN_TIME_0,
    TOKEN_TIME_SHIFT_500ms,  # Base: 300
    TOKEN_NOTE_OFF_60,   # Base: 400-527
    ...
]

# Vocabulary design:
vocab = {
    # Note events: 0-255
    "NOTE_ON_0": 0,
    "NOTE_ON_1": 1,
    ...
    "NOTE_ON_127": 127,
    "NOTE_OFF_0": 128,
    ...
    "NOTE_OFF_127": 255,

    # Velocity: 256-383 (128 bins)
    "VEL_0": 256,    # ppp
    "VEL_1": 257,
    ...
    "VEL_127": 383,  # fff

    # Time shifts: 384-511 (128 bins)
    "TIME_0ms": 384,
    "TIME_10ms": 385,
    "TIME_20ms": 386,
    ...
    "TIME_2000ms": 511,

    # Special tokens: 512-527
    "BAR": 512,
    "POSITION_0": 513,
    ...
    "POSITION_15": 528,  # 16ë¶„ìŒí‘œ ë‹¨ìœ„

    # Total vocabulary: ~530 tokens
}

# Method 2: REMI (Representation of MIDi)
# ì¥ì : êµ¬ì¡°í™”ë¨, quantized timing
# ë‹¨ì : ëœ í‘œí˜„ë ¥

tokens_remi = [
    BAR_START,        # 512
    POSITION_0,       # 513 (16ë¶„ìŒí‘œ ìœ„ì¹˜)
    PITCH_60,         # 0-127
    VELOCITY_80,      # 256-383
    DURATION_8,       # 530-545 (8ë¶„ìŒí‘œ)
    POSITION_2,       # 515
    PITCH_64,
    VELOCITY_75,
    DURATION_8,
    ...
]

# â•â•â• êµ¬í˜„: EventBasedMIDITokenizer â•â•â•

class EventBasedMIDITokenizer:
    """
    MIDI events â†” Token IDs
    """

    def __init__(self):
        self.vocab = self._build_vocab()
        self.vocab_size = len(self.vocab)
        self.id_to_token = {v: k for k, v in self.vocab.items()}

        # Time quantization
        self.time_bins = np.linspace(0, 2000, 128)  # 0-2000ms, 128 bins

    def _build_vocab(self):
        vocab = {}
        idx = 0

        # NOTE_ON: 0-127
        for pitch in range(128):
            vocab[f"NOTE_ON_{pitch}"] = idx
            idx += 1

        # NOTE_OFF: 128-255
        for pitch in range(128):
            vocab[f"NOTE_OFF_{pitch}"] = idx
            idx += 1

        # VELOCITY: 256-383
        for vel in range(128):
            vocab[f"VEL_{vel}"] = idx
            idx += 1

        # TIME_SHIFT: 384-511
        for i in range(128):
            vocab[f"TIME_{i}"] = idx
            idx += 1

        # POSITION: 512-527 (16ë¶„ìŒí‘œ ë‹¨ìœ„)
        for pos in range(16):
            vocab[f"POS_{pos}"] = idx
            idx += 1

        # Special tokens
        vocab["PAD"] = idx; idx += 1
        vocab["BOS"] = idx; idx += 1  # Begin of sequence
        vocab["EOS"] = idx; idx += 1  # End of sequence
        vocab["BAR"] = idx; idx += 1

        return vocab

    def encode(self, midi_file, duration=2.0):
        """
        MIDI file â†’ Token IDs

        Args:
            midi_file: path to MIDI file
            duration: chunk duration in seconds

        Returns:
            tokens: List[int]
        """
        # 1. Parse MIDI
        midi = mido.MidiFile(midi_file)
        events = []
        current_time = 0

        for msg in midi:
            current_time += msg.time

            if msg.type == 'note_on' and msg.velocity > 0:
                events.append({
                    'type': 'note_on',
                    'pitch': msg.note,
                    'velocity': msg.velocity,
                    'time': current_time
                })
            elif msg.type == 'note_off' or (msg.type == 'note_on' and msg.velocity == 0):
                events.append({
                    'type': 'note_off',
                    'pitch': msg.note,
                    'time': current_time
                })

        # 2. Filter by duration
        events = [e for e in events if e['time'] <= duration]

        # 3. Convert to tokens
        tokens = [self.vocab["BOS"]]
        prev_time = 0

        for event in events:
            # Time shift
            time_delta = event['time'] - prev_time
            if time_delta > 0:
                time_bin = np.digitize(time_delta * 1000, self.time_bins)
                time_bin = min(time_bin, 127)
                tokens.append(self.vocab[f"TIME_{time_bin}"])

            # Note event
            if event['type'] == 'note_on':
                tokens.append(self.vocab[f"NOTE_ON_{event['pitch']}"])
                tokens.append(self.vocab[f"VEL_{event['velocity']}"])
            else:
                tokens.append(self.vocab[f"NOTE_OFF_{event['pitch']}"])

            prev_time = event['time']

        tokens.append(self.vocab["EOS"])
        return tokens

    def decode(self, tokens):
        """
        Token IDs â†’ MIDI events

        Args:
            tokens: List[int]

        Returns:
            midi_events: List[dict]
        """
        events = []
        current_time = 0
        current_velocity = 64  # Default

        for token_id in tokens:
            token = self.id_to_token[token_id]

            if token.startswith("TIME_"):
                # Time shift
                bin_idx = int(token.split("_")[1])
                time_delta = self.time_bins[bin_idx] / 1000  # ms â†’ s
                current_time += time_delta

            elif token.startswith("NOTE_ON_"):
                pitch = int(token.split("_")[2])
                events.append({
                    'type': 'note_on',
                    'pitch': pitch,
                    'velocity': current_velocity,
                    'time': current_time
                })

            elif token.startswith("NOTE_OFF_"):
                pitch = int(token.split("_")[2])
                events.append({
                    'type': 'note_off',
                    'pitch': pitch,
                    'time': current_time
                })

            elif token.startswith("VEL_"):
                current_velocity = int(token.split("_")[1])

        return events

    def to_midi_file(self, events, output_path, tempo=120):
        """
        Events â†’ MIDI file
        """
        midi = mido.MidiFile()
        track = mido.MidiTrack()
        midi.tracks.append(track)

        # Tempo
        track.append(mido.MetaMessage('set_tempo',
                                      tempo=mido.bpm2tempo(tempo)))

        # Convert events to messages
        prev_time = 0
        for event in events:
            delta_time = int((event['time'] - prev_time) * 480)  # ticks

            if event['type'] == 'note_on':
                track.append(mido.Message(
                    'note_on',
                    note=event['pitch'],
                    velocity=event['velocity'],
                    time=delta_time
                ))
            elif event['type'] == 'note_off':
                track.append(mido.Message(
                    'note_off',
                    note=event['pitch'],
                    velocity=0,
                    time=delta_time
                ))

            prev_time = event['time']

        midi.save(output_path)

# â•â•â• ì‚¬ìš© ì˜ˆì‹œ â•â•â•

# 1. Tokenizer ìƒì„±
tokenizer = EventBasedMIDITokenizer()
print(f"Vocabulary size: {tokenizer.vocab_size}")  # ~530

# 2. MIDI â†’ Tokens
tokens = tokenizer.encode("my_improvisation.mid", duration=2.0)
print(f"2 seconds = {len(tokens)} tokens")  # ~100 tokens

# 3. Tokens â†’ MIDI
events = tokenizer.decode(tokens)
tokenizer.to_midi_file(events, "reconstructed.mid")

# 4. í’ˆì§ˆ í™•ì¸
original = mido.MidiFile("my_improvisation.mid")
reconstructed = mido.MidiFile("reconstructed.mid")
# â†’ ì²­ì·¨ & ë¹„êµ

# â•â•â• SpectroStreamì™€ í†µí•© â•â•â•

# Magenta RTì—ì„œ SpectroStream ëŒ€ì‹  MIDI tokenizer ì‚¬ìš©

class MagentaRTMIDI:
    """
    Magenta RT adapted for MIDI
    """

    def __init__(self):
        self.tokenizer = EventBasedMIDITokenizer()
        self.encoder = MagentaRTEncoder()  # ê·¸ëŒ€ë¡œ ì‚¬ìš©!
        self.decoder = MagentaRTDecoder()  # ê·¸ëŒ€ë¡œ ì‚¬ìš©!
        # SpectroStream â†’ tokenizerë¡œ ëŒ€ì²´

    def generate_chunk(self, history_midi, style):
        """
        Input: MIDI events (10ì´ˆ)
        Output: MIDI events (2ì´ˆ)
        """
        # 1. MIDI â†’ Tokens
        history_tokens = []
        for chunk_midi in history_midi:
            tokens = self.tokenizer.encode(chunk_midi)
            # Coarse: ì „ì²´ tokens (MIDIëŠ” ì´ë¯¸ ì¶©ë¶„íˆ ì‘ìŒ)
            history_tokens.append(tokens)

        # 2. Encoder
        encoder_input = torch.cat(history_tokens + [style])
        encoder_output = self.encoder(encoder_input)

        # 3. Decoder
        next_tokens = self.decoder.generate(
            encoder_output,
            max_length=100  # ~2ì´ˆ ë¶„ëŸ‰
        )

        # 4. Tokens â†’ MIDI
        next_events = self.tokenizer.decode(next_tokens)

        return next_events

# íš¨ìœ¨ ë¹„êµ:
# Audio (SpectroStream):
# - 2ì´ˆ = 3,200 tokens
# - Encoding: 50ms
# - Decoding: 100ms

# MIDI (EventBased):
# - 2ì´ˆ = 100 tokens (32ë°° ì ìŒ!)
# - Encoding: 5ms (10ë°° ë¹ ë¦„!)
# - Decoding: 10ms (10ë°° ë¹ ë¦„!)

# â†’ Total latency: 800ms â†’ 50ms! ğŸš€
```

**í•™ìŠµ ë¦¬ì†ŒìŠ¤**:
```
1. MIDI ê¸°ì´ˆ:
   - Mido ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¬¸ì„œ
   - MIDI specification
   - Pretty_midi ë¼ì´ë¸ŒëŸ¬ë¦¬

2. Tokenization ì—°êµ¬:
   - Miditok ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶„ì„
   - "This Time with Feeling" ë…¼ë¬¸
   - Music Transformer tokenization

3. ì‹¤ìŠµ:
   - ë‹¤ì–‘í•œ MIDI íŒŒì¼ë¡œ í…ŒìŠ¤íŠ¸
   - Tokenize â†’ Detokenize í’ˆì§ˆ í‰ê°€
   - Vocabulary size ìµœì í™”
   - ë‚´ ì—°ì£¼ ë°ì´í„°ë¡œ ì‹¤í—˜

4. í†µí•©:
   - Magenta RT ì½”ë“œ ì½ê¸°
   - SpectroStream ì‚¬ìš© ë¶€ë¶„ ì°¾ê¸°
   - MIDI tokenizerë¡œ êµì²´ ê³„íš
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] Event-based vs REMI ë¹„êµ ì„¤ëª…
- [ ] Vocabulary ì„¤ê³„ ë…¼ë¦¬ ì´í•´
- [ ] EventBasedMIDITokenizer êµ¬í˜„ ì™„ë£Œ
- [ ] Encode/decode ë™ì‘ í™•ì¸
- [ ] Reconstruction í’ˆì§ˆ í‰ê°€
- [ ] SpectroStream ëŒ€ì²´ ê³„íš ìˆ˜ë¦½
- [ ] íš¨ìœ¨ ê°œì„  ê³„ì‚° (32ë°°)

**ì˜ˆìƒ ì‹œê°„**: 5-6ì¼

---

### 4.2 Real-time MIDI Generation System â­â­â­

**ì™œ ë°°ì›Œì•¼?**
- ëª¨ë“  ê²ƒì„ í†µí•©í•˜ëŠ” ìµœì¢… ì‹œìŠ¤í…œ!
- "ë‚˜ì™€ ê°€ìƒì˜ ë‚´ê°€ JAM!" ì‹¤í˜„

**ë¬´ì—‡ì„ ë°°ìš¸ê¹Œ?**

```python
# ì „ì²´ ì‹œìŠ¤í…œ ì„¤ê³„

# â•â•â• System Architecture â•â•â•

"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input Layer (Real-time MIDI Input)            â”‚
â”‚  - MIDI Keyboard                               â”‚
â”‚  - Ableton/FL Studio                          â”‚
â”‚  - Virtual MIDI ports                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Capture & Buffer Layer                        â”‚
â”‚  - 2-second chunks                            â”‚
â”‚  - Thread-safe queue                           â”‚
â”‚  - Timing synchronization                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Analysis Layer                                â”‚
â”‚  - Chord detection                             â”‚
â”‚  - Rhythm analysis                             â”‚
â”‚  - Style extraction                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Generation Layer                           â”‚
â”‚  - MagentaRT-MIDI model                       â”‚
â”‚  - My style (fine-tuned)                      â”‚
â”‚  - Context: 10s history                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MIDI Injection Layer                          â”‚
â”‚  - Merge user + AI MIDI                       â”‚
â”‚  - Harmonic blending                           â”‚
â”‚  - Timing sync                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output Layer                                  â”‚
â”‚  - Virtual MIDI out                            â”‚
â”‚  - DAW routing                                 â”‚
â”‚  - Synth/VST                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

# â•â•â• Implementation â•â•â•

import threading
import queue
import time
import mido
import torch

class RealTimeMIDIDuetSystem:
    """
    ë‚˜ì™€ ê°€ìƒì˜ ë‚´ê°€ JAM!
    """

    def __init__(self, model_path, input_port, output_port):
        # 1. Model
        self.model = self.load_finetuned_model(model_path)
        self.tokenizer = EventBasedMIDITokenizer()

        # 2. MIDI I/O
        self.input_port = mido.open_input(input_port)
        self.output_port = mido.open_output(output_port)

        # 3. State
        self.context_buffer = []  # ìµœê·¼ 10ì´ˆ MIDI
        self.user_playing = False
        self.ai_enabled = True

        # 4. Threading
        self.input_queue = queue.Queue()
        self.output_queue = queue.Queue()

        # 5. Timing
        self.chunk_duration = 2.0  # seconds
        self.latency_target = 0.05  # 50ms

        # 6. Style
        self.my_style = self.load_my_style()

    def load_finetuned_model(self, path):
        """Load fine-tuned model"""
        base_model = MagentaRTMIDI.from_pretrained('large')
        model = PeftModel.from_pretrained(base_model, path)
        model.eval()
        model.to('cuda')
        return model

    def load_my_style(self):
        """Load my style embedding"""
        # Option 1: From text
        style = self.model.encode_text("ohhalim jazz piano style")

        # Option 2: From audio reference
        # my_recording = "my_best_improv.wav"
        # style = self.model.encode_audio(my_recording)

        return style

    def start(self):
        """Start the duet system"""
        # Start threads
        input_thread = threading.Thread(target=self.input_loop)
        generation_thread = threading.Thread(target=self.generation_loop)
        output_thread = threading.Thread(target=self.output_loop)

        input_thread.start()
        generation_thread.start()
        output_thread.start()

        print("ğŸ¹ Real-time MIDI Duet System Started!")
        print("Play on your MIDI keyboard...")
        print("Press Ctrl+C to stop")

        try:
            input_thread.join()
            generation_thread.join()
            output_thread.join()
        except KeyboardInterrupt:
            print("\nğŸ›‘ Stopping...")
            self.stop()

    def input_loop(self):
        """Thread 1: Capture MIDI input"""
        chunk = []
        chunk_start_time = time.time()

        for msg in self.input_port:
            current_time = time.time()
            elapsed = current_time - chunk_start_time

            # Add to current chunk
            chunk.append({
                'message': msg,
                'time': elapsed
            })

            # Check if chunk is complete
            if elapsed >= self.chunk_duration:
                # Put chunk in queue
                self.input_queue.put(chunk)

                # Start new chunk
                chunk = []
                chunk_start_time = current_time

    def generation_loop(self):
        """Thread 2: AI generation"""
        while True:
            # Wait for input chunk
            user_chunk = self.input_queue.get()

            # Detect if user is playing
            self.user_playing = len(user_chunk) > 0

            if self.ai_enabled:
                # Generate AI response
                t0 = time.time()

                # 1. Tokenize user input
                user_tokens = self.midi_chunk_to_tokens(user_chunk)

                # 2. Update context
                self.context_buffer.append(user_tokens)
                if len(self.context_buffer) > 5:  # Keep last 10s
                    self.context_buffer.pop(0)

                # 3. Generate AI response
                ai_tokens = self.model.generate_chunk(
                    history=self.context_buffer,
                    style=self.my_style,
                    temperature=1.0,
                    top_k=40
                )

                # 4. Decode to MIDI
                ai_events = self.tokenizer.decode(ai_tokens)

                # 5. MIDI Injection (blend with user)
                blended_events = self.midi_injection(
                    user_chunk,
                    ai_events,
                    mix_ratio=0.3
                )

                # 6. Update context with blended
                blended_tokens = self.events_to_tokens(blended_events)
                self.context_buffer[-1] = blended_tokens

                # 7. Put AI events to output queue
                self.output_queue.put(ai_events)

                # 8. Measure latency
                latency = time.time() - t0
                if latency > self.latency_target:
                    print(f"âš ï¸ Latency: {latency*1000:.1f}ms "
                          f"(target: {self.latency_target*1000:.0f}ms)")

    def output_loop(self):
        """Thread 3: MIDI output"""
        while True:
            # Wait for AI events
            ai_events = self.output_queue.get()

            # Send to MIDI output
            for event in ai_events:
                # Convert to mido message
                if event['type'] == 'note_on':
                    msg = mido.Message(
                        'note_on',
                        note=event['pitch'],
                        velocity=event['velocity']
                    )
                elif event['type'] == 'note_off':
                    msg = mido.Message(
                        'note_off',
                        note=event['pitch']
                    )

                # Send!
                self.output_port.send(msg)

                # Wait for timing
                if 'delta_time' in event:
                    time.sleep(event['delta_time'])

    def midi_injection(self, user_chunk, ai_events, mix_ratio=0.3):
        """
        Blend user MIDI with AI MIDI

        Strategy: Harmonic blending
        - User notes: 100% kept
        - AI notes: Added if harmonically compatible
        """
        blended = []

        # Extract user notes (time â†’ notes mapping)
        user_notes_by_time = self.group_by_time(user_chunk)
        ai_notes_by_time = self.group_by_time(ai_events)

        # Merge
        all_times = sorted(set(
            list(user_notes_by_time.keys()) +
            list(ai_notes_by_time.keys())
        ))

        for t in all_times:
            user_notes = user_notes_by_time.get(t, [])
            ai_notes = ai_notes_by_time.get(t, [])

            # Add all user notes
            blended.extend(user_notes)

            # Add AI notes if:
            # 1. No user notes at this time (AI solo)
            # 2. Harmonically compatible with user
            if len(user_notes) == 0:
                # AI solo
                blended.extend(ai_notes)
            else:
                # Check harmony
                user_pitches = [n['pitch'] for n in user_notes]
                for ai_note in ai_notes:
                    if self.is_harmonically_compatible(
                        ai_note['pitch'],
                        user_pitches
                    ):
                        # Compatible! Add as background
                        ai_note['velocity'] = int(ai_note['velocity'] * 0.6)
                        blended.append(ai_note)

        return blended

    def is_harmonically_compatible(self, ai_pitch, user_pitches):
        """
        Check if AI note is harmonically compatible with user notes
        """
        # Simple heuristic: interval check
        for user_pitch in user_pitches:
            interval = abs((ai_pitch - user_pitch) % 12)

            # Dissonant intervals: m2, M7, tritone
            if interval in [1, 6, 11]:
                return False

        # Consonant!
        return True

    def stop(self):
        """Stop the system"""
        self.input_port.close()
        self.output_port.close()
        print("âœ… System stopped")

# â•â•â• ì‚¬ìš© ì˜ˆì‹œ â•â•â•

# 1. MIDI í¬íŠ¸ í™•ì¸
print("Available MIDI inputs:")
print(mido.get_input_names())
print("\nAvailable MIDI outputs:")
print(mido.get_output_names())

# 2. ì‹œìŠ¤í…œ ì‹œì‘
system = RealTimeMIDIDuetSystem(
    model_path='ohhalim-lora',
    input_port='MIDI Keyboard',
    output_port='Virtual MIDI Bus 1'
)

# 3. ë“€ì—£ ì‹œì‘!
system.start()

# â†’ ì´ì œ MIDI í‚¤ë³´ë“œë¡œ ì—°ì£¼í•˜ë©´
# â†’ AIê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜ì‘! ğŸ¹âœ¨

# â•â•â• DAW í†µí•© (Ableton ì˜ˆì‹œ) â•â•â•

"""
Setup in Ableton:

Track 1: "Me" (Real piano)
  - Input: MIDI Keyboard
  - Monitor: In
  - Instrument: Piano VST

Track 2: "AI Me" (Virtual me)
  - Input: Virtual MIDI Bus 1 (from our system)
  - Monitor: In
  - Instrument: Electric Piano VST

â†’ ë‘ íŠ¸ë™ì´ ë™ì‹œì— ì—°ì£¼!
â†’ ë‚˜ì™€ ê°€ìƒì˜ ë‚´ê°€ JAM! ğŸ‰
"""

# â•â•â• Performance Tuning â•â•â•

class OptimizedDuetSystem(RealTimeMIDIDuetSystem):
    """
    Optimized version with profiling
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # Profiling
        self.latencies = []
        self.rtf_values = []

        # Optimizations
        self.model = torch.compile(self.model)  # PyTorch 2.0
        self.model.half()  # FP16

        # KV-cache warm-up
        self.warmup_model()

    def warmup_model(self):
        """Warm-up to reduce first inference latency"""
        print("ğŸ”¥ Warming up model...")
        dummy_input = torch.zeros(1, 100).long().to('cuda')
        for _ in range(5):
            _ = self.model(dummy_input)
        print("âœ… Model ready!")

    def generation_loop(self):
        """With profiling"""
        while True:
            user_chunk = self.input_queue.get()

            if self.ai_enabled:
                t0 = time.time()

                # Generation
                ai_tokens = self.model.generate_chunk(
                    history=self.context_buffer,
                    style=self.my_style
                )

                latency = time.time() - t0
                self.latencies.append(latency)

                # RTF
                rtf = self.chunk_duration / latency
                self.rtf_values.append(rtf)

                # Print stats every 10 chunks
                if len(self.latencies) % 10 == 0:
                    avg_latency = np.mean(self.latencies[-10:])
                    avg_rtf = np.mean(self.rtf_values[-10:])
                    print(f"ğŸ“Š Latency: {avg_latency*1000:.1f}ms, "
                          f"RTF: {avg_rtf:.1f}x")

                # ... rest of generation loop
```

**í•™ìŠµ ë¦¬ì†ŒìŠ¤**:
```
1. Real-time programming:
   - Python threading & multiprocessing
   - Queue & synchronization
   - Low-latency best practices

2. MIDI I/O:
   - Mido library advanced usage
   - Virtual MIDI ports (loopMIDI on Windows)
   - rtmidi library

3. DAW integration:
   - Ableton MIDI routing
   - FL Studio MIDI setup
   - Logic Pro MIDI environment

4. ì‹¤ìŠµ:
   - ê°„ë‹¨í•œ MIDI echo í”„ë¡œê·¸ë¨
   - Latency ì¸¡ì • ë„êµ¬
   - MIDI injection í”„ë¡œí† íƒ€ì…
   - ì „ì²´ ì‹œìŠ¤í…œ í†µí•©

5. Debugging:
   - MIDI monitor ë„êµ¬
   - Latency profiling
   - Thread synchronization ì´ìŠˆ
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] Multi-threading êµ¬ì¡° ì´í•´
- [ ] MIDI I/O ì½”ë“œ ì‘ì„± ê°€ëŠ¥
- [ ] Chunk-based capture êµ¬í˜„
- [ ] AI generation loop êµ¬í˜„
- [ ] MIDI injection êµ¬í˜„
- [ ] DAW í†µí•© ì„¤ì • ì™„ë£Œ
- [ ] Latency profiling êµ¬í˜„
- [ ] ì „ì²´ ì‹œìŠ¤í…œ ë™ì‘ í™•ì¸
- [ ] "ë‚˜ì™€ ê°€ìƒì˜ ë‚´ê°€ JAM!" ì„±ê³µ! ğŸ‰

**ì˜ˆìƒ ì‹œê°„**: 7ì¼ (ê°€ì¥ ì¤‘ìš”!)

---

## ğŸ—“ï¸ ì£¼ì°¨ë³„ í•™ìŠµ í”Œëœ

### Week 1-2: ê¸°ì´ˆ ì´ë¡ 
```
Day 1-3: Transformer ì´í•´
Day 4-5: Audio/MIDI tokenization
Day 6-7: Music generation ê¸°ì´ˆ
Weekend: ë³µìŠµ & ì‹¤ìŠµ í”„ë¡œì íŠ¸
```

### Week 3-4: Magenta RealTime
```
Day 1-3: Architecture ë¶„í•´
Day 4-5: Real-time generation ê¸°ìˆ 
Day 6-7: Audio injection ë©”ì»¤ë‹ˆì¦˜
Weekend: Colab ë°ëª¨ ì™„ì „ ë¶„ì„
```

### Week 5-6: Fine-tuning
```
Day 1-3: Transfer learning ê¸°ì´ˆ
Day 4-6: LoRA & QLoRA ë§ˆìŠ¤í„°
Day 7: ì‘ì€ ì‹¤í—˜ (10ê°œ MIDI)
Weekend: ë°ì´í„° ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½
```

### Week 7-8: ì‹¤ì „ êµ¬í˜„
```
Day 1-3: MIDI tokenizer êµ¬í˜„
Day 4-7: Real-time system êµ¬í˜„
Weekend: í†µí•© í…ŒìŠ¤íŠ¸
```

### Week 9-10: ë°ì´í„° & Fine-tuning
```
Week 9: ë‚´ ì—°ì£¼ 20ì‹œê°„ ë…¹ìŒ
Week 10: Fine-tuning ì‹¤í–‰
Weekend: í’ˆì§ˆ í‰ê°€ & ê°œì„ 
```

### Week 11-12: ì™„ì„± & í…ŒìŠ¤íŠ¸
```
Week 11: ì‹¤ì‹œê°„ ë“€ì—£ ì‹œìŠ¤í…œ ì™„ì„±
Week 12: í…ŒìŠ¤íŠ¸, ë””ë²„ê¹…, ìµœì í™”
Weekend: ğŸ‰ First JAM with AI me!
```

---

## ğŸ“š í•™ìŠµ ë¦¬ì†ŒìŠ¤ ì´ì •ë¦¬

### ë…¼ë¬¸
- [ ] Attention Is All You Need (Transformer)
- [ ] Music Transformer (Google Magenta)
- [ ] LoRA (Hu et al.)
- [ ] QLoRA (Dettmers et al.)
- [ ] Live Music Models (arxiv 2508.04651)
- [ ] SoundStream / EnCodec (Audio codecs)

### ì½”ë“œ & ë¼ì´ë¸ŒëŸ¬ë¦¬
- [ ] github.com/magenta/magenta-realtime
- [ ] HuggingFace Transformers
- [ ] HuggingFace PEFT
- [ ] Miditok
- [ ] Mido
- [ ] PyTorch

### íŠœí† ë¦¬ì–¼ & ê°•ì˜
- [ ] The Illustrated Transformer (Jay Alammar)
- [ ] HuggingFace NLP Course
- [ ] Fast.ai Practical Deep Learning
- [ ] Stanford CS224N (NLP)
- [ ] MIT 6.S191 (Deep Learning)

### ë„êµ¬
- [ ] Colab (ë¬´ë£Œ TPU)
- [ ] Weights & Biases (ì‹¤í—˜ tracking)
- [ ] TensorBoard (ì‹œê°í™”)
- [ ] MIDI Monitor (ë””ë²„ê¹…)
- [ ] DAW (Ableton / FL Studio)

---

## ğŸ¯ ì„±ê³µ ê¸°ì¤€

### Phase 1 ì™„ë£Œ (2ì£¼)
- [ ] Transformer ìˆ˜ì‹ ìœ ë„ ê°€ëŠ¥
- [ ] MIDI tokenization ì½”ë“œ ì‘ì„±
- [ ] Music LM ê°œë… ì„¤ëª… ê°€ëŠ¥

### Phase 2 ì™„ë£Œ (2ì£¼)
- [ ] Magenta RT architecture ì™„ì „ ì´í•´
- [ ] Chunk-based generation êµ¬í˜„ ê°€ëŠ¥
- [ ] Colab ë°ëª¨ ìˆ˜ì • ê°€ëŠ¥

### Phase 3 ì™„ë£Œ (1.5ì£¼)
- [ ] LoRA ì ìš©í•˜ì—¬ fine-tuning ì‹¤í–‰
- [ ] ë‚´ 10ì‹œê°„ ë°ì´í„°ë¡œ ì‹¤í—˜ ì„±ê³µ
- [ ] "ohhalim style" í•™ìŠµ í™•ì¸

### Phase 4 ì™„ë£Œ (2ì£¼)
- [ ] MIDI tokenizer ì™„ì„±
- [ ] Real-time duet system ì‘ë™
- [ ] Latency <50ms ë‹¬ì„±

### ìµœì¢… ëª©í‘œ ë‹¬ì„± (3ê°œì›”)
- [ ] ë‚˜ì™€ ê°€ìƒì˜ ë‚´ê°€ ì‹¤ì‹œê°„ JAM! ğŸ¹
- [ ] 10ë¶„ ì´ìƒ ì•ˆì •ì  ë“€ì—£ ì—°ì£¼
- [ ] AIê°€ ë‚´ ìŠ¤íƒ€ì¼ë¡œ ë°˜ì‘
- [ ] ìŒì•…ì ìœ¼ë¡œ ë§Œì¡±ìŠ¤ëŸ¬ì›€

---

## ğŸ’ª ì‹¤í–‰ íŒ

### ë§¤ì¼ í•™ìŠµ
- **ì‹œê°„**: 2-3ì‹œê°„ (ì§‘ì¤‘)
- **ë°©ì‹**: ì´ë¡  1ì‹œê°„ + ì‹¤ìŠµ 2ì‹œê°„
- **ê¸°ë¡**: ë§¤ì¼ ë°°ìš´ ê²ƒ ì •ë¦¬ (ë…¸íŠ¸)

### ì£¼ë§ í”„ë¡œì íŠ¸
- **ì‹œê°„**: 4-6ì‹œê°„
- **ë°©ì‹**: í•œ ì£¼ ë°°ìš´ ê²ƒ í†µí•©
- **ëª©í‘œ**: ì‘ë™í•˜ëŠ” ì½”ë“œ ì™„ì„±

### ë§‰í ë•Œ
1. ê³µì‹ ë¬¸ì„œ ë‹¤ì‹œ ì½ê¸°
2. ì½”ë“œ ë””ë²„ê¹… (print ì°ê¸°)
3. ê°„ë‹¨í•œ ë²„ì „ë¶€í„° (MVP)
4. ì»¤ë®¤ë‹ˆí‹° ì§ˆë¬¸ (Stack Overflow, Reddit)

### ë™ê¸°ë¶€ì—¬
- **ë¹„ì „ ìƒê¸°**: "ë‚˜ì™€ ê°€ìƒì˜ ë‚´ê°€ JAM!"
- **ì‘ì€ ì„±ê³µ**: ë§¤ì¼ ì‘ì€ ì§„ì „ ì¶•í•˜
- **ìŒì•… ë“£ê¸°**: Brad Mehldau ë“± ì˜ê°
- **íœ´ì‹**: ë²ˆì•„ì›ƒ ë°©ì§€!

---

## ğŸ‰ ë§ˆì§€ë§‰ ë©”ì‹œì§€

**3ê°œì›” í›„ ë‹¹ì‹ ì˜ ëª¨ìŠµ:**

```
ë‚˜: [MIDI í‚¤ë³´ë“œë¡œ ì—°ì£¼ ì‹œì‘]
AI: [ë‚´ ìŠ¤íƒ€ì¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì‘]
ë‚˜: "ì˜¤, ì´ê±´ ë‚´ê°€ 3ë…„ ì „ì— ì³¤ë˜ í”„ë ˆì´ì¦ˆë„¤!"
AI: [ê³„ì† ëŒ€í™”í•˜ë“¯ ì—°ì£¼]
ë‚˜: "ì§„ì§œ ë‚˜ë‘ ë“€ì—£í•˜ëŠ” ê²ƒ ê°™ì•„... ì‹ ê¸°í•˜ë‹¤"

â†’ ëª©í‘œ ë‹¬ì„±! ğŸ¹âœ¨
```

**Let's make it happen!** ğŸ’ª

**ì§€ê¸ˆ ì‹œì‘í•˜ì„¸ìš”:**
1. ì´ ë¬¸ì„œë¥¼ í”„ë¦°íŠ¸í•˜ê±°ë‚˜ ë¶ë§ˆí¬
2. Week 1 Day 1 ì‹œì‘: "Attention Is All You Need" ë…¼ë¬¸ ì½ê¸°
3. ë§¤ì¼ ì¡°ê¸ˆì”©, ê¾¸ì¤€íˆ!

**You got this!** ğŸš€
