"""
FAD (Frechet Audio Distance) ê³„ì‚° ìŠ¤í¬ë¦½íŠ¸

ìƒì„±ëœ ì˜¤ë””ì˜¤ì™€ ì‹¤ì œ ì¬ì¦ˆ ì˜¤ë””ì˜¤ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•´.
ë‚®ì„ìˆ˜ë¡ ë” ìœ ì‚¬í•¨ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ).

ì‚¬ìš©ë²•:
    python scripts/calculate_fad.py \
        --generated_dir ./generated_audio \
        --reference_dir ./reference_jazz

í•„ìš”í•œ íŒ¨í‚¤ì§€:
    pip install frechet_audio_distance librosa numpy
"""

import argparse
import numpy as np
from pathlib import Path
import librosa
import torch
from typing import List, Tuple


def load_audio_files(directory: str, sr: int = 16000) -> List[np.ndarray]:
    """
    í´ë”ì—ì„œ ëª¨ë“  ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ

    Args:
        directory: ì˜¤ë””ì˜¤ íŒŒì¼ì´ ìˆëŠ” í´ë”
        sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸

    Returns:
        ì˜¤ë””ì˜¤ ë°°ì—´ ë¦¬ìŠ¤íŠ¸
    """
    audio_files = []
    directory = Path(directory)

    for ext in ['*.wav', '*.mp3', '*.flac']:
        for audio_path in directory.glob(ext):
            try:
                audio, _ = librosa.load(audio_path, sr=sr, mono=True)
                audio_files.append(audio)
                print(f"âœ… ë¡œë“œ: {audio_path.name}")
            except Exception as e:
                print(f"âŒ ë¡œë“œ ì‹¤íŒ¨: {audio_path.name} - {e}")

    return audio_files


def extract_vggish_features(audio_list: List[np.ndarray]) -> np.ndarray:
    """
    VGGish ëª¨ë¸ë¡œ ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ

    Args:
        audio_list: ì˜¤ë””ì˜¤ ë°°ì—´ ë¦¬ìŠ¤íŠ¸

    Returns:
        íŠ¹ì§• ë²¡í„° ë°°ì—´ (N, 128)
    """
    try:
        # VGGish ëª¨ë¸ ë¡œë“œ (ì‚¬ì „í•™ìŠµëœ ì˜¤ë””ì˜¤ ì„ë² ë”© ëª¨ë¸)
        import tensorflow as tf
        import tensorflow_hub as hub

        model = hub.load('https://tfhub.dev/google/vggish/1')

        features = []
        for i, audio in enumerate(audio_list):
            # VGGishëŠ” 16kHz ëª¨ë…¸ ì˜¤ë””ì˜¤ í•„ìš”
            # 0.96ì´ˆ ì²­í¬ë¡œ ë¶„í• 
            chunk_length = int(0.96 * 16000)

            audio_chunks = [
                audio[i:i+chunk_length]
                for i in range(0, len(audio) - chunk_length, chunk_length)
            ]

            chunk_features = []
            for chunk in audio_chunks:
                if len(chunk) == chunk_length:
                    # VGGish ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    chunk_tensor = tf.constant(chunk, dtype=tf.float32)
                    embedding = model(chunk_tensor)
                    chunk_features.append(embedding.numpy())

            if chunk_features:
                # í‰ê·  íŠ¹ì§• ë²¡í„°
                mean_feature = np.mean(chunk_features, axis=0)
                features.append(mean_feature)

            print(f"   íŠ¹ì§• ì¶”ì¶œ ì¤‘... {i+1}/{len(audio_list)}")

        return np.array(features)

    except ImportError:
        print("âš ï¸  TensorFlow Hub ì—†ìŒ. librosa íŠ¹ì§•ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        return extract_librosa_features(audio_list)


def extract_librosa_features(audio_list: List[np.ndarray], sr: int = 16000) -> np.ndarray:
    """
    librosaë¡œ ê°„ë‹¨í•œ ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ (VGGish ëŒ€ì•ˆ)

    Args:
        audio_list: ì˜¤ë””ì˜¤ ë°°ì—´ ë¦¬ìŠ¤íŠ¸
        sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸

    Returns:
        íŠ¹ì§• ë²¡í„° ë°°ì—´ (N, feature_dim)
    """
    features = []

    for i, audio in enumerate(audio_list):
        # ì—¬ëŸ¬ íŠ¹ì§• ì¶”ì¶œ
        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)
        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)
        zero_crossing = librosa.feature.zero_crossing_rate(audio)
        chroma = librosa.feature.chroma_stft(y=audio, sr=sr)

        # í‰ê· ê°’ìœ¼ë¡œ ìš”ì•½
        feature_vector = np.concatenate([
            np.mean(mfcc, axis=1),           # 13
            np.mean(spectral_centroid),      # 1
            np.mean(spectral_rolloff),       # 1
            np.mean(zero_crossing),          # 1
            np.mean(chroma, axis=1),         # 12
        ])

        features.append(feature_vector)
        print(f"   íŠ¹ì§• ì¶”ì¶œ ì¤‘... {i+1}/{len(audio_list)}")

    return np.array(features)


def calculate_frechet_distance(mu1: np.ndarray, sigma1: np.ndarray,
                               mu2: np.ndarray, sigma2: np.ndarray) -> float:
    """
    Frechet Distance ê³„ì‚°

    FD = ||mu1 - mu2||^2 + Tr(sigma1 + sigma2 - 2*sqrt(sigma1*sigma2))

    Args:
        mu1, mu2: í‰ê·  ë²¡í„°
        sigma1, sigma2: ê³µë¶„ì‚° í–‰ë ¬

    Returns:
        Frechet distance (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)
    """
    # í‰ê·  ì°¨ì´
    diff = mu1 - mu2
    mean_dist = np.sum(diff ** 2)

    # ê³µë¶„ì‚° í–‰ë ¬ì˜ ì œê³±ê·¼
    from scipy import linalg
    covmean = linalg.sqrtm(sigma1.dot(sigma2))

    # ìˆ˜ì¹˜ ì˜¤ë¥˜ë¡œ ì¸í•œ ë³µì†Œìˆ˜ ì œê±°
    if np.iscomplexobj(covmean):
        covmean = covmean.real

    # Frechet distance
    fd = mean_dist + np.trace(sigma1 + sigma2 - 2 * covmean)

    return fd


def compute_fad(generated_features: np.ndarray,
                reference_features: np.ndarray) -> float:
    """
    FAD (Frechet Audio Distance) ê³„ì‚°

    Args:
        generated_features: ìƒì„±ëœ ì˜¤ë””ì˜¤ íŠ¹ì§• (N1, D)
        reference_features: ë ˆí¼ëŸ°ìŠ¤ ì˜¤ë””ì˜¤ íŠ¹ì§• (N2, D)

    Returns:
        FAD ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
    """
    # í†µê³„ëŸ‰ ê³„ì‚°
    mu_gen = np.mean(generated_features, axis=0)
    sigma_gen = np.cov(generated_features, rowvar=False)

    mu_ref = np.mean(reference_features, axis=0)
    sigma_ref = np.cov(reference_features, rowvar=False)

    # FAD ê³„ì‚°
    fad_score = calculate_frechet_distance(mu_gen, sigma_gen, mu_ref, sigma_ref)

    return fad_score


def evaluate_fad(generated_dir: str, reference_dir: str, output_dir: str = './evaluation'):
    """ì „ì²´ FAD í‰ê°€ ì‹¤í–‰"""
    print("=" * 60)
    print("ğŸµ FAD (Frechet Audio Distance) ê³„ì‚° ì‹œì‘")
    print("=" * 60)

    # 1. ì˜¤ë””ì˜¤ ë¡œë“œ
    print("\nğŸ“‚ ìƒì„±ëœ ì˜¤ë””ì˜¤ ë¡œë“œ...")
    generated_audio = load_audio_files(generated_dir)

    print(f"\nğŸ“‚ ë ˆí¼ëŸ°ìŠ¤ ì¬ì¦ˆ ì˜¤ë””ì˜¤ ë¡œë“œ...")
    reference_audio = load_audio_files(reference_dir)

    if len(generated_audio) == 0 or len(reference_audio) == 0:
        print("âŒ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        return

    print(f"\nâœ… ë¡œë“œ ì™„ë£Œ:")
    print(f"   ìƒì„± ì˜¤ë””ì˜¤: {len(generated_audio)}ê°œ")
    print(f"   ë ˆí¼ëŸ°ìŠ¤: {len(reference_audio)}ê°œ")

    # 2. íŠ¹ì§• ì¶”ì¶œ
    print(f"\nğŸ” ìƒì„± ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ...")
    generated_features = extract_librosa_features(generated_audio)

    print(f"\nğŸ” ë ˆí¼ëŸ°ìŠ¤ ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ...")
    reference_features = extract_librosa_features(reference_audio)

    # 3. FAD ê³„ì‚°
    print(f"\nğŸ“Š FAD ê³„ì‚° ì¤‘...")
    fad_score = compute_fad(generated_features, reference_features)

    # 4. ê²°ê³¼ ì¶œë ¥
    print(f"\n" + "=" * 60)
    print(f"ğŸ¯ FAD ì ìˆ˜: {fad_score:.2f}")

    if fad_score < 5.0:
        print(f"   âœ… ë§¤ìš° ìœ ì‚¬ (FAD < 5.0)")
        print(f"   â†’ ì‹¤ì œ ì¬ì¦ˆì™€ ê±°ì˜ êµ¬ë¶„ ë¶ˆê°€")
    elif fad_score < 10.0:
        print(f"   âœ… ìœ ì‚¬ (FAD < 10.0)")
        print(f"   â†’ ì¬ì¦ˆ ìŠ¤íƒ€ì¼ ì˜ í•™ìŠµë¨")
    elif fad_score < 20.0:
        print(f"   ğŸŸ¡ ë³´í†µ (FAD < 20.0)")
        print(f"   â†’ ì–´ëŠ ì •ë„ ì¬ì¦ˆ ëŠë‚Œì€ ìˆìŒ")
    else:
        print(f"   âŒ ì°¨ì´ í¼ (FAD >= 20.0)")
        print(f"   â†’ ì¬ì¦ˆ ìŠ¤íƒ€ì¼ í•™ìŠµ ë¶€ì¡±")

    print("=" * 60)

    # 5. ê²°ê³¼ ì €ì¥
    Path(output_dir).mkdir(exist_ok=True)
    result_file = f"{output_dir}/fad_score.txt"

    with open(result_file, 'w') as f:
        f.write(f"FAD Score: {fad_score:.2f}\n")
        f.write(f"Generated samples: {len(generated_audio)}\n")
        f.write(f"Reference samples: {len(reference_audio)}\n")

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {result_file}")


def main():
    parser = argparse.ArgumentParser(description="FAD ê³„ì‚°")
    parser.add_argument(
        '--generated_dir',
        type=str,
        required=True,
        help='ìƒì„±ëœ ì˜¤ë””ì˜¤ í´ë”'
    )
    parser.add_argument(
        '--reference_dir',
        type=str,
        required=True,
        help='ë ˆí¼ëŸ°ìŠ¤ ì¬ì¦ˆ ì˜¤ë””ì˜¤ í´ë”'
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        default='./evaluation',
        help='ê²°ê³¼ ì €ì¥ í´ë”'
    )

    args = parser.parse_args()

    evaluate_fad(args.generated_dir, args.reference_dir, args.output_dir)


if __name__ == '__main__':
    main()
