# Magenta RealTime ì‘ë™ ì›ë¦¬ ì™„ë²½ ë¶„ì„

Magenta RealTimeì´ **ì‹¤ì œë¡œ ì–´ë–»ê²Œ ì‹¤ì‹œê°„ ìŒì•…ì„ ìƒì„±í•˜ëŠ”ì§€** ë‚´ë¶€ êµ¬ì¡°ë¶€í„° í•™ìŠµ ë°©ë²•ê¹Œì§€ ì „ë¶€ ë¶„ì„í•œ ê°€ì´ë“œì•¼.

---

## ëª©ì°¨

1. [ì „ì²´ êµ¬ì¡° ê°œìš”](#ì „ì²´-êµ¬ì¡°-ê°œìš”)
2. [í•µì‹¬ ì»´í¬ë„ŒíŠ¸ 3ê°€ì§€](#í•µì‹¬-ì»´í¬ë„ŒíŠ¸-3ê°€ì§€)
3. [ìƒì„± í”„ë¡œì„¸ìŠ¤ (ì‹¤ì œ ì‘ë™ íë¦„)](#ìƒì„±-í”„ë¡œì„¸ìŠ¤)
4. [í•™ìŠµ ë°©ë²•](#í•™ìŠµ-ë°©ë²•)
5. [íŒŒì¸íŠœë‹ ì‘ë™ ì›ë¦¬](#íŒŒì¸íŠœë‹-ì‘ë™-ì›ë¦¬)
6. [ì‹¤ì‹œê°„ ìƒì„± ìµœì í™”](#ì‹¤ì‹œê°„-ìƒì„±-ìµœì í™”)
7. [ì½”ë“œ ë ˆë²¨ ë¶„ì„](#ì½”ë“œ-ë ˆë²¨-ë¶„ì„)

---

## ì „ì²´ êµ¬ì¡° ê°œìš”

### Magenta RealTime = 3ê°€ì§€ ëª¨ë¸ì˜ ì¡°í•©

```
ì…ë ¥ (í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸)
    â†“
[1] MusicCoCa (í…ìŠ¤íŠ¸ â†’ ìŒì•… ì„ë² ë”©)
    â†“
[2] Transformer (ì„ë² ë”© â†’ ì˜¤ë””ì˜¤ í† í°)
    â†“
[3] SpectroStream (í† í° â†’ ì‹¤ì œ ì˜¤ë””ì˜¤)
    â†“
ì¶œë ¥ (48kHz ìŠ¤í…Œë ˆì˜¤ ì˜¤ë””ì˜¤)
```

**í•µì‹¬ ì•„ì´ë””ì–´**:
- ì˜¤ë””ì˜¤ë¥¼ **discrete tokens**ë¡œ ë³€í™˜ (VQ-VAE ë°©ì‹)
- Transformerë¡œ **í† í° ì‹œí€€ìŠ¤** ìƒì„±
- í† í°ì„ ë‹¤ì‹œ **ì˜¤ë””ì˜¤ë¡œ ë³µì›**

---

## í•µì‹¬ ì»´í¬ë„ŒíŠ¸ 3ê°€ì§€

### 1. MusicCoCa (Music Contrastive Captioner)

**ì—­í• **: í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìŒì•… ìŠ¤íƒ€ì¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜

**ì‘ë™ ë°©ì‹**:
```python
# "Bill Evans modal jazz piano" ê°™ì€ í…ìŠ¤íŠ¸ ì…ë ¥
text_prompt = "Bill Evans modal jazz piano, slow tempo"

# MusicCoCaê°€ 512ì°¨ì› ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
style_embedding = MusicCoCa(text_prompt)
# shape: (512,)

# ì´ ì„ë² ë”©ì´ Transformerì˜ conditioning ì‹ í˜¸ê°€ ë¨
```

**í•™ìŠµ ë°©ë²•**:
- **Contrastive Learning**: í…ìŠ¤íŠ¸ì™€ ì˜¤ë””ì˜¤ë¥¼ ê°™ì€ ê³µê°„ì— ë§¤í•‘
- YouTube Music Description Dataset (ìˆ˜ë°±ë§Œ ê³¡)
- "ì¬ì¦ˆ í”¼ì•„ë…¸" í…ìŠ¤íŠ¸ â†” ì‹¤ì œ ì¬ì¦ˆ í”¼ì•„ë…¸ ì˜¤ë””ì˜¤ë¥¼ ê°€ê¹ê²Œ ë°°ì¹˜

**ì™œ ì¤‘ìš”í•œê°€**:
- ë„¤ê°€ "ohhalim jazz style"ì´ë¼ê³  ì…ë ¥í•˜ë©´, ì´ê²Œ ìˆ˜ì¹˜ ë²¡í„°ë¡œ ë³€í™˜ë¼ì„œ ëª¨ë¸ì— ì „ë‹¬ë¨
- íŒŒì¸íŠœë‹ ì‹œ **ì´ ì„ë² ë”© ê³µê°„ì„ ì¡°ì •**í•´ì„œ ë„¤ ìŠ¤íƒ€ì¼ì„ í•™ìŠµí•¨

---

### 2. Transformer (Token Sequence Model)

**ì—­í• **: ìŠ¤íƒ€ì¼ ì„ë² ë”©ì„ ë°›ì•„ì„œ ì˜¤ë””ì˜¤ í† í° ì‹œí€€ìŠ¤ ìƒì„±

**êµ¬ì¡°**:
```
ëª¨ë¸ í¬ê¸°: 760M íŒŒë¼ë¯¸í„°
ë ˆì´ì–´ ìˆ˜: 24 layers
í—¤ë“œ ìˆ˜: 16 attention heads
ì„ë² ë”© ì°¨ì›: 1024
```

**ì‘ë™ ë°©ì‹**:
```python
# ì…ë ¥ = ì´ì „ ì˜¤ë””ì˜¤ í† í°ë“¤ + ìŠ¤íƒ€ì¼ ì„ë² ë”©
previous_tokens = [t1, t2, t3, ..., t_n]  # ì´ì „ì— ìƒì„±ëœ í† í°ë“¤
style_embedding = [512ì°¨ì› ë²¡í„°]

# Transformerê°€ ë‹¤ìŒ í† í° ì˜ˆì¸¡
next_token = Transformer(
    previous_tokens,
    conditioning=style_embedding
)

# Autoregressive ë°©ì‹ìœ¼ë¡œ ê³„ì† ìƒì„±
# t_1 â†’ t_2 â†’ t_3 â†’ ... â†’ t_n
```

**Attention Mechanism**:
- **Self-Attention**: ì´ì „ í† í°ë“¤ ê°„ì˜ ê´€ê³„ íŒŒì•… (ë©œë¡œë”” íŒ¨í„´, ë¦¬ë“¬)
- **Cross-Attention**: ìŠ¤íƒ€ì¼ ì„ë² ë”©ê³¼ í† í°ì˜ ê´€ê³„ (ìŠ¤íƒ€ì¼ ì¼ê´€ì„± ìœ ì§€)

**Context Window**:
- **10ì´ˆ ì»¨í…ìŠ¤íŠ¸**: 10ì´ˆ ì „ê¹Œì§€ì˜ ì˜¤ë””ì˜¤ë¥¼ "ê¸°ì–µ"
- ê¸´ ì¦‰í¥ì—°ì£¼ì—ì„œë„ ì¼ê´€ì„± ìœ ì§€

---

### 3. SpectroStream (Audio Codec)

**ì—­í• **: ì˜¤ë””ì˜¤ â†” í† í° ë³€í™˜ (ì••ì¶•/ë³µì›)

**êµ¬ì¡°**:
```
ì˜¤ë””ì˜¤ (48kHz)
    â†“ Encoder (8x ë‹¤ìš´ìƒ˜í”Œ)
ìŠ¤í™íŠ¸ë¡œê·¸ë¨ (6kHz)
    â†“ RVQ (Residual Vector Quantization)
1024ê°œ í† í° / ì´ˆ
    â†“ Decoder (8x ì—…ìƒ˜í”Œ)
ì˜¤ë””ì˜¤ (48kHz)
```

**RVQ (Residual Vector Quantization)**:

ì˜¤ë””ì˜¤ë¥¼ ì—¬ëŸ¬ ë ˆë²¨ë¡œ ì–‘ìí™”í•´ì„œ ì •ë³´ ì†ì‹¤ ìµœì†Œí™”

```python
# ì›ë³¸ ì˜¤ë””ì˜¤ â†’ ìŠ¤í™íŠ¸ë¡œê·¸ë¨
spectrogram = STFT(audio)

# 8ê°œ ë ˆë²¨ë¡œ ì–‘ìí™”
codebook_size = 2048  # ê° ë ˆë²¨ë§ˆë‹¤ 2048ê°œ ì½”ë“œ
num_levels = 8

tokens = []
residual = spectrogram

for level in range(8):
    # ê°€ì¥ ê°€ê¹Œìš´ ì½”ë“œ ì°¾ê¸°
    code = find_nearest_code(residual, codebook[level])
    tokens.append(code)

    # ì”ì°¨(residual) ê³„ì‚°
    residual = residual - decode(code)

# ìµœì¢… í† í°: [level_0, level_1, ..., level_7]
# â†’ 8 tokens per frame
```

**ì••ì¶•ë¥ **:
- ì›ë³¸: 48kHz Ã— 2ì±„ë„ = 96k samples/sec
- ì••ì¶• í›„: 1024 tokens/sec (ì•½ 94ë°° ì••ì¶•)

**ìŒì§ˆ**:
- SNR (Signal-to-Noise Ratio): ~40dB
- ì‚¬ëŒ ê·€ë¡œëŠ” ì›ë³¸ê³¼ ê±°ì˜ êµ¬ë¶„ ë¶ˆê°€

---

## ìƒì„± í”„ë¡œì„¸ìŠ¤

### ì‹¤ì œ ì‘ë™ íë¦„ (2ì´ˆ ì²­í¬ ìƒì„±)

```python
# Step 1: í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ â†’ ìŠ¤íƒ€ì¼ ì„ë² ë”©
prompt = "ohhalim jazz piano style, modal improvisation"
style_emb = MusicCoCa.encode(prompt)  # (512,)

# Step 2: ì´ˆê¸° ìƒíƒœ (ì²˜ìŒ ìƒì„± ì‹œ)
state = {
    'prev_tokens': [],      # ì´ì „ í† í° (ë¹ˆ ìƒíƒœ)
    'kv_cache': None,       # Attention ìºì‹œ (ì†ë„ ìµœì í™”)
    'context_audio': None   # 10ì´ˆ ì»¨í…ìŠ¤íŠ¸
}

# Step 3: 2ì´ˆ ì²­í¬ ìƒì„± (1024 tokens/sec Ã— 2ì´ˆ = 2048 í† í°)
generated_tokens = []

for i in range(2048):
    # Transformerë¡œ ë‹¤ìŒ í† í° ì˜ˆì¸¡
    logits = Transformer(
        input_tokens=state['prev_tokens'][-1024:],  # ìµœê·¼ 1ì´ˆë§Œ ì‚¬ìš©
        style_emb=style_emb,
        kv_cache=state['kv_cache']  # ì´ì „ ê³„ì‚° ì¬ì‚¬ìš©
    )

    # Sampling (top-p, temperature)
    next_token = sample(logits, temperature=0.95, top_p=0.9)
    generated_tokens.append(next_token)

    # ìƒíƒœ ì—…ë°ì´íŠ¸
    state['prev_tokens'].append(next_token)
    state['kv_cache'] = update_kv_cache(state['kv_cache'])

# Step 4: í† í° â†’ ì˜¤ë””ì˜¤ ë³µì›
audio_chunk = SpectroStream.decode(generated_tokens)  # (96000,) = 2ì´ˆ @ 48kHz

# Step 5: ë‹¤ìŒ ì²­í¬ ìƒì„± ì‹œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
state['context_audio'] = audio_chunk
```

---

### Chunk-based Generation (ì™œ 2ì´ˆì”© ìƒì„±í•˜ë‚˜?)

**ë¬¸ì œ**: ê¸´ ì˜¤ë””ì˜¤ë¥¼ í•œ ë²ˆì— ìƒì„±í•˜ë©´ ë©”ëª¨ë¦¬ í­ë°œ

**í•´ê²°ì±…**: 2ì´ˆ ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ìƒì„±

```
ì²­í¬ 1 (0-2ì´ˆ)
    â†“ [context]
ì²­í¬ 2 (2-4ì´ˆ)  â† ì²­í¬ 1ì˜ ë§ˆì§€ë§‰ 0.5ì´ˆë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
    â†“ [context]
ì²­í¬ 3 (4-6ì´ˆ)  â† ì²­í¬ 2ì˜ ë§ˆì§€ë§‰ 0.5ì´ˆë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
    â†“
...
```

**ì˜¤ë²„ë© ê¸°ë²•**:
- ê° ì²­í¬ì˜ ë§ˆì§€ë§‰ 0.5ì´ˆì™€ ë‹¤ìŒ ì²­í¬ì˜ ì²« 0.5ì´ˆë¥¼ **í¬ë¡œìŠ¤í˜ì´ë“œ**
- ëŠê¹€ ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§

---

## í•™ìŠµ ë°©ë²•

### Pre-training (ì‚¬ì „ í•™ìŠµ)

**ë°ì´í„°ì…‹**:
- YouTube Music: ìˆ˜ë°±ë§Œ ì‹œê°„
- FMA (Free Music Archive): 10ë§Œ+ ê³¡
- MusicCaps: 5,500ê³¡ (ê³ í’ˆì§ˆ ì„¤ëª… í¬í•¨)

**3ë‹¨ê³„ í•™ìŠµ**:

#### Stage 1: SpectroStream í•™ìŠµ
```python
# ì˜¤ë””ì˜¤ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ê³  ë‹¤ì‹œ ë³µì›
audio = load_audio()
tokens = SpectroStream.encode(audio)
reconstructed = SpectroStream.decode(tokens)

# Reconstruction Loss
loss = MSE(audio, reconstructed) + perceptual_loss(audio, reconstructed)
```

**ëª©í‘œ**: ì˜¤ë””ì˜¤ â†” í† í° ë³€í™˜ì„ ì™„ë²½í•˜ê²Œ

---

#### Stage 2: Transformer í•™ìŠµ (ìŒì•… ìƒì„±)
```python
# ìŒì•… í† í° ì‹œí€€ìŠ¤ ì˜ˆì¸¡
tokens = [t1, t2, t3, ..., t_n]

for i in range(len(tokens) - 1):
    # ië²ˆì§¸ê¹Œì§€ ë³´ê³  i+1ë²ˆì§¸ ì˜ˆì¸¡
    predicted = Transformer(tokens[:i])
    target = tokens[i+1]

    # Cross-Entropy Loss
    loss += CE(predicted, target)
```

**ëª©í‘œ**: ë‹¤ìŒ í† í°ì„ ì •í™•í•˜ê²Œ ì˜ˆì¸¡ (ì–¸ì–´ ëª¨ë¸ê³¼ ë™ì¼)

---

#### Stage 3: MusicCoCa í•™ìŠµ (í…ìŠ¤íŠ¸-ìŒì•… ë§¤ì¹­)
```python
# Contrastive Learning
text = "slow jazz piano ballad"
audio = matching_jazz_audio

# í…ìŠ¤íŠ¸ì™€ ì˜¤ë””ì˜¤ë¥¼ ê°™ì€ ê³µê°„ì— ì„ë² ë”©
text_emb = MusicCoCa.text_encoder(text)    # (512,)
audio_emb = MusicCoCa.audio_encoder(audio)  # (512,)

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœëŒ€í™”
loss = 1 - cosine_similarity(text_emb, audio_emb)

# ë‹¤ë¥¸ ìŒì•…ê³¼ëŠ” ê±°ë¦¬ ë©€ê²Œ
for other_audio in batch:
    other_emb = MusicCoCa.audio_encoder(other_audio)
    loss += max(0, margin - distance(text_emb, other_emb))
```

**ëª©í‘œ**: "ì¬ì¦ˆ í”¼ì•„ë…¸" í…ìŠ¤íŠ¸ â†’ ì¬ì¦ˆ í”¼ì•„ë…¸ ìŒì•… ì„ë² ë”©ê³¼ ê°€ê¹Œì›Œì§€ê²Œ

---

### Fine-tuning (íŒŒì¸íŠœë‹)

**QLoRA ì‘ë™ ì›ë¦¬**:

```python
# ê¸°ì¡´ Transformer íŒŒë¼ë¯¸í„°ëŠ” freeze (ê³ ì •)
for param in Transformer.parameters():
    param.requires_grad = False

# LoRA ì–´ëŒ‘í„°ë§Œ í•™ìŠµ
# ì›ë˜ Weight: W (1024 Ã— 1024)
# LoRA: W_A (1024 Ã— 8) Ã— W_B (8 Ã— 1024)
# â†’ 8ë°° ì‘ì€ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ

class LoRALayer:
    def __init__(self, in_dim=1024, out_dim=1024, rank=8):
        self.W_A = nn.Linear(in_dim, rank, bias=False)  # (1024, 8)
        self.W_B = nn.Linear(rank, out_dim, bias=False)  # (8, 1024)

    def forward(self, x):
        # ì›ë˜ ë³€í™˜ + LoRA ë³€í™˜
        original = self.original_weight @ x
        lora_delta = self.W_B(self.W_A(x))

        return original + lora_delta  # ì›ë³¸ì— ì‘ì€ ë³€í™” ì¶”ê°€

# í•™ìŠµ
for audio in your_jazz_dataset:
    tokens = SpectroStream.encode(audio)

    # ë‹¤ìŒ í† í° ì˜ˆì¸¡ (LoRAë¡œë§Œ ì¡°ì •)
    predicted = Transformer(tokens[:-1])  # LoRAê°€ ì ìš©ëœ ì¶œë ¥
    loss = CE(predicted, tokens[1:])

    # LoRA íŒŒë¼ë¯¸í„°ë§Œ ì—…ë°ì´íŠ¸
    loss.backward()
    optimizer.step()  # W_A, W_Bë§Œ ì—…ë°ì´íŠ¸
```

**ì™œ QLoRAê°€ íš¨ìœ¨ì ì¸ê°€**:
1. **ë©”ëª¨ë¦¬ ì ˆì•½**: 760M â†’ 2M íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ (0.3%)
2. **ë¹ ë¥¸ í•™ìŠµ**: ì ì€ íŒŒë¼ë¯¸í„° = ë¹ ë¥¸ ìˆ˜ë ´
3. **ê³¼ì í•© ë°©ì§€**: ë„ˆë¬´ ë§ì´ ë³€í•˜ì§€ ì•ŠìŒ

---

## íŒŒì¸íŠœë‹ ì‘ë™ ì›ë¦¬

### ë„¤ ìŠ¤íƒ€ì¼ì„ ì–´ë–»ê²Œ í•™ìŠµí•˜ë‚˜?

**1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„**
```python
# ë„¤ ì¬ì¦ˆ ì—°ì£¼ 20ê°œ
your_jazz_files = [
    "ohhalim_improv_01.wav",
    "ohhalim_improv_02.wav",
    ...
    "ohhalim_improv_20.wav"
]

# SpectroStreamìœ¼ë¡œ í† í°í™”
tokenized_dataset = []
for audio_file in your_jazz_files:
    audio = load_audio(audio_file)
    tokens = SpectroStream.encode(audio)
    tokenized_dataset.append(tokens)
```

**2ë‹¨ê³„: LoRA í•™ìŠµ**
```python
# "ohhalim jazz style" â†’ ìŠ¤íƒ€ì¼ ì„ë² ë”©
style_prompt = "ohhalim jazz piano improvisation style"
style_emb = MusicCoCa.encode(style_prompt)

# ë„¤ ë°ì´í„°ë¡œ Transformer ì¡°ì •
for epoch in range(50):
    for tokens in tokenized_dataset:
        # ì´ ìŠ¤íƒ€ì¼ë¡œ ìƒì„±í–ˆì„ ë•Œ, ë„¤ í† í°ê³¼ ì¼ì¹˜í•˜ê²Œ
        predicted_tokens = Transformer(
            prev_tokens=tokens[:-1],
            style_emb=style_emb  # "ohhalim style"
        )

        # Loss: ì˜ˆì¸¡ì´ ì‹¤ì œ ë„¤ ì—°ì£¼ì™€ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ê°€
        loss = CrossEntropy(predicted_tokens, tokens[1:])

        # LoRAë§Œ ì—…ë°ì´íŠ¸
        loss.backward()
        optimizer.step()  # W_A, W_B ì¡°ì •
```

**3ë‹¨ê³„: ê²°ê³¼**

íŒŒì¸íŠœë‹ í›„:
- "ohhalim jazz style" í”„ë¡¬í”„íŠ¸ â†’ **ë„¤ ìŠ¤íƒ€ì¼ì˜ ì¬ì¦ˆ** ìƒì„±
- ë„¤ê°€ ìì£¼ ì“°ëŠ” ì½”ë“œ ì§„í–‰, ë¦¬ë“¬ íŒ¨í„´, í„°ì¹˜ê°ì´ ë°˜ì˜ë¨

**ì™œ ì‘ë™í•˜ë‚˜?**
- LoRAê°€ ì›ë³¸ ëª¨ë¸ì— **"ë„¤ ìŠ¤íƒ€ì¼ì˜ bias"**ë¥¼ ì¶”ê°€í•¨
- ë² ì´ìŠ¤ ëª¨ë¸: "ì¼ë°˜ì ì¸ ì¬ì¦ˆ"
- LoRA: "+ohhalim íŠ¹ìœ ì˜ íŒ¨í„´"
- ê²°ê³¼: "ì¼ë°˜ ì¬ì¦ˆ + ë„¤ ìŠ¤íƒ€ì¼"

---

## ì‹¤ì‹œê°„ ìƒì„± ìµœì í™”

### RTF (Real-Time Factor) 1.6x ë‹¬ì„± ë°©ë²•

**ë¬¸ì œ**: TransformerëŠ” ëŠë¦¼ (íŠ¹íˆ ê¸´ ì‹œí€€ìŠ¤)

**í•´ê²°ì±… 5ê°€ì§€**:

#### 1. KV-Cache (Attention ìºì‹œ)
```python
# ë§¤ë²ˆ ëª¨ë“  í† í°ì„ ë‹¤ì‹œ ê³„ì‚°í•˜ì§€ ì•ŠìŒ
# ì´ì „ í† í°ì˜ Key, Valueë¥¼ ì €ì¥

class TransformerWithCache:
    def forward(self, new_token, kv_cache=None):
        # ìƒˆ í† í°ì˜ Q, K, Vë§Œ ê³„ì‚°
        Q_new = self.query(new_token)
        K_new = self.key(new_token)
        V_new = self.value(new_token)

        if kv_cache is not None:
            # ì´ì „ K, V ì¬ì‚¬ìš©
            K_all = torch.cat([kv_cache['K'], K_new], dim=1)
            V_all = torch.cat([kv_cache['V'], V_new], dim=1)
        else:
            K_all = K_new
            V_all = V_new

        # Attention ê³„ì‚°
        attention = softmax(Q_new @ K_all.T / sqrt(d_k))
        output = attention @ V_all

        # ìºì‹œ ì—…ë°ì´íŠ¸
        new_cache = {'K': K_all, 'V': V_all}

        return output, new_cache
```

**ì†ë„ ê°œì„ **: ~3ë°° ë¹ ë¦„

---

#### 2. Chunk-based Generation

2ì´ˆì”©ë§Œ ìƒì„± â†’ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¼ì •

---

#### 3. Mixed Precision (FP16)

```python
# 32ë¹„íŠ¸ â†’ 16ë¹„íŠ¸ ì—°ì‚°
model = model.half()  # FP16

# ë©”ëª¨ë¦¬ 50% ì ˆì•½, ì†ë„ 2ë°° í–¥ìƒ
```

---

#### 4. Speculative Decoding

```python
# ì‘ì€ ëª¨ë¸ë¡œ ì—¬ëŸ¬ í† í° ë¨¼ì € ì˜ˆì¸¡
small_model_predictions = small_model.generate(n=5)  # 5ê°œ ì˜ˆì¸¡

# í° ëª¨ë¸ë¡œ í•œ ë²ˆì— ê²€ì¦
verified = large_model.verify(small_model_predictions)

# ë§ìœ¼ë©´ 5ê°œ í•œ ë²ˆì— ìˆ˜ë½, í‹€ë¦¬ë©´ 1ê°œë§Œ
if all(verified):
    tokens.extend(small_model_predictions)  # 5ë°° ì†ë„
else:
    tokens.append(verified[0])  # ì •í™•ë„ ìœ ì§€
```

---

#### 5. Batching

ì—¬ëŸ¬ ìƒì„± ìš”ì²­ì„ í•œ ë²ˆì— ì²˜ë¦¬

---

## ì½”ë“œ ë ˆë²¨ ë¶„ì„

### ì‹¤ì œ ìƒì„± ì½”ë“œ (ì˜ì‚¬ì½”ë“œ)

```python
class MagentaRT:
    def __init__(self):
        self.musiccoca = MusicCoCa()          # í…ìŠ¤íŠ¸ ì¸ì½”ë”
        self.transformer = Transformer()       # 760M íŒŒë¼ë¯¸í„°
        self.spectrostream = SpectroStream()   # ì˜¤ë””ì˜¤ ì½”ë±

    def generate(self, prompt, duration=16):
        # 1. í…ìŠ¤íŠ¸ â†’ ìŠ¤íƒ€ì¼ ì„ë² ë”©
        style_emb = self.musiccoca.encode(prompt)

        # 2. ì²­í¬ ë‹¨ìœ„ ìƒì„±
        num_chunks = duration // 2  # 2ì´ˆ ì²­í¬
        chunks = []
        state = None

        for i in range(num_chunks):
            # í† í° ìƒì„±
            tokens, state = self.generate_chunk(
                style_emb=style_emb,
                state=state
            )

            # í† í° â†’ ì˜¤ë””ì˜¤
            audio_chunk = self.spectrostream.decode(tokens)
            chunks.append(audio_chunk)

        # 3. ì²­í¬ í•©ì¹˜ê¸° (í¬ë¡œìŠ¤í˜ì´ë“œ)
        final_audio = self.concatenate_chunks(chunks)

        return final_audio

    def generate_chunk(self, style_emb, state):
        """2ì´ˆ ì²­í¬ ìƒì„±"""
        tokens = []

        # 2048 í† í° = 2ì´ˆ
        for _ in range(2048):
            # Transformer ì¶”ë¡ 
            logits, state = self.transformer(
                prev_tokens=tokens[-1024:],  # ìµœê·¼ 1ì´ˆ
                style_emb=style_emb,
                state=state  # KV-cache
            )

            # Sampling
            next_token = self.sample(logits, temp=0.95)
            tokens.append(next_token)

        return tokens, state

    def sample(self, logits, temp=1.0, top_p=0.9):
        """Temperature + Nucleus Sampling"""
        # Temperature scaling
        logits = logits / temp

        # Softmax
        probs = softmax(logits)

        # Top-p (nucleus) sampling
        sorted_probs, indices = torch.sort(probs, descending=True)
        cumsum = torch.cumsum(sorted_probs, dim=0)

        # ëˆ„ì  í™•ë¥ ì´ top_p ë„˜ëŠ” ìˆœê°„ê¹Œì§€ë§Œ
        cutoff = (cumsum <= top_p).sum()
        top_probs = sorted_probs[:cutoff]
        top_indices = indices[:cutoff]

        # ì¬ì •ê·œí™” í›„ ìƒ˜í”Œë§
        top_probs = top_probs / top_probs.sum()
        sampled_idx = torch.multinomial(top_probs, 1)

        return top_indices[sampled_idx]
```

---

### íŒŒì¸íŠœë‹ ì½”ë“œ (ì˜ì‚¬ì½”ë“œ)

```python
class FineTuner:
    def __init__(self, base_model):
        self.model = base_model

        # LoRA ì–´ëŒ‘í„° ì¶”ê°€
        self.add_lora_adapters(rank=8, alpha=16)

        # ë² ì´ìŠ¤ ëª¨ë¸ freeze
        for param in self.model.transformer.parameters():
            param.requires_grad = False

    def add_lora_adapters(self, rank, alpha):
        """ëª¨ë“  Attention layerì— LoRA ì¶”ê°€"""
        for layer in self.model.transformer.layers:
            # Q, K, V, O projectionì— LoRA ì ìš©
            layer.attention.q_proj = LoRALinear(layer.attention.q_proj, rank, alpha)
            layer.attention.k_proj = LoRALinear(layer.attention.k_proj, rank, alpha)
            layer.attention.v_proj = LoRALinear(layer.attention.v_proj, rank, alpha)
            layer.attention.o_proj = LoRALinear(layer.attention.o_proj, rank, alpha)

    def train(self, dataset, style_prompt="ohhalim jazz style"):
        style_emb = self.model.musiccoca.encode(style_prompt)

        optimizer = torch.optim.AdamW(
            [p for p in self.model.parameters() if p.requires_grad],
            lr=1e-4
        )

        for epoch in range(50):
            total_loss = 0

            for audio in dataset:
                # í† í°í™”
                tokens = self.model.spectrostream.encode(audio)

                # ë‹¤ìŒ í† í° ì˜ˆì¸¡
                for i in range(len(tokens) - 1):
                    # ì…ë ¥: tokens[0:i], ìŠ¤íƒ€ì¼ ì„ë² ë”©
                    logits = self.model.transformer(
                        tokens[:i],
                        style_emb=style_emb
                    )

                    # íƒ€ê²Ÿ: tokens[i+1]
                    loss = F.cross_entropy(logits, tokens[i+1])

                    # ì—­ì „íŒŒ (LoRAë§Œ ì—…ë°ì´íŠ¸)
                    loss.backward()
                    optimizer.step()
                    optimizer.zero_grad()

                    total_loss += loss.item()

            print(f"Epoch {epoch}: Loss = {total_loss / len(dataset)}")

        # LoRA ì–´ëŒ‘í„° ì €ì¥
        self.save_lora_adapters("ohhalim-jazz-style/")

class LoRALinear(nn.Module):
    def __init__(self, original_layer, rank, alpha):
        super().__init__()
        self.original = original_layer
        self.original.requires_grad_(False)  # Freeze

        in_dim = original_layer.in_features
        out_dim = original_layer.out_features

        # LoRA íŒŒë¼ë¯¸í„°
        self.lora_A = nn.Linear(in_dim, rank, bias=False)
        self.lora_B = nn.Linear(rank, out_dim, bias=False)
        self.scaling = alpha / rank

        # ì´ˆê¸°í™”
        nn.init.kaiming_uniform_(self.lora_A.weight)
        nn.init.zeros_(self.lora_B.weight)

    def forward(self, x):
        # ì›ë³¸ ì¶œë ¥
        original_out = self.original(x)

        # LoRA ì¶œë ¥
        lora_out = self.lora_B(self.lora_A(x)) * self.scaling

        # í•©ì¹˜ê¸°
        return original_out + lora_out
```

---

## ìš”ì•½

### Magenta RealTime í•µì‹¬ ë™ì‘ ì›ë¦¬

1. **í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸** â†’ MusicCoCa â†’ **ìŠ¤íƒ€ì¼ ì„ë² ë”©**
2. **ìŠ¤íƒ€ì¼ ì„ë² ë”©** â†’ Transformer â†’ **ì˜¤ë””ì˜¤ í† í° ì‹œí€€ìŠ¤**
3. **í† í° ì‹œí€€ìŠ¤** â†’ SpectroStream â†’ **48kHz ì˜¤ë””ì˜¤**

### íŒŒì¸íŠœë‹ í•µì‹¬

- **QLoRA**: 760M íŒŒë¼ë¯¸í„° ì¤‘ 2Më§Œ í•™ìŠµ (0.3%)
- ë„¤ ì¬ì¦ˆ ë°ì´í„°ë¡œ LoRA ì–´ëŒ‘í„° ì¡°ì •
- "ohhalim style" í”„ë¡¬í”„íŠ¸ â†’ ë„¤ ìŠ¤íƒ€ì¼ ì¬ì¦ˆ ìƒì„±

### ì‹¤ì‹œê°„ ìƒì„± ë¹„ë²•

- **KV-Cache**: ì´ì „ ê³„ì‚° ì¬ì‚¬ìš©
- **Chunk ìƒì„±**: 2ì´ˆì”© ë‚˜ëˆ ì„œ
- **FP16**: ë©”ëª¨ë¦¬/ì†ë„ 2ë°° ê°œì„ 
- **Speculative Decoding**: ì‘ì€ ëª¨ë¸ë¡œ ë¯¸ë¦¬ ì˜ˆì¸¡

---

ì´ì œ Magenta RTê°€ **ë‚´ë¶€ì—ì„œ ì–´ë–»ê²Œ ëŒì•„ê°€ëŠ”ì§€** ì™„ì „íˆ ì´í•´í–ˆì„ ê±°ì•¼! ğŸ¹âœ¨

íŒŒì¸íŠœë‹í•  ë•Œ ì´ ì›ë¦¬ë¥¼ ì•Œë©´ í›¨ì”¬ íš¨ê³¼ì ìœ¼ë¡œ í•  ìˆ˜ ìˆì–´.
