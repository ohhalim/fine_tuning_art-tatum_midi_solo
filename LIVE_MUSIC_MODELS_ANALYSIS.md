# Live Music Models - ë…¼ë¬¸ ë¶„ì„

**Paper**: Live Music Models (NeurIPS 2025 Creative AI Track)
**Authors**: Lyria Team, Google DeepMind
**ArXiv**: 2508.04651

---

## ğŸ¯ í•µì‹¬ ìš”ì•½ (3ì¤„)

1. **ì‹¤ì‹œê°„ ìŒì•… ìƒì„±**: ì‚¬ìš©ì ì…ë ¥ì— ì¦‰ê° ë°˜ì‘í•˜ëŠ” ì—°ì†ì  ìŒì•… ìŠ¤íŠ¸ë¦¼
2. **Magenta RealTime**: ì˜¤í”ˆì†ŒìŠ¤, 760M íŒŒë¼ë¯¸í„°, RTF 1.8x (ì‹¤ì‹œê°„ë³´ë‹¤ 1.8ë°° ë¹ ë¦„)
3. **Chunk-based generation**: 2ì´ˆ ì²­í¬ ë‹¨ìœ„ë¡œ ë¬´í•œíˆ ìƒì„±, 10ì´ˆ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€

---

## ğŸ“– 1. ë…¼ë¬¸ì˜ í•µì‹¬ ê°œë…

### 1.1 Live Music Modelì´ë€?

**ê¸°ì¡´ AI ìŒì•… ìƒì„± (Offline)**:
```
ì‚¬ìš©ì: "ì¬ì¦ˆ í”¼ì•„ë…¸ ìŒì•… ë§Œë“¤ì–´ì¤˜"
    â†“ ê¸°ë‹¤ë¦¼ (10-30ì´ˆ)
AI: [ì™„ì„±ëœ 30ì´ˆ ìŒì•… íŒŒì¼]
```

**Live Music Model (Real-time)**:
```
ì‚¬ìš©ì: [ì‹¤ì‹œê°„ìœ¼ë¡œ ì»¨íŠ¸ë¡¤ ì…ë ¥]
    â†“ ì¦‰ê° ë°˜ì‘ (<1ì´ˆ ì§€ì—°)
AI: [ëŠì„ì—†ì´ íë¥´ëŠ” ìŒì•… ìŠ¤íŠ¸ë¦¼]
    â†“ ì‚¬ìš©ìê°€ ì»¨íŠ¸ë¡¤ ë³€ê²½
AI: [ìŒì•…ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë³€í™”]
```

### 1.2 Live Music Modelì˜ 3ê°€ì§€ í•„ìˆ˜ ì¡°ê±´

1. **Real-time generation**: RTF â‰¥ 1x (ì‹¤ì‹œê°„ë³´ë‹¤ ë¹ ë¥´ê²Œ ìƒì„±)
2. **Causal streaming**: ì—°ì†ì ìœ¼ë¡œ ìƒì„±, ê³¼ê±° ì¶œë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ìƒì„±
3. **Responsive controls**: ë‚®ì€ ì§€ì—°ì‹œê°„ (ì‚¬ìš©ì ì…ë ¥ì— ì¦‰ê° ë°˜ì‘)

---

## ğŸ—ï¸ 2. Magenta RealTime ì•„í‚¤í…ì²˜

### ì „ì²´ íŒŒì´í”„ë¼ì¸

```
User Input (Text/Audio Prompt)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. MusicCoCa (Style Embedding)         â”‚
â”‚  - Text â†’ 768D vector                   â”‚
â”‚  - Audio â†’ 768D vector                  â”‚
â”‚  - Quantized to 12 tokens              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Encoder-Decoder Transformer         â”‚
â”‚                                          â”‚
â”‚  Encoder (Bidirectional):               â”‚
â”‚  - 10s audio context (4 RVQ depth)      â”‚
â”‚  - 12 style tokens                      â”‚
â”‚  - Total: 1012 tokens                   â”‚
â”‚                                          â”‚
â”‚  Decoder (Causal):                      â”‚
â”‚  - "Temporal" module: frame-level       â”‚
â”‚  - "Depth" module: RVQ prediction       â”‚
â”‚  - Generates 2s chunk (16 RVQ depth)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. SpectroStream (Audio Codec)         â”‚
â”‚  - Discrete tokens â†’ Audio              â”‚
â”‚  - 48kHz stereo, 16kbps                 â”‚
â”‚  - RVQ: 25Hz frame rate, 64 depth       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Output: 2 seconds of audio
    â†“
Append to context â†’ Generate next chunk...
```

---

## ğŸ”¬ 3. í•µì‹¬ ê¸°ìˆ  ìƒì„¸

### 3.1 SpectroStream Audio Codec

**ëª©ì **: ì˜¤ë””ì˜¤ë¥¼ discrete tokensìœ¼ë¡œ ë³€í™˜ (LLMì²˜ëŸ¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´)

```python
# Audio â†’ Tokens
audio = [48000 samples/sec Ã— 2 channels]  # 48kHz stereo
tokens = SpectroStream.encode(audio)

# Token êµ¬ì¡°:
# - Frame rate: 25Hz (ì´ˆë‹¹ 25 í”„ë ˆì„)
# - RVQ depth: 64 levels (hierarchical quantization)
# - Vocabulary: 1024 tokens per level
# - Bandwidth: 16kbps

# 2ì´ˆ ì˜¤ë””ì˜¤ = 50 frames Ã— 64 RVQ levels = 3200 tokens
```

**RVQ (Residual Vector Quantization) ê³„ì¸µ êµ¬ì¡°**:
```
Level 1-4:   Coarse (ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ - í”¼ì¹˜, ë¦¬ë“¬)
Level 5-16:  Medium (ìŒìƒ‰, í•˜ëª¨ë‹‰ìŠ¤)
Level 17-64: Fine (ì„¸ë°€í•œ ë””í…Œì¼, ë…¸ì´ì¦ˆ)
```

**ì‹¤ì‹œê°„ ìµœì í™”**:
- Training: 6 RVQ levelsë§Œ ì‚¬ìš©
- Context: 4 RVQ levels (coarse only)
- Generation: 16 RVQ levels (high fidelity)
- â†’ ë©”ëª¨ë¦¬ì™€ ì†ë„ ìµœì í™”!

### 3.2 MusicCoCa (Style Embedding Model)

**ëª©ì **: Textì™€ Audioë¥¼ ê°™ì€ ê³µê°„ì— ì„ë² ë”© â†’ ìŠ¤íƒ€ì¼ ì»¨íŠ¸ë¡¤

```python
# Architecture:
MusicCoCa = {
    'audio_tower': ViT-12layers,      # Vision Transformer for mel-spectrogram
    'text_tower': Transformer-12layers,
    'text_decoder': Transformer-3layers,  # Regularizationìš©
    'embedding_dim': 768,
    'quantized_tokens': 12,
    'codebook_size': 1024
}

# Usage:
text_embedding = MusicCoCa.text("jazz piano, upbeat")
audio_embedding = MusicCoCa.audio("reference_track.mp3")

# Weighted mixing:
style = 0.7 * text_embedding + 0.3 * audio_embedding

# â†’ 12 discrete tokensë¡œ quantization
style_tokens = quantize(style)  # [12 tokens]
```

**ì…ë ¥ ì‚¬ì–‘**:
- Audio: 10ì´ˆ, 16kHz, log-mel spectrogram (128 channels)
- Text: ìµœëŒ€ 128 tokens
- Output: 768D vector â†’ 12 discrete tokens

**ì¥ì **:
```python
# Embedding arithmetic ê°€ëŠ¥!
techno = embed("techno")
flute = embed("flute")
techno_flute = 0.5 * techno + 0.5 * flute
# â†’ "techno with flute" ìŠ¤íƒ€ì¼!

# Multiple prompts blending:
style = (
    2.0 * embed("brad mehldau piano") +
    1.0 * embed("bebop jazz") +
    0.5 * embed(my_audio_sample)
) / 3.5
```

### 3.3 Chunk-based Autoregression

**ë¬¸ì œ**: ë¬´í•œíˆ ê¸´ ìŒì•…ì„ ì–´ë–»ê²Œ ìƒì„±?

**ê¸°ì¡´ ë°©ì‹ (Sliding Window)**:
```
[í† í°1, í† í°2, í† í°3, ..., í† í°10000] â†’ ë©”ëª¨ë¦¬ í­ë°œ!
```

**Magenta RT ë°©ì‹ (Chunk-based)**:
```python
# Chunk = 2ì´ˆ ë‹¨ìœ„
# Context = ìµœê·¼ 5 chunks (10ì´ˆ)

state = None
while True:  # ë¬´í•œ ìƒì„±!
    # 1. Encoder: 10ì´ˆ ì»¨í…ìŠ¤íŠ¸ + ìŠ¤íƒ€ì¼ ì²˜ë¦¬
    encoder_input = [
        Coarse(chunk_i-5),  # 10ì´ˆ ì „
        Coarse(chunk_i-4),  # 8ì´ˆ ì „
        Coarse(chunk_i-3),  # 6ì´ˆ ì „
        Coarse(chunk_i-2),  # 4ì´ˆ ì „
        Coarse(chunk_i-1),  # 2ì´ˆ ì „ (ë°”ë¡œ ì§ì „)
        style_tokens        # 12 tokens
    ]  # Total: 1012 tokens

    # 2. Decoder: ë‹¤ìŒ 2ì´ˆ ìƒì„±
    chunk_i = decoder.generate(encoder_input)
    # â†’ 50 frames Ã— 16 RVQ = 800 tokens

    # 3. Audio ë³€í™˜ & ì¬ìƒ
    audio_2s = SpectroStream.decode(chunk_i)
    play(audio_2s)

    # 4. Context ì—…ë°ì´íŠ¸ (sliding)
    # ê°€ì¥ ì˜¤ë˜ëœ chunk ë²„ë¦¼, ìƒˆ chunk ì¶”ê°€
```

**ì¥ì **:
1. **ë¬´í•œ ìƒì„±**: ì»¨í…ìŠ¤íŠ¸ê°€ ê³ ì • ê¸¸ì´ (1012 tokens)
2. **Stateless**: ê° ì²­í¬ê°€ ë…ë¦½ì  (error accumulation ê°ì†Œ)
3. **ìœ ì—°í•œ ì»¨íŠ¸ë¡¤**: ì²­í¬ë§ˆë‹¤ ìŠ¤íƒ€ì¼ ë³€ê²½ ê°€ëŠ¥
4. **ë©”ëª¨ë¦¬ íš¨ìœ¨**: 10ì´ˆ ì»¨í…ìŠ¤íŠ¸ë§Œ ìœ ì§€

### 3.4 Encoder-Decoder Transformer

**T5 ì•„í‚¤í…ì²˜ ê¸°ë°˜**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ENCODER (Bidirectional)                â”‚
â”‚                                          â”‚
â”‚  Input: 1012 tokens                     â”‚
â”‚  - Audio context: 1000 tokens           â”‚
â”‚    (5 chunks Ã— 50 frames Ã— 4 RVQ)       â”‚
â”‚  - Style: 12 tokens                     â”‚
â”‚                                          â”‚
â”‚  T5 Base: 220M params                   â”‚
â”‚  T5 Large: 770M params                  â”‚
â”‚                                          â”‚
â”‚  Output: Encoded representation         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DECODER (Causal)                       â”‚
â”‚                                          â”‚
â”‚  Two-stage architecture:                â”‚
â”‚                                          â”‚
â”‚  1. Temporal Module:                    â”‚
â”‚     - Process frame by frame            â”‚
â”‚     - 50 frames (2ì´ˆ)                   â”‚
â”‚                                          â”‚
â”‚  2. Depth Module:                       â”‚
â”‚     - Predict 16 RVQ tokens per frame   â”‚
â”‚     - Autoregressive within frame       â”‚
â”‚                                          â”‚
â”‚  Output: 800 tokens (50Ã—16)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì‹¤ì‹œê°„ ì„±ëŠ¥**:
- **T5 Large (770M)**: RTF = 1.8x on H100 GPU
- 2ì´ˆ ìŒì•…ì„ 1.1ì´ˆì— ìƒì„± (1.8ë°° ë¹ ë¦„)
- â†’ ì‹¤ì‹œê°„ ìƒì„± ê°€ëŠ¥!

---

## ğŸ“Š 4. ì„±ëŠ¥ í‰ê°€

### 4.1 ê¸°ì¡´ ëª¨ë¸ë“¤ê³¼ ë¹„êµ

| Model | Live? | Sample Rate | Params | FDâ†“ | KLâ†“ | CLAPâ†‘ |
|-------|-------|-------------|--------|-----|-----|-------|
| **Magenta RT** | âœ… | 48kHz | 760M | **72.14** | **0.47** | 0.35 |
| Stable Audio | âŒ | 44.1kHz | 1.1B | 96.51 | 0.55 | **0.41** |
| MusicGen Large | âŒ | 32kHz | 3.3B | 190.47 | 0.52 | 0.31 |

**ê²°ê³¼**:
- **FD (FrÃ©chet Distance)**: ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ â†’ Magenta RT ì••ë„ì  1ìœ„
- **KL (Kullback-Leibler)**: ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ â†’ Magenta RT 1ìœ„
- **CLAP Score**: ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ â†’ Stable Audioê°€ ì•½ê°„ ìš°ì„¸
- **íŒŒë¼ë¯¸í„° ìˆ˜**: Magenta RTê°€ ê°€ì¥ ì‘ìŒ (íš¨ìœ¨ì !)

### 4.2 Prompt Transition (ì‹¤ì‹œê°„ ì»¨íŠ¸ë¡¤ ë³€í™”)

**ì‹¤í—˜**: Prompt A â†’ Prompt Bë¡œ 60ì´ˆ ë™ì•ˆ ì ì§„ì  ì „í™˜

```python
# Example:
prompt_A = "calm piano ballad"
prompt_B = "energetic jazz piano"

# 10ì´ˆë§ˆë‹¤ interpolation
for t in [0, 10, 20, 30, 40, 50, 60]:
    alpha = t / 60
    style = (1 - alpha) * embed(A) + alpha * embed(B)
    generate_chunk(style)
```

**ê²°ê³¼**:
- ë§¤ë„ëŸ¬ìš´ ìŠ¤íƒ€ì¼ ì „í™˜ (smooth transition)
- ì´ì „ ì»¨í…ìŠ¤íŠ¸ì˜ ì˜í–¥ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë³€í™”
- Cosine similarityê°€ ì„ í˜•ì ìœ¼ë¡œ ë³€í™”

**ì˜ë¯¸**: ì‹¤ì‹œê°„ ì—°ì£¼ ì¤‘ ìŠ¤íƒ€ì¼ ë³€ê²½ ê°€ëŠ¥!

---

## ğŸ® 5. ì»¨íŠ¸ë¡¤ ë°©ì‹

### 5.1 Text Prompt

```python
# Simple text
style = embed("jazz piano, upbeat, bebop style")

# Multiple weighted prompts
style = weighted_avg([
    (2.0, "brad mehldau style"),
    (1.0, "modal jazz"),
    (0.5, "ambient")
])
```

**íŠ¹ì§•**:
- ì¥ë¥´, ì•…ê¸°, ë¬´ë“œ, í…œí¬ ë“± high-level ì»¨íŠ¸ë¡¤
- ì§ê´€ì ì´ì§€ë§Œ ì„¸ë°€í•œ ì»¨íŠ¸ë¡¤ ì–´ë ¤ì›€

### 5.2 Audio Prompt

```python
# Reference audioë¡œ ìŠ¤íƒ€ì¼ ì§€ì •
reference = "my_favorite_track.mp3"
style = embed(reference)

# Text + Audio blending
style = weighted_avg([
    (1.0, "jazz piano"),
    (2.0, reference_audio)  # Audioê°€ ë” ê°•í•œ ì˜í–¥
])
```

**íŠ¹ì§•**:
- ë§ë¡œ í‘œí˜„í•˜ê¸° ì–´ë ¤ìš´ ìŠ¤íƒ€ì¼ë„ ê°€ëŠ¥
- Training ì¡°ê±´ê³¼ ìœ ì‚¬ â†’ ë” íš¨ê³¼ì 
- **ë‚´ ì—°ì£¼ë¥¼ referenceë¡œ ì‚¬ìš© ê°€ëŠ¥!** â† ì¤‘ìš”!

### 5.3 Audio Injection (í˜ì‹ !)

**ê°œë…**: ì‹¤ì‹œê°„ìœ¼ë¡œ ì˜¤ë””ì˜¤ ì…ë ¥ì„ ëª¨ë¸ì— ì£¼ì…

```python
while generating:
    # 1. ì‚¬ìš©ì ì…ë ¥ ìº¡ì²˜ (ë§ˆì´í¬/MIDI)
    user_audio = capture_input()

    # 2. ëª¨ë¸ ì¶œë ¥ê³¼ ë¯¹ì‹±
    mixed = mix(user_audio, model_output, ratio=0.3)

    # 3. ë¯¹ì‹±ëœ ì˜¤ë””ì˜¤ë¥¼ tokenize
    mixed_tokens = SpectroStream.encode(mixed)

    # 4. ë‹¤ìŒ ì²­í¬ ìƒì„± ì‹œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
    next_chunk = generate(context=mixed_tokens, style=style)
```

**ì‘ë™ ë°©ì‹**:
```
User plays: [C E G]
    â†“ (mix with model output)
Model sees: [Previous output + User's C E G]
    â†“ (generate continuation)
Model output: [Responds to user's phrase...]
```

**íš¨ê³¼**:
- ëª¨ë¸ì´ ì‚¬ìš©ì ì…ë ¥ì„ "ë“£ê³ " ë°˜ì‘
- Call-response improvisation ê°€ëŠ¥!
- ì‚¬ìš©ì ì˜¤ë””ì˜¤ëŠ” ì§ì ‘ ì¬ìƒ ì•ˆ ë¨ (ëª¨ë¸ì´ í•´ì„í•´ì„œ ë°˜ì˜)

---

## ğŸ¹ 6. ë‹¹ì‹ ì˜ í”„ë¡œì íŠ¸ì— ì ìš©í•˜ê¸°

### 6.1 í˜„ì¬ Magenta RTì˜ í•œê³„

1. **Audio ê¸°ë°˜**: MIDI ì•„ë‹˜ (ì˜¤ë””ì˜¤ë¡œ ìƒì„±)
2. **RTF 1.8x**: ë¹ ë¥´ì§€ë§Œ MIDIê°€ ë” ë¹ ë¥¼ ìˆ˜ ìˆìŒ
3. **48kHz stereo**: ê³ í’ˆì§ˆì´ì§€ë§Œ ë¬´ê±°ì›€

### 6.2 MIDI ë²„ì „ìœ¼ë¡œ ê°œì¡°í•˜ê¸°

**ì•„ì´ë””ì–´**: SpectroStream ëŒ€ì‹  MIDI tokenizer ì‚¬ìš©

```python
# Original (Audio):
SpectroStream: Audio â†’ RVQ tokens (3200 tokens/2s)

# Your MIDI version:
MIDITokenizer: MIDI â†’ Event tokens (~100 tokens/2s)
# â†’ 30ë°° ê°€ë²¼ì›€!
# â†’ RTF 50x ì´ìƒ ì˜ˆìƒ (ì´ˆê³ ì†!)
```

**ì•„í‚¤í…ì²˜ ìˆ˜ì •**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. MusicCoCa                           â”‚
â”‚  - Audio prompt: ë‚´ ì—°ì£¼ ë…¹ìŒ           â”‚
â”‚  - Text prompt: "ohhalim jazz style"   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Encoder-Decoder Transformer         â”‚
â”‚  - Context: 10ì´ˆ MIDI events           â”‚
â”‚  - Generate: 2ì´ˆ MIDI events           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. MIDI Output (NOT SpectroStream)     â”‚
â”‚  - Event tokens â†’ MIDI messages         â”‚
â”‚  - Latency: ~20ms (ì´ˆê³ ì†!)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.3 Fine-tuning ì „ëµ

**Step 1: ë‚´ ì—°ì£¼ ë…¹ìŒ â†’ MIDI**
```python
# 100ì‹œê°„ ë…¹ìŒ (ëª©í‘œ)
my_recordings = [
    "improvisation_01.mid",
    "improvisation_02.mid",
    ...
]

# Audioë„ í•¨ê»˜ ì €ì¥ (MusicCoCaìš©)
my_audio = [
    "improvisation_01.wav",
    "improvisation_02.wav",
    ...
]
```

**Step 2: MusicCoCa Fine-tuning**
```python
# ë‚´ ìŠ¤íƒ€ì¼ í•™ìŠµ
MusicCoCa_personal = finetune(
    MusicCoCa_pretrained,
    audio_samples=my_audio,
    text_labels=["ohhalim style", "my jazz piano", ...]
)

# ê²°ê³¼: embed("ohhalim style") â†’ ë‚˜ë§Œì˜ ë²¡í„°!
```

**Step 3: Transformer Fine-tuning (QLoRA)**
```python
from peft import LoraConfig

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1
)

model = apply_qlora_to_model(
    magenta_rt_transformer,
    lora_config
)

trainer.train(
    model=model,
    train_data=my_midi_tokenized,
    style_embeddings=embed("ohhalim style"),
    epochs=50
)
```

### 6.4 ì‹¤ì‹œê°„ ë“€ì—£ ì‹œìŠ¤í…œ

```python
from magenta_rt_midi import PersonalDuetSystem

duet = PersonalDuetSystem(
    my_model="ohhalim_finetuned.ckpt",
    input_device="MIDI Keyboard",
    output_device="DAW MIDI In"
)

duet.start_session(
    mode="call_response",  # ë˜ëŠ” "simultaneous"
    latency_target=50,     # ms
    context_window=10      # seconds
)

# ì‹¤ì‹œê°„ ì—°ì£¼:
while True:
    # 1. ë‚´ ì—°ì£¼ ìº¡ì²˜
    my_phrase = duet.capture_input()

    # 2. AI ì‘ë‹µ ìƒì„± (ë‚´ ìŠ¤íƒ€ì¼ë¡œ!)
    ai_phrase = duet.generate_response(
        context=my_phrase,
        style=embed("ohhalim style")
    )

    # 3. AI ì—°ì£¼ ì¶œë ¥
    duet.play_output(ai_phrase)
```

---

## ğŸ’¡ 7. í•µì‹¬ ì¸ì‚¬ì´íŠ¸

### 7.1 ë…¼ë¬¸ì—ì„œ ë°°ìš¸ ì 

1. **Chunk-based generation**: ë¬´í•œ ìŠ¤íŠ¸ë¦¼ ìƒì„± í•µì‹¬!
   - 2ì´ˆ ì²­í¬
   - 10ì´ˆ ì»¨í…ìŠ¤íŠ¸
   - Stateless (ê°„ë‹¨!)

2. **Coarse context**: ë©”ëª¨ë¦¬/ì†ë„ ìµœì í™”
   - Context: 4 RVQ levels (coarse)
   - Generation: 16 RVQ levels (high-fi)
   - â†’ 4ë°° íš¨ìœ¨ í–¥ìƒ!

3. **MusicCoCa embedding**: ìœ ì—°í•œ ì»¨íŠ¸ë¡¤
   - Text + Audio blending
   - Weighted averaging
   - **ë‚´ ì—°ì£¼ë¥¼ promptë¡œ!**

4. **Audio injection**: ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©
   - ì‚¬ìš©ì ì…ë ¥ì„ contextì— ë¯¹ì‹±
   - ëª¨ë¸ì´ "ë“£ê³ " ë°˜ì‘
   - â†’ Call-response ê°€ëŠ¥!

### 7.2 ë‹¹ì‹ ì˜ í”„ë¡œì íŠ¸ì— ì ìš©

**ëª©í‘œ**: "ë‚˜ + AI(ë‚˜) = JAM!"

```python
# Phase 1: ë‚˜ë¥¼ ë…¹ìŒ
my_data = record_myself(100_hours)

# Phase 2: ë‚˜ë¥¼ í•™ìŠµ
my_ai = finetune(
    magenta_rt,
    my_data,
    style_name="ohhalim"
)

# Phase 3: ë‚˜ì™€ ë“€ì—£
while jamming:
    i_play()
    my_ai_responds()  # ë‚´ ìŠ¤íƒ€ì¼ë¡œ!
    i_respond_back()
    my_ai_continues()

    # â†’ Musical dialogue!
```

**í•µì‹¬ ì°¨ì´ì **:
- âŒ Brad Mehldau ëª¨ë°© (ìœ ëª…ì¸)
- âœ… ë‚˜ ìì‹  í•™ìŠµ (personal)
- âŒ Offline ìƒì„± (í„´ì œ)
- âœ… Real-time ë“€ì—£ (ë¼ì´ë¸Œ)
- âŒ Audio ìƒì„± (ë¬´ê±°ì›€)
- âœ… MIDI ìƒì„± (ë¹ ë¦„)

---

## ğŸš€ 8. ë‹¤ìŒ ë‹¨ê³„

### 8.1 ì¦‰ì‹œ ì‹œë„ (ì´ë²ˆ ì£¼)

```bash
# 1. Magenta RT Colab ì‹¤í–‰
open https://github.com/magenta/magenta-realtime

# 2. Audio prompt í…ŒìŠ¤íŠ¸
# ë‚´ ì—°ì£¼ 10ë¶„ ë…¹ìŒ â†’ Audio promptë¡œ ì‚¬ìš©
# â†’ AIê°€ ë‚´ ìŠ¤íƒ€ì¼ í‰ë‚´?

# 3. Audio injection ë°ëª¨
# ì‹¤ì‹œê°„ìœ¼ë¡œ ë§ˆì´í¬ ì…ë ¥í•˜ë©° AI ë°˜ì‘ í™•ì¸
```

### 8.2 ì—°êµ¬í•  ë‚´ìš© (ì´ë²ˆ ë‹¬)

1. **MIDI tokenizer ê°œë°œ**
   - SpectroStream ëŒ€ì‹ 
   - Event-based MIDI representation
   - Target: ~100 tokens/2s

2. **MusicCoCa fine-tuning**
   - ë‚´ ì—°ì£¼ 10-20ê°œë¡œ ì‹¤í—˜
   - "ohhalim style" ì„ë² ë”© í•™ìŠµ
   - Effectiveness ì¸¡ì •

3. **Latency ìµœì í™”**
   - Target: <50ms
   - MIDIê°€ audioë³´ë‹¤ ë¹ ë¦„
   - Quantization (INT8, FP16)

### 8.3 êµ¬í˜„ ë¡œë“œë§µ (3ê°œì›”)

**Month 1: ê¸°ì´ˆ**
- Magenta RT ì½”ë“œ ë¶„ì„
- MIDI tokenizer ê°œë°œ
- Chunk-based generation êµ¬í˜„

**Month 2: Fine-tuning**
- ë‚´ ì—°ì£¼ 50ì‹œê°„ ë…¹ìŒ
- MusicCoCa + Transformer fine-tuning
- í’ˆì§ˆ í‰ê°€

**Month 3: Real-time System**
- Audio injection â†’ MIDI injection
- Latency <50ms ë‹¬ì„±
- Live duet demo!

---

## ğŸ“š 9. ë…¼ë¬¸ì˜ ì² í•™ì  ë©”ì‹œì§€

### "Music as a verb" (ìŒì•…ì€ ë™ì‚¬ë‹¤)

**ê¸°ì¡´ AI ìŒì•…**:
- Music as a noun (ëª…ì‚¬)
- ì™„ì„±ëœ ì‘í’ˆ ìƒì„±
- Static, ê³ ì •ë¨

**Live Music Models**:
- Music as a verb (ë™ì‚¬)
- ì§„í–‰ ì¤‘ì¸ í–‰ìœ„
- Dynamic, ì‚´ì•„ìˆìŒ
- **Process > Product**

**ë‹¹ì‹ ì˜ í”„ë¡œì íŠ¸ì™€ ì¼ì¹˜**:
```
"ë‚œ ë‚´ê°€ ë§Œë“  ì¸ê³µì§€ëŠ¥ê³¼ ì¦‰í¥ì—°ì£¼í•˜ê³  ì‹¶ì–´"
                      ^^^^
                    (ë™ì‚¬!)

â†’ ì™„ì„±ëœ ê³¡ì´ ì•„ë‹Œ
â†’ í•¨ê»˜ ì—°ì£¼í•˜ëŠ” ê³¼ì •!
```

### Human-in-the-loop

**ê¸°ì¡´**: Human â†’ AI â†’ Output (ì¼ë°©í–¥)

**Live**: Human â‡„ AI â‡„ Output (ì–‘ë°©í–¥)
```
ë‚˜: [í”„ë ˆì´ì¦ˆ]
AI: [ì‘ë‹µ] â† ë‚˜ë¥¼ í•™ìŠµí•œ ìŠ¤íƒ€ì¼ë¡œ!
ë‚˜: [ë°˜ì‘]
AI: [ê³„ì†...]

â†’ ì§„ì§œ ëŒ€í™”!
```

---

## ğŸ¯ 10. ìµœì¢… ìš”ì•½

### ë…¼ë¬¸ì˜ í•µì‹¬ ê¸°ì—¬

1. **Live music model ì •ì˜**: ì‹¤ì‹œê°„, ì—°ì†, ë°˜ì‘í˜•
2. **Magenta RealTime**: ì˜¤í”ˆì†ŒìŠ¤, 760M, RTF 1.8x
3. **Chunk-based autoregression**: ë¬´í•œ ìŠ¤íŠ¸ë¦¼ ìƒì„±
4. **Audio injection**: ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš© ë©”ì»¤ë‹ˆì¦˜
5. **SOTA ì„±ëŠ¥**: ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ë†’ì€ í’ˆì§ˆ

### ë‹¹ì‹ ì˜ í”„ë¡œì íŠ¸ì— ì£¼ëŠ” ì˜ë¯¸

```
Magenta RT = ì™„ë²½í•œ ì¶œë°œì !

1. Architecture: âœ… Chunk-based (ê²€ì¦ë¨)
2. Open-source: âœ… ì½”ë“œ & weights ê³µê°œ
3. Fine-tunable: âœ… ë‚´ ìŠ¤íƒ€ì¼ í•™ìŠµ ê°€ëŠ¥
4. Real-time: âœ… ë¼ì´ë¸Œ ë“€ì—£ ê°€ëŠ¥

ìˆ˜ì •í•  ë¶€ë¶„:
- SpectroStream â†’ MIDI tokenizer
- Audio injection â†’ MIDI injection
- Style: Brad Mehldau â†’ Ohhalim!

â†’ ì™„ë²½í•˜ê²Œ ì‹¤í˜„ ê°€ëŠ¥! ğŸš€
```

---

## ğŸ’ª ì‹¤í–‰ ê³„íš

### Today (1ì‹œê°„):
```bash
# Magenta RT GitHub í´ë¡ 
git clone https://github.com/magenta/magenta-realtime.git

# Colab ë°ëª¨ ì‹¤í–‰
# â†’ ì‹¤ì‹œê°„ ìƒì„± ì²´í—˜

# ë…¼ë¬¸ ë‹¤ì‹œ ì½ê¸°
# â†’ ì•„í‚¤í…ì²˜ ì™„ì „ ì´í•´
```

### This Week:
```python
# 1. ë‚´ ì—°ì£¼ 10ë¶„ ë…¹ìŒ
record_my_playing("improvisation_test.wav")

# 2. Audio promptë¡œ í…ŒìŠ¤íŠ¸
style = embed(my_audio="improvisation_test.wav")
generate(style=style)
# â†’ AIê°€ ë‚´ ìŠ¤íƒ€ì¼ í‰ë‚´ë‚´ë‚˜?

# 3. MIDI tokenizer ì„¤ê³„ ì‹œì‘
design_midi_tokenizer()
```

### This Month:
```
Week 1: Architecture understanding
Week 2: MIDI tokenizer implementation
Week 3: Fine-tuning experiments (10 files)
Week 4: Real-time MIDI generation prototype
```

---

**"Live Music Models" ë…¼ë¬¸ = ë‹¹ì‹ ì˜ ë¹„ì „ì„ ì‹¤í˜„í•  ì™„ë²½í•œ blueprint! ğŸ¹âœ¨**

**ë‹¤ìŒ ë‹¨ê³„: ì½”ë“œ ë¶„ì„ & MIDI ë²„ì „ ê°œë°œ ì‹œì‘!**
