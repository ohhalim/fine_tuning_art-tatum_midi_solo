# Magenta RealTime Fine-tuning Guide (QLoRA)

**ëª©í‘œ**: ë“œëìš© ì¬ì¦ˆ í´ë¦½ ìƒì„± (10-20ì´ˆ) with "ohhalim style"

**í™˜ê²½**:
- Colab (ë¬´ë£Œ TPU/GPU) â†’ í…ŒìŠ¤íŠ¸ & ì‘ì€ ì‹¤í—˜
- ëŸ°íŒŸ RTX 3060 8GB ($10) â†’ ë³¸ê²© fine-tuning

---

## ğŸ“‹ ì „ì²´ ì›Œí¬í”Œë¡œìš°

```
Day 1: ê¸°ë³¸ ìƒì„± í…ŒìŠ¤íŠ¸
  â†’ Colab ë°ëª¨ë¡œ ì²« ì¬ì¦ˆ í´ë¦½ 10ê°œ ìƒì„±
  â†’ ì‘ë™ í™•ì¸ & í’ˆì§ˆ ì²´í¬

Day 2: ë°ì´í„° ì¤€ë¹„ & Audio Injection
  â†’ Public dataset ë‹¤ìš´ë¡œë“œ (Bill Evans ë“±)
  â†’ Audio prompt í…ŒìŠ¤íŠ¸
  â†’ Fine-tuning ë°ì´í„° ì¤€ë¹„

Day 3: Fine-tuning ì‹¤í–‰ (QLoRA)
  â†’ ëŸ°íŒŸ ë˜ëŠ” Colab Pro
  â†’ "ohhalim style" í•™ìŠµ
  â†’ ìƒì„± í…ŒìŠ¤íŠ¸

Day 4+: FL Studio í†µí•©
  â†’ ë“œëì— ì¬ì¦ˆ í´ë¦½ ì‚½ì…
  â†’ Export & Rekordbox
```

---

## ğŸš€ Day 1: ê¸°ë³¸ ìƒì„± í…ŒìŠ¤íŠ¸

### Step 1: Colab ë°ëª¨ ì‹¤í–‰

**1-1. Colab ì—´ê¸°**
```
https://colab.research.google.com/github/magenta/magenta-realtime/blob/main/notebooks/Magenta_RT_Demo.ipynb
```

**1-2. ëŸ°íƒ€ì„ ì„¤ì •**
```
Runtime â†’ Change runtime type â†’ TPU v2-8 ì„ íƒ
```

**1-3. ì „ì²´ ì‹¤í–‰**
```
Runtime â†’ Run all
```

**ì˜ˆìƒ ì‹œê°„**: 5-10ë¶„ (ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)

---

### Step 2: ë“œëìš© ì¬ì¦ˆ í´ë¦½ 10ê°œ ìƒì„±

ë…¸íŠ¸ë¶ ë§ˆì§€ë§‰ì— ìƒˆ ì…€ ì¶”ê°€:

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë“œëìš© ì¬ì¦ˆ í´ë¦½ Batch ìƒì„±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from magenta_rt import audio, system
import numpy as np

# ëª¨ë¸ ë¡œë“œ (ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŒ)
mrt = system.MagentaRT()

# ì¬ì¦ˆ ìŠ¤íƒ€ì¼ 10ê°€ì§€
jazz_styles = [
    "Bill Evans modal jazz piano, slow tempo, impressionistic",
    "Bud Powell bebop piano, fast 140 BPM, energetic",
    "Herbie Hancock jazz funk piano, groovy, syncopated",
    "Keith Jarrett improvisation, lyrical, flowing",
    "McCoy Tyner quartal harmony, modal, powerful",
    "Oscar Peterson swing piano, uptempo, virtuosic",
    "Chick Corea latin jazz piano, rhythmic",
    "Brad Mehldau contemporary jazz piano, introspective",
    "Red Garland blues piano, soulful, bluesy",
    "Wynton Kelly hard bop piano, swinging, bright"
]

# ìƒì„± ì„¤ì •
CLIP_DURATION = 16  # ì´ˆ (ë“œëìš©)
CHUNK_LENGTH = 2    # ì´ˆ (Magenta RT ê¸°ë³¸)
NUM_CHUNKS = CLIP_DURATION // CHUNK_LENGTH

# ìƒì„±!
all_clips = []

for i, style_text in enumerate(jazz_styles):
    print(f"\n{'='*60}")
    print(f"ğŸ¹ Generating {i+1}/10: {style_text[:50]}...")
    print(f"{'='*60}")

    # ìŠ¤íƒ€ì¼ ì„ë² ë”©
    style = system.embed_style(style_text)

    # ì²­í¬ ìƒì„± (16ì´ˆ = 8 chunks)
    chunks = []
    state = None

    for j in range(NUM_CHUNKS):
        state, chunk = mrt.generate_chunk(
            state=state,
            style=style,
            temperature=1.0,  # ë‹¤ì–‘ì„±
            top_k=40
        )
        chunks.append(chunk)
        print(f"  âœ“ Chunk {j+1}/{NUM_CHUNKS} generated")

    # í•©ì¹˜ê¸°
    generated = audio.concatenate(chunks)

    # ì €ì¥
    filename = f"drop_jazz_{i:03d}.wav"
    generated.save(filename)
    print(f"  âœ… Saved: {filename}")
    print(f"     Duration: {CLIP_DURATION}s")
    print(f"     Style: {style_text}")

    all_clips.append({
        'file': filename,
        'style': style_text,
        'duration': CLIP_DURATION
    })

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del chunks, generated, state
    import gc
    gc.collect()

print(f"\n{'='*60}")
print(f"ğŸ‰ Complete! Generated {len(all_clips)} clips")
print(f"{'='*60}")

# ìš”ì•½
for i, clip in enumerate(all_clips):
    print(f"{i+1}. {clip['file']} - {clip['style'][:40]}...")
```

**ì‹¤í–‰ í›„:**

```python
# ë‹¤ìš´ë¡œë“œ
from google.colab import files
import zipfile

# ZIPìœ¼ë¡œ ë¬¶ê¸°
with zipfile.ZipFile('jazz_clips_day1.zip', 'w') as zipf:
    for clip in all_clips:
        zipf.write(clip['file'])

print("ğŸ“¦ Downloading zip file...")
files.download('jazz_clips_day1.zip')
print("âœ… Download complete!")
```

**ê²°ê³¼:**
- âœ… 10ê°œ ì¬ì¦ˆ í´ë¦½ (ê° 16ì´ˆ)
- âœ… ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ (Bill Evans ~ Wynton Kelly)
- âœ… ZIP ë‹¤ìš´ë¡œë“œ
- âœ… FL Studioì— ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥!

---

## ğŸ“Š Day 2: ë°ì´í„° ì¤€ë¹„

### Step 1: Public Dataset ë‹¤ìš´ë¡œë“œ

**Option A: PiJAMA Dataset (ì¶”ì²œ)**

ìƒˆ Colab ë…¸íŠ¸ë¶:

```python
# PiJAMA: 200+ hours jazz piano MIDI
# https://github.com/CPJKU/pijama

# ë‹¤ìš´ë¡œë“œ (5-10ë¶„)
!wget https://zenodo.org/record/5120004/files/pijama_dataset_audio.zip
!unzip pijama_dataset_audio.zip -d pijama/

# êµ¬ì¡° í™•ì¸
!ls -lh pijama/

# Bill Evans ìŠ¤íƒ€ì¼ë§Œ ì¶”ì¶œ (ì˜ˆì‹œ)
import glob

all_files = glob.glob("pijama/**/*.wav", recursive=True)
print(f"Total files: {len(all_files)}")

# í•„í„°ë§ (íŒŒì¼ëª…ì— 'evans' í¬í•¨)
bill_evans_files = [f for f in all_files if 'evans' in f.lower()]
print(f"Bill Evans files: {len(bill_evans_files)}")

# ì²« 20ê°œë§Œ ì‚¬ìš©
training_files = bill_evans_files[:20]
```

**Option B: YouTube â†’ MIDI ë³€í™˜**

```python
# ë‹¹ì‹ ì˜ ì¬ì¦ˆ ì—°ì£¼ê°€ ìˆë‹¤ë©´:
# 1. MIDI íŒŒì¼ ì¤€ë¹„
# 2. ë˜ëŠ” Audio â†’ MIDI ë³€í™˜

# Colabì— ì—…ë¡œë“œ
from google.colab import files
uploaded = files.upload()  # MIDI/Audio íŒŒì¼ ì„ íƒ

# í™•ì¸
import os
uploaded_files = list(uploaded.keys())
print(f"Uploaded: {uploaded_files}")
```

**Option C: ì§ì ‘ ë…¹ìŒ (ë‚˜ì¤‘ì—)**

```
1. FL Studioì—ì„œ MIDI ë…¹ìŒ
2. 20-50ê°œ ì¦‰í¥ì—°ì£¼
3. Export as MIDI
4. Fine-tuning ë°ì´í„°ë¡œ ì‚¬ìš©
```

---

### Step 2: Audio Injection í…ŒìŠ¤íŠ¸

**Audio Injection Colab ì—´ê¸°:**
```
https://colab.research.google.com/github/magenta/magenta-realtime/blob/main/notebooks/Magenta_RT_Audio_Injection.ipynb
```

**ì‹¤í–‰:**

```python
# ë…¸íŠ¸ë¶ ì „ì²´ ì‹¤í–‰ í›„...

from magenta_rt import audio, musiccoca
import numpy as np

# ìŠ¤íƒ€ì¼ ëª¨ë¸
style_model = musiccoca.MusicCoCa()

# ë ˆí¼ëŸ°ìŠ¤ ì˜¤ë””ì˜¤ (ë‹¹ì‹ ì˜ ì—°ì£¼ ë˜ëŠ” Bill Evans)
reference_audio = audio.Waveform.from_file('/content/my_jazz.wav')

# Text + Audio blending
weighted_styles = [
    (3.0, reference_audio),  # Audioê°€ ê°€ì¥ ê°•í•¨!
    (1.0, "modal jazz piano"),
    (0.5, "bebop improvisation")
]

# ì„ë² ë”©
weights = np.array([w for w, _ in weighted_styles])
styles = style_model.embed([s for _, s in weighted_styles])
weights_norm = weights / weights.sum()
blended_style = (weights_norm[:, np.newaxis] * styles).mean(axis=0)

# ìƒì„± í…ŒìŠ¤íŠ¸
mrt = system.MagentaRT()
chunks = []
state = None

for i in range(8):  # 16ì´ˆ
    state, chunk = mrt.generate_chunk(
        state=state,
        style=blended_style,
        temperature=1.0
    )
    chunks.append(chunk)
    print(f"Chunk {i+1}/8")

generated = audio.concatenate(chunks)
generated.save("my_style_test.wav")

# ë‹¤ìš´ë¡œë“œ
from google.colab import files
files.download("my_style_test.wav")
```

**í™•ì¸:**
- ìƒì„±ëœ ìŒì•…ì´ ë ˆí¼ëŸ°ìŠ¤ì™€ ìœ ì‚¬í•œê°€?
- ìŠ¤íƒ€ì¼ì´ ë°˜ì˜ë˜ì—ˆëŠ”ê°€?
- â†’ Fine-tuningí•˜ë©´ ë” ì •í™•í•´ì§!

---

## ğŸ”¥ Day 3: Fine-tuning (QLoRA)

### Step 1: Fine-tuning Colab ì—´ê¸°

```
https://colab.research.google.com/github/magenta/magenta-realtime/blob/main/notebooks/Magenta_RT_Finetune.ipynb
```

**ëŸ°íƒ€ì„ ì„¤ì •:**
```
Runtime â†’ Change runtime type
â†’ GPU: T4 (ë¬´ë£Œ) ë˜ëŠ” V100 (Colab Pro)
```

---

### Step 2: ë°ì´í„° ì¤€ë¹„

ë…¸íŠ¸ë¶ì—ì„œ ì‹¤í–‰:

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 2-1: ë°ì´í„° ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Option A: Colabì— ì—…ë¡œë“œ
from google.colab import files
import os

print("Upload your jazz MIDI/Audio files:")
uploaded = files.upload()

training_files = list(uploaded.keys())
print(f"\nâœ… Uploaded {len(training_files)} files")
for f in training_files:
    print(f"  - {f}")

# Option B: Public dataset
# (ìœ„ì˜ PiJAMA ë‹¤ìš´ë¡œë“œ ì½”ë“œ ì‚¬ìš©)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 2-2: Audio â†’ Tokens ë³€í™˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from magenta_rt import audio, spectrostream

# SpectroStream codec
codec = spectrostream.SpectroStream()

# ê° íŒŒì¼ ì²˜ë¦¬
tokenized_data = []

for i, file in enumerate(training_files):
    print(f"\n[{i+1}/{len(training_files)}] Processing: {file}")

    # ì˜¤ë””ì˜¤ ë¡œë“œ
    waveform = audio.Waveform.from_file(file)

    # Tokenize (2ì´ˆ ì²­í¬)
    # SpectroStream: 48kHz stereo â†’ discrete tokens
    tokens = codec.encode(waveform)

    print(f"  Shape: {tokens.shape}")
    print(f"  Duration: {waveform.duration:.1f}s")

    tokenized_data.append({
        'file': file,
        'tokens': tokens,
        'duration': waveform.duration
    })

print(f"\nâœ… Tokenized {len(tokenized_data)} files")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 2-3: Data Augmentation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Augmentationìœ¼ë¡œ ë°ì´í„° ëŠ˜ë¦¬ê¸°
# 1. Pitch shifting (Â±2 semitones)
# 2. Time stretching (0.9x, 1.0x, 1.1x)

augmented_data = []

for data in tokenized_data:
    original_tokens = data['tokens']

    # Original
    augmented_data.append(original_tokens)

    # Pitch shifts (Â±1, Â±2 semitones)
    # Note: SpectroStream tokensëŠ” ì§ì ‘ pitch shift ì–´ë ¤ì›€
    # â†’ Audio ë‹¨ê³„ì—ì„œ augmentation ê¶Œì¥

    # ê°„ë‹¨ ë²„ì „: ì›ë³¸ ë°ì´í„°ë§Œ ì‚¬ìš©
    # (ë³¸ê²© ë²„ì „ì€ MIDIì—ì„œ augmentation í›„ audio ë³€í™˜)

print(f"Training samples: {len(augmented_data)}")
```

---

### Step 3: QLoRA ì„¤ì •

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# QLoRA Fine-tuning Setup
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import torch
from transformers import AutoModelForCausalLM
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    PeftModel
)
from transformers import BitsAndBytesConfig

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 3-1: 4-bit Quantization Config
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                      # 4-bit quantization
    bnb_4bit_quant_type="nf4",              # NormalFloat4
    bnb_4bit_compute_dtype=torch.float16,   # Compute in FP16
    bnb_4bit_use_double_quant=True          # Double quantization
)

print("âœ… Quantization config ready")
print(f"  - 4-bit: {bnb_config.load_in_4bit}")
print(f"  - Type: {bnb_config.bnb_4bit_quant_type}")
print(f"  - Compute: {bnb_config.bnb_4bit_compute_dtype}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 3-2: Base Model ë¡œë“œ (4-bit)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ“¥ Loading base model (4-bit)...")

# Magenta RTì˜ ì‹¤ì œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
# (ë…¸íŠ¸ë¶ì— ê²½ë¡œê°€ ìˆì„ ê²ƒ)
model_checkpoint = "path/to/magenta-rt-checkpoint"  # ë…¸íŠ¸ë¶ì—ì„œ í™•ì¸!

# ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    model_checkpoint,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

print("âœ… Base model loaded")
print(f"  Model: {type(model).__name__}")
print(f"  Device: {next(model.parameters()).device}")

# GPU ë©”ëª¨ë¦¬ í™•ì¸
if torch.cuda.is_available():
    allocated = torch.cuda.memory_allocated(0) / 1024**3
    reserved = torch.cuda.memory_reserved(0) / 1024**3
    print(f"  GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 3-3: LoRA Preparation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ”§ Preparing model for k-bit training...")
model = prepare_model_for_kbit_training(model)
print("âœ… Model prepared")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 3-4: LoRA Config
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

lora_config = LoraConfig(
    r=8,                        # Rank (í•µì‹¬ íŒŒë¼ë¯¸í„°!)
    lora_alpha=16,              # Scaling factor (ë³´í†µ r*2)
    target_modules=[            # Attention layersì— ì ìš©
        "q_proj",               # Query projection
        "v_proj",               # Value projection
        "k_proj",               # Key projection (optional)
        "o_proj",               # Output projection (optional)
    ],
    lora_dropout=0.1,           # Dropout for regularization
    bias="none",                # Bias í•™ìŠµ ì•ˆ í•¨
    task_type="CAUSAL_LM",      # Causal Language Modeling
    inference_mode=False        # Training mode
)

print("âœ… LoRA config ready")
print(f"  Rank (r): {lora_config.r}")
print(f"  Alpha: {lora_config.lora_alpha}")
print(f"  Target modules: {lora_config.target_modules}")
print(f"  Dropout: {lora_config.lora_dropout}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Step 3-5: Apply LoRA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ¯ Applying LoRA to model...")
model = get_peft_model(model, lora_config)
print("âœ… LoRA applied")

# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸
model.print_trainable_parameters()

# Expected output:
# trainable params: 2,097,152 / 760,000,000 = 0.28%
# â†’ 99.7% íŒŒë¼ë¯¸í„°ëŠ” freeze!
```

---

### Step 4: Training ì‹¤í–‰

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Training Configuration
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from transformers import Trainer, TrainingArguments, DataCollatorWithPadding
from transformers.trainer_callback import ProgressCallback
import os

# ì¶œë ¥ ë””ë ‰í† ë¦¬
output_dir = "./ohhalim-jazz-style"
os.makedirs(output_dir, exist_ok=True)

# Training arguments
training_args = TrainingArguments(
    output_dir=output_dir,

    # Epochs & Batch
    num_train_epochs=50,                    # 50 epochs (ì¡°ì • ê°€ëŠ¥)
    per_device_train_batch_size=1,          # GPU ë©”ëª¨ë¦¬ ê³ ë ¤
    gradient_accumulation_steps=4,          # Effective batch = 4

    # Learning rate
    learning_rate=1e-4,                     # QLoRA ê¶Œì¥
    warmup_steps=100,                       # Warmup
    lr_scheduler_type="cosine",             # Cosine decay

    # Optimization
    optim="paged_adamw_8bit",               # 8-bit AdamW (QLoRA)
    weight_decay=0.01,                      # L2 regularization
    max_grad_norm=1.0,                      # Gradient clipping

    # Mixed precision
    fp16=True,                              # FP16 training

    # Logging
    logging_steps=10,
    logging_dir=f"{output_dir}/logs",
    report_to="tensorboard",                # TensorBoard

    # Saving
    save_steps=500,
    save_total_limit=3,                     # ìµœê·¼ 3ê°œ checkpointë§Œ
    save_strategy="steps",

    # Evaluation (optional)
    # evaluation_strategy="steps",
    # eval_steps=500,

    # Hardware
    dataloader_num_workers=2,
    dataloader_pin_memory=True,
)

print("âœ… Training arguments configured")
print(f"  Epochs: {training_args.num_train_epochs}")
print(f"  Batch size: {training_args.per_device_train_batch_size}")
print(f"  Gradient accumulation: {training_args.gradient_accumulation_steps}")
print(f"  Effective batch: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
print(f"  Learning rate: {training_args.learning_rate}")
print(f"  Optimizer: {training_args.optim}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Dataset Preparation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# tokenized_dataë¥¼ HuggingFace Datasetìœ¼ë¡œ ë³€í™˜
# (ë…¸íŠ¸ë¶ì— ì˜ˆì œ ì½”ë“œê°€ ìˆì„ ê²ƒ)

from datasets import Dataset

# ê°„ë‹¨í•œ ì˜ˆì œ (ì‹¤ì œëŠ” ë…¸íŠ¸ë¶ ì½”ë“œ ì‚¬ìš©)
train_dataset = Dataset.from_dict({
    'input_ids': [data['tokens'] for data in tokenized_data],
    # ... ê¸°íƒ€ í•„ìš”í•œ ì»¬ëŸ¼
})

print(f"âœ… Dataset prepared: {len(train_dataset)} samples")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Trainer ìƒì„±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    # eval_dataset=val_dataset,  # Optional
    # data_collator=data_collator,
    callbacks=[ProgressCallback()]
)

print("âœ… Trainer ready")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Training ì‹œì‘!
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*60)
print("ğŸ”¥ Starting fine-tuning...")
print("="*60)
print(f"Training samples: {len(train_dataset)}")
print(f"Epochs: {training_args.num_train_epochs}")
print(f"Steps per epoch: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}")
print(f"Total steps: ~{len(train_dataset) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}")
print("="*60 + "\n")

# ì‹œì‘!
trainer.train()

print("\n" + "="*60)
print("âœ… Training complete!")
print("="*60)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ëª¨ë¸ ì €ì¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ’¾ Saving model...")

# LoRA weights ì €ì¥ (ì‘ìŒ! ~10MB)
model.save_pretrained(output_dir)
print(f"âœ… Model saved to: {output_dir}")

# Tokenizerë„ ì €ì¥ (ìˆë‹¤ë©´)
# tokenizer.save_pretrained(output_dir)

# íŒŒì¼ í¬ê¸° í™•ì¸
import subprocess
size = subprocess.check_output(['du', '-sh', output_dir]).split()[0].decode('utf-8')
print(f"  Model size: {size}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë‹¤ìš´ë¡œë“œ (Colab)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ“¦ Creating zip for download...")

!zip -r ohhalim-jazz-style.zip {output_dir}

from google.colab import files
files.download("ohhalim-jazz-style.zip")

print("âœ… Download complete!")
print("\nNext steps:")
print("1. Extract zip file locally")
print("2. Upload to ëŸ°íŒŸ or use locally")
print("3. Generate with your fine-tuned model!")
```

**ì˜ˆìƒ ì‹œê°„:**
- Colab ë¬´ë£Œ GPU (T4): 3-6ì‹œê°„
- Colab Pro GPU (V100): 1-3ì‹œê°„
- ëŸ°íŒŸ RTX 3060: 2-4ì‹œê°„

---

## ğŸµ Day 4: Fine-tuned Modelë¡œ ìƒì„±

### Step 1: Model ë¡œë“œ

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Fine-tuned Model ë¡œë“œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from peft import PeftModel
from transformers import AutoModelForCausalLM
import torch

# Base model ë¡œë“œ
print("Loading base model...")
base_model = AutoModelForCausalLM.from_pretrained(
    "path/to/magenta-rt-checkpoint",
    torch_dtype=torch.float16,
    device_map="auto"
)

# LoRA weights ì ìš©
print("Loading LoRA weights...")
model_finetuned = PeftModel.from_pretrained(
    base_model,
    "./ohhalim-jazz-style"  # ë‹¤ìš´ë¡œë“œí•œ í´ë”
)

print("âœ… Fine-tuned model loaded!")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Generation í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from magenta_rt import audio, system

def generate_with_finetuned_model(
    model,
    prompt="ohhalim jazz piano style",
    duration=16,
    temperature=1.0,
    output_file="ohhalim_jazz_001.wav"
):
    """
    Fine-tuned modelë¡œ ì¬ì¦ˆ í´ë¦½ ìƒì„±
    """

    # ìŠ¤íƒ€ì¼ ì„ë² ë”©
    style = system.embed_style(prompt)

    # ìƒì„±
    mrt = system.MagentaRT(model=model)  # Fine-tuned model ì‚¬ìš©

    chunks = []
    state = None
    num_chunks = duration // 2

    for i in range(num_chunks):
        state, chunk = mrt.generate_chunk(
            state=state,
            style=style,
            temperature=temperature
        )
        chunks.append(chunk)
        print(f"Chunk {i+1}/{num_chunks}")

    # í•©ì¹˜ê¸°
    generated = audio.concatenate(chunks)

    # ì €ì¥
    generated.save(output_file)
    print(f"âœ… Saved: {output_file}")

    return generated

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìƒì„± í…ŒìŠ¤íŠ¸!
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Test 1: ë‹¹ì‹  ìŠ¤íƒ€ì¼
jazz_1 = generate_with_finetuned_model(
    model_finetuned,
    prompt="ohhalim jazz piano improvisation, modal",
    duration=16,
    output_file="drop_jazz_ohhalim_001.wav"
)

# Test 2: Bill Evans ì˜í–¥
jazz_2 = generate_with_finetuned_model(
    model_finetuned,
    prompt="ohhalim style, Bill Evans influence, introspective",
    duration=16,
    output_file="drop_jazz_ohhalim_002.wav"
)

# Test 3: Uptempo bebop
jazz_3 = generate_with_finetuned_model(
    model_finetuned,
    prompt="ohhalim jazz, fast bebop, 140 BPM",
    duration=16,
    output_file="drop_jazz_ohhalim_003.wav"
)

print("\nğŸ‰ Generation complete!")
print("Listen and compare with base model!")
```

---

### Step 2: Batch ìƒì„± (ë“œëìš© 10ê°œ)

```python
# ë‹¤ì–‘í•œ ë“œë ì‹œë‚˜ë¦¬ì˜¤ìš© í´ë¦½ ìƒì„±

drop_scenarios = [
    {
        'prompt': "ohhalim jazz piano, energetic drop, 128 BPM",
        'duration': 16,
        'temperature': 1.0,
        'name': 'energetic_drop'
    },
    {
        'prompt': "ohhalim modal jazz, floating, ambient",
        'duration': 20,
        'temperature': 1.1,
        'name': 'ambient_drop'
    },
    {
        'prompt': "ohhalim bebop piano, fast lines, 140 BPM",
        'duration': 12,
        'temperature': 0.9,
        'name': 'fast_bebop'
    },
    {
        'prompt': "ohhalim jazz funk, groovy, syncopated",
        'duration': 16,
        'temperature': 1.0,
        'name': 'funk_drop'
    },
    {
        'prompt': "ohhalim blues jazz, soulful, slow",
        'duration': 20,
        'temperature': 1.2,
        'name': 'blues_drop'
    },
    # ... 5ê°œ ë” ì¶”ê°€
]

# ìƒì„±!
for i, scenario in enumerate(drop_scenarios):
    print(f"\n[{i+1}/{len(drop_scenarios)}] {scenario['name']}")

    jazz = generate_with_finetuned_model(
        model_finetuned,
        prompt=scenario['prompt'],
        duration=scenario['duration'],
        temperature=scenario['temperature'],
        output_file=f"drop_ohhalim_{i:03d}_{scenario['name']}.wav"
    )

# ZIPìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
!zip -r ohhalim_drop_clips.zip drop_ohhalim_*.wav
files.download("ohhalim_drop_clips.zip")
```

---

## ğŸ›ï¸ FL Studio í†µí•©

### ì›Œí¬í”Œë¡œìš°

```
1. FL Studio í”„ë¡œì íŠ¸ ì—´ê¸°
   - í•˜ìš°ìŠ¤/í…Œí¬ë…¸ íŠ¸ë™ ì‘ì—… ì¤‘

2. ë“œë ìœ„ì¹˜ íŒŒì•…
   - ë³´í†µ: Bar 64, 128, 192 ë“±

3. ì¬ì¦ˆ í´ë¦½ ì‚½ì…
   - Playlist ì˜¤ë¥¸ìª½ í´ë¦­ â†’ Insert â†’ Audio clip
   - ë‹¤ìš´ë¡œë“œí•œ ì¬ì¦ˆ í´ë¦½ ì„ íƒ
   - ë“œë ì‹œì‘ ìœ„ì¹˜ì— ë°°ì¹˜

4. ì´í™íŠ¸ ì²´ì¸
   - EQ: High-pass 100Hz (í‚¥ê³¼ ë¶„ë¦¬)
   - Reverb: Wet 20-30% (ê³µê°„ê°)
   - Sidechain: Kickì—ì„œ (íŒí•‘ íš¨ê³¼)

5. Export
   - File â†’ Export â†’ Wave file
   - 44.1kHz, 16/24-bit

6. Rekordboxë¡œ Import
   - DJìš© ìµœì¢… íŠ¸ë™!
```

---

### Python ìë™í™” (Advanced)

```python
# FL Studio Python API (FlStudioApi)
# https://github.com/demberto/PyFLP

from pyflp import Project

# í”„ë¡œì íŠ¸ ì—´ê¸°
project = Project.load("my_track.flp")

# ë“œë ìœ„ì¹˜ ì°¾ê¸°
drop_positions = [64, 128, 192]  # Bar ë²ˆí˜¸

# ì¬ì¦ˆ í´ë¦½ ì‚½ì…
for i, pos in enumerate(drop_positions):
    # ì˜¤ë””ì˜¤ í´ë¦½ ì¶”ê°€
    clip = project.add_audio_clip(
        file=f"drop_ohhalim_{i:03d}.wav",
        position=pos * 4 * 96  # Bar â†’ Ticks ë³€í™˜ (96 ticks/beat)
    )

    # ì´í™íŠ¸ ì¶”ê°€
    clip.add_effect("Fruity Reverb 2", wet=0.3)
    clip.add_effect("Fruity Parametric EQ 2")

# ì €ì¥
project.save("my_track_with_jazz.flp")
```

---

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: Out of Memory (OOM)

**ì¦ìƒ:**
```
RuntimeError: CUDA out of memory
```

**í•´ê²°:**

```python
# 1. Batch size ì¤„ì´ê¸°
per_device_train_batch_size=1  # ì´ë¯¸ ìµœì†Œ

# 2. Gradient accumulation ì¤„ì´ê¸°
gradient_accumulation_steps=2  # 4 â†’ 2

# 3. LoRA rank ì¤„ì´ê¸°
lora_config = LoraConfig(
    r=4,  # 8 â†’ 4
    lora_alpha=8,  # 16 â†’ 8
)

# 4. ë°ì´í„° ê¸¸ì´ ì¤„ì´ê¸°
# ì²­í¬ë¥¼ 1ì´ˆë¡œ (ì›ë˜ 2ì´ˆ)

# 5. Gradient checkpointing
model.gradient_checkpointing_enable()
```

---

### ë¬¸ì œ 2: Training Loss ì•ˆ ë–¨ì–´ì§

**ì¦ìƒ:**
```
Epoch 1: loss=2.5
Epoch 10: loss=2.4
Epoch 20: loss=2.4
...
```

**í•´ê²°:**

```python
# 1. Learning rate ì¡°ì •
learning_rate=5e-5  # 1e-4 â†’ 5e-5 (ë” ì‘ê²Œ)

# 2. Warmup ëŠ˜ë¦¬ê¸°
warmup_steps=200  # 100 â†’ 200

# 3. Epochs ëŠ˜ë¦¬ê¸°
num_train_epochs=100  # 50 â†’ 100

# 4. ë°ì´í„° í™•ì¸
# - ë„ˆë¬´ ì ì€ê°€? (ìµœì†Œ 10ê°œ)
# - í’ˆì§ˆì´ ì¢‹ì€ê°€?
# - Augmentation í•„ìš”?
```

---

### ë¬¸ì œ 3: ìƒì„± í’ˆì§ˆì´ Base modelê³¼ ì°¨ì´ ì—†ìŒ

**ì¦ìƒ:**
- Fine-tuned modelì´ baseì™€ ë˜‘ê°™ì´ ë“¤ë¦¼
- ë‹¹ì‹  ìŠ¤íƒ€ì¼ì´ ë°˜ì˜ ì•ˆ ë¨

**í•´ê²°:**

```python
# 1. ë” ë§ì€ ë°ì´í„°
# 10ê°œ â†’ 20-50ê°œ

# 2. ë” ê°•í•œ Fine-tuning
lora_config = LoraConfig(
    r=16,  # 8 â†’ 16 (ë” ê°•ë ¥)
    lora_alpha=32,
)

# 3. Learning rate ë†’ì´ê¸°
learning_rate=2e-4  # 1e-4 â†’ 2e-4

# 4. Epochs ëŠ˜ë¦¬ê¸°
num_train_epochs=100

# 5. Audio prompt weight ë†’ì´ê¸°
weighted_styles = [
    (5.0, my_audio),  # 3.0 â†’ 5.0
    (1.0, "jazz piano"),
]
```

---

### ë¬¸ì œ 4: Colab 12ì‹œê°„ ì œí•œ

**í•´ê²°:**

```python
# 1. Checkpoint ì €ì¥ ìë™í™”
save_steps=100  # ìì£¼ ì €ì¥

# 2. Resume from checkpoint
trainer.train(resume_from_checkpoint=True)

# 3. ëŸ°íŒŸ ì‚¬ìš© ($10)
# - ì‹œê°„ ì œí•œ ì—†ìŒ
# - ì–¸ì œë“  ì¬ì‹œì‘ ê°€ëŠ¥
```

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

### Base Model vs Fine-tuned

**í…ŒìŠ¤íŠ¸:**

```python
# Base model ìƒì„±
base_jazz = generate_with_base_model(
    prompt="Bill Evans modal jazz piano",
    duration=16
)

# Fine-tuned model ìƒì„±
finetuned_jazz = generate_with_finetuned_model(
    model_finetuned,
    prompt="ohhalim jazz piano, modal",
    duration=16
)

# ë¹„êµ ì²­ì·¨!
# ì°¨ì´ì :
# - í™”ì„± voicing
# - ë¦¬ë“¬ íŒ¨í„´
# - í”„ë ˆì´ì¦ˆ ê¸¸ì´
# - "ë‚˜ë‹¤ì›€"
```

---

## ğŸ’° ë¹„ìš© & ì‹œê°„ ì˜ˆìƒ

### Colab ë¬´ë£Œ

```
âœ… ì¥ì :
- ë¹„ìš©: $0
- TPU v2-8 ì‚¬ìš© ê°€ëŠ¥
- í…ŒìŠ¤íŠ¸ì— ì¶©ë¶„

âŒ ë‹¨ì :
- 12ì‹œê°„ ì„¸ì…˜ ì œí•œ
- 90ë¶„ idle timeout
- Fine-tuning ì¤‘ë‹¨ ìœ„í—˜
```

**ì˜ˆìƒ ì‹œê°„:**
- ë°ì´í„° 10ê°œ: 1-2ì‹œê°„
- ë°ì´í„° 20ê°œ: 3-4ì‹œê°„
- ë°ì´í„° 50ê°œ: 6-8ì‹œê°„

---

### Colab Pro ($10/month)

```
âœ… ì¥ì :
- 24ì‹œê°„ ì„¸ì…˜
- V100 GPU (ë” ë¹ ë¦„)
- ì•ˆì •ì 

âŒ ë‹¨ì :
- ì›” $10
```

**ì˜ˆìƒ ì‹œê°„:**
- ë°ì´í„° 10ê°œ: 30ë¶„-1ì‹œê°„
- ë°ì´í„° 20ê°œ: 1-2ì‹œê°„
- ë°ì´í„° 50ê°œ: 3-4ì‹œê°„

---

### ëŸ°íŒŸ RTX 3060 ($10 credit)

```
âœ… ì¥ì :
- ì‹œê°„ ì œí•œ ì—†ìŒ
- ì–¸ì œë“  ì¬ì‹œì‘
- QLoRA ì¶©ë¶„

âŒ ë‹¨ì :
- ì´ˆê¸° ì„¤ì • í•„ìš”
```

**ì˜ˆìƒ ì‹œê°„:**
- ë°ì´í„° 20ê°œ: 2-3ì‹œê°„
- ë°ì´í„° 50ê°œ: 4-6ì‹œê°„

**ë¹„ìš©:**
- RTX 3060: $0.20/hour
- 3ì‹œê°„ = $0.60
- 6ì‹œê°„ = $1.20
- â†’ $10 í¬ë ˆë”§ìœ¼ë¡œ ì¶©ë¶„!

---

## ğŸ¯ ì¶”ì²œ í”Œëœ

### ë‹¹ì‹ ì˜ ìƒí™©:
- ëŸ°íŒŸ $10 í¬ë ˆë”§
- Colab ë¬´ë£Œ ì‚¬ìš© ê°€ëŠ¥
- 3ì¼ ë‚¨ìŒ

### ìµœì  ì „ëµ:

**Day 1: Colab ë¬´ë£Œ**
```
âœ… ê¸°ë³¸ ìƒì„± í…ŒìŠ¤íŠ¸
âœ… ì²« ì¬ì¦ˆ í´ë¦½ 10ê°œ
âœ… ì‘ë™ í™•ì¸
â†’ ë¹„ìš©: $0
```

**Day 2: Colab ë¬´ë£Œ**
```
âœ… Audio Injection í…ŒìŠ¤íŠ¸
âœ… ë°ì´í„° ì¤€ë¹„
âœ… Fine-tuning ì‹œì‘ (ì‘ì€ ë°ì´í„°)
â†’ ë¹„ìš©: $0
```

**Day 3: ëŸ°íŒŸ ($1-2)**
```
âœ… ë³¸ê²© Fine-tuning (í° ë°ì´í„°)
âœ… 3-6ì‹œê°„ í•™ìŠµ
âœ… "ohhalim style" ì™„ì„±
â†’ ë¹„ìš©: $0.60 - $1.20
```

**ë‚¨ì€ í¬ë ˆë”§ ($8-9)**
```
â†’ ë‚˜ì¤‘ì— ì¶”ê°€ ì‹¤í—˜
â†’ ë” í° ëª¨ë¸ ì‹œë„
â†’ ë” ë§ì€ ë°ì´í„°
```

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Day 1: ê¸°ë³¸ ìƒì„±
- [ ] Colab ë°ëª¨ ì‹¤í–‰
- [ ] ì¬ì¦ˆ í´ë¦½ 10ê°œ ìƒì„±
- [ ] ZIP ë‹¤ìš´ë¡œë“œ
- [ ] í’ˆì§ˆ í™•ì¸

### Day 2: ë°ì´í„° ì¤€ë¹„
- [ ] Public dataset ë‹¤ìš´ë¡œë“œ
- [ ] Audio Injection í…ŒìŠ¤íŠ¸
- [ ] Fine-tuning ë°ì´í„° ì¤€ë¹„
- [ ] Tokenization ì™„ë£Œ

### Day 3: Fine-tuning
- [ ] QLoRA ì„¤ì •
- [ ] Training ì‹¤í–‰
- [ ] Model ì €ì¥ & ë‹¤ìš´ë¡œë“œ
- [ ] ìƒì„± í…ŒìŠ¤íŠ¸

### Day 4: í†µí•©
- [ ] Fine-tuned model ë¡œë“œ
- [ ] ë“œëìš© í´ë¦½ 10ê°œ ìƒì„±
- [ ] FL Studio í†µí•©
- [ ] Export & Rekordbox

---

## ğŸ”— ì°¸ê³  ìë£Œ

### ê³µì‹ ë§í¬
- GitHub: https://github.com/magenta/magenta-realtime
- Paper: https://arxiv.org/abs/2508.04651
- Colab Demo: https://colab.research.google.com/github/magenta/magenta-realtime/blob/main/notebooks/Magenta_RT_Demo.ipynb
- Finetune: https://colab.research.google.com/github/magenta/magenta-realtime/blob/main/notebooks/Magenta_RT_Finetune.ipynb

### QLoRA ë¦¬ì†ŒìŠ¤
- QLoRA Paper: https://arxiv.org/abs/2305.14314
- PEFT Library: https://github.com/huggingface/peft
- BitsAndBytes: https://github.com/TimDettmers/bitsandbytes

### Datasets
- PiJAMA: https://github.com/CPJKU/pijama
- Jazznet: https://github.com/tosiron/jazznet

---

## ğŸ’ª ë‹¤ìŒ ë‹¨ê³„ (3ì¼ í›„)

### ë‹¹ì‹ ì´ ê°€ì§ˆ ê²ƒ:
```
âœ… ë“œëìš© ì¬ì¦ˆ í´ë¦½ 10-20ê°œ
âœ… "ohhalim style" fine-tuned model
âœ… Generation pipeline
âœ… FL Studio í†µí•© ì›Œí¬í”Œë¡œìš°
```

### ì¶”ê°€ ê°œì„ :
```
1. ë” ë§ì€ ì—°ì£¼ ë…¹ìŒ (50-100ê°œ)
2. ë” ì •êµí•œ Fine-tuning
3. Real-time generation ë„ì „
4. ë¼ì´ë¸Œ ì„¸ì…˜ í…ŒìŠ¤íŠ¸
```

---

## ğŸ‰ ìµœì¢… ëª©í‘œ

**"ë‚˜ì™€ ê°€ìƒì˜ ë‚´ê°€ JAM!"**

```
ë‚˜: FL Studioì—ì„œ í•˜ìš°ìŠ¤ íŠ¸ë™ ì‘ê³¡
    â†“
AI: ë“œëì—ì„œ "ohhalim style" ì¬ì¦ˆ ì¦‰í¥ì—°ì£¼
    â†“
ë‚˜: Export â†’ Rekordbox â†’ ë¼ì´ë¸Œ ë””ì œì‰!
    â†“
ì²­ì¤‘: "ì™€, ì´ ë“œë ë¯¸ì³¤ë‹¤!" ğŸ”¥

â†’ ê¿ˆ ì‹¤í˜„! ğŸ’¯
```

---

**Let's go! ì§€ê¸ˆ ë°”ë¡œ ì‹œì‘í•˜ì„¸ìš”!** ğŸš€

**ì²« ë‹¨ê³„:** Colab ë°ëª¨ ì—´ê¸°
```
https://colab.research.google.com/github/magenta/magenta-realtime/blob/main/notebooks/Magenta_RT_Demo.ipynb
```
